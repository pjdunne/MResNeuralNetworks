{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions, Gradient Flow and Dead Neurons\n",
    "\n",
    "This notebook is heavily based on one from Universitaet van Amsterdam's deep learning course written by Phillip Lippe.\n",
    "\n",
    "You can see his original filled notebook at:\n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial3/Activation_Functions.ipynb)\n",
    "\n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial3)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1sFpZUpDJVjiYEvIqISqfkFizfsTnPf4s?usp=sharing)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've been introduced to neural networks last week by Jarvist. In this tutorial, we will take a closer look at the response of each neuron, known as the activation function. We'll investigate the effect of several popular fuctions on the optimization properties in neural networks. \n",
    "\n",
    "Activation functions are a crucial part of deep learning models as they add the non-linearity to neural networks. Without non-linearity, the number of layers doesn't really matter as the result will always be some linear combination of the input variables that you could have done in a single neuron.\n",
    "\n",
    "There is a great variety of activation functions in the literature, and some are more beneficial than others.\n",
    "The goal of this tutorial is to show the importance of choosing a good activation function (and how to do so), and what problems might occur if we don't.\n",
    "\n",
    "Before we start, we import our standard libraries and set up basic functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a function to set a seed for the randum numbers used in all libraries we might interact with in this tutorial (here numpy and torch). This allows us to make our training reproducible. However, note that in contrast to the CPU, the same seed on different GPU architectures can give different results. All the prebuilt models linked above here have been trained on an NVIDIA GTX1080Ti.\n",
    "\n",
    "Additionally, the following cell defines two paths: `DATASET_PATH` and `CHECKPOINT_PATH`. The dataset path is the directory where we will download datasets used in the notebooks. It is recommended to store all datasets from PyTorch in one joined directory to prevent duplicate downloads. The checkpoint path is the directory where we will store trained model weights and additional files. The needed files will be automatically downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial3\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell downloads all pretrained models we will use in this notebook. These are copies from Philip Lippe's course. The files are stored in a separate [repository](https://github.com/pjdunne/saved_models). Please let me (Patrick) know if an error occurs so it can be fixed for all students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/pjdunne/saved_models/main/tutorial3/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"FashionMNIST_elu.config\", \"FashionMNIST_elu.tar\", \n",
    "                    \"FashionMNIST_leakyrelu.config\", \"FashionMNIST_leakyrelu.tar\",\n",
    "                    \"FashionMNIST_relu.config\", \"FashionMNIST_relu.tar\",\n",
    "                    \"FashionMNIST_sigmoid.config\", \"FashionMNIST_sigmoid.tar\",\n",
    "                    \"FashionMNIST_swish.config\", \"FashionMNIST_swish.tar\",\n",
    "                    \"FashionMNIST_tanh.config\", \"FashionMNIST_tanh.tar\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we will implement some common activation functions by ourselves. Of course, most of them can also be found in the `torch.nn` package (see the [documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for an overview).\n",
    "However, we'll write our own functions here for a better understanding and insights.\n",
    "\n",
    "For an easier time of comparing various activation functions, we start with defining a base class from which all our future modules will inherit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ActivationFunction(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = self.__class__.__name__\n",
    "        self.config = {\"name\": self.name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every activation function will be an `nn.Module` so that we can integrate them nicely in a network. We will use the `config` dictionary to store adjustable parameters for some activation functions.\n",
    "\n",
    "Next, we implement two of the \"oldest\" activation functions that are still commonly used for various tasks: sigmoid and tanh. \n",
    "Both the sigmoid and tanh activation can be also found as PyTorch functions (`torch.sigmoid`, `torch.tanh`) or as modules (`nn.Sigmoid`, `nn.Tanh`). \n",
    "Here, you should implement them by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2488940349.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [5]\u001b[0;36m\u001b[0m\n\u001b[0;31m    return !!!!!!!!!!!\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return !!!!!!!!!!!\n",
    "\n",
    "##############################   \n",
    "    \n",
    "class Tanh(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        !!!!!!!\n",
    "        return !!!!!!!!!!\n",
    "    \n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular activation function that has allowed the training of deeper networks, is the Rectified Linear Unit (ReLU). \n",
    "\n",
    "This function just returns the input for inputs greater than 0, and 0 for input less than or equal to 0.\n",
    "\n",
    "Despite its simplicity of being a piecewise linear function, ReLU has one major benefit compared to sigmoid and tanh: a strong, stable gradient for a large range of values.\n",
    "Based on this idea, a lot of variations of ReLU have been proposed, of which we will implement the following three: LeakyReLU, ELU, and Swish. \n",
    "\n",
    "LeakyReLU: This replaces the zero returned in the negative part of the input with a smaller slope, with gradient given by 'alpha'. This means the gradient is non-zero so gradients can 'flow' also in this part of the input. We'll see what this means later.\n",
    "\n",
    "ELU: This is similar to the leaky ReLU but replaces the negative part with an exponential decay.\n",
    "\n",
    "Swish: This is the most recently proposed of these activation functions, and is actually the result of a large experiment with the purpose of finding the \"optimal\" activation function. Swish returns the input multiplied by the sigmoid of the input.\n",
    "Compared to the other activation functions, Swish is both smooth and non-monotonic (i.e. contains a change of sign in the gradient). Swish is designed to solve a problem called 'dead neurons' which happens with a standard ReLU. We'll see what this is later, but it is a particular problem with deep networks.\n",
    "If interested, a more detailed discussion of the benefits of Swish can be found in [this paper](https://arxiv.org/abs/1710.05941) [1].\n",
    "\n",
    "Now let's implement the four activation functions below, fill in the blanks yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return !!!!!!!!!!!\n",
    "\n",
    "##############################\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    \n",
    "    def __init__(self, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.config[\"alpha\"] = alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return !!!!!!!!!!!!!!!\n",
    "\n",
    "##############################\n",
    "    \n",
    "class ELU(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return !!!!!!!!!!!!!!!!!\n",
    "\n",
    "##############################\n",
    "    \n",
    "class Swish(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return !!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later usage, we summarize all our activation functions in a dictionary mapping the name to the class object. In case you implement a new activation function by yourself, add it here to include it in future comparisons as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act_fn_by_name = {\n",
    "    \"sigmoid\": Sigmoid,\n",
    "    \"tanh\": Tanh,\n",
    "    \"relu\": ReLU,\n",
    "    \"leakyrelu\": LeakyReLU,\n",
    "    \"elu\": ELU,\n",
    "    \"swish\": Swish\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing activation functions\n",
    "\n",
    "To get an idea of what each activation function actually does, we will visualize them in the following. \n",
    "Next to the actual activation value, the gradient of the function is an important aspect as it is crucial for optimizing the neural network. \n",
    "PyTorch allows us to compute the gradients simply by calling the `backward` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_grads(act_fn, x):\n",
    "    \"\"\"\n",
    "    Computes the gradients of an activation function at specified positions.\n",
    "    \n",
    "    Inputs:\n",
    "        act_fn - An object of the class \"ActivationFunction\" with an implemented forward pass.\n",
    "        x - 1D input tensor. \n",
    "    Output:\n",
    "        A tensor with the same size of x containing the gradients of act_fn at x.\n",
    "    \"\"\"\n",
    "    x = x.clone().requires_grad_() # Mark the input as tensor for which we want to store gradients\n",
    "    out = act_fn(x)\n",
    "    out.sum().backward() # Summing results in an equal gradient flow to each element in x\n",
    "    return x.grad # Accessing the gradients of x by \"x.grad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize all our activation functions including their gradients. We've defined some functions for you to fill in to make nice plots of all of the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_fn_by_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mgetoutput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!!!!!!!!!!!!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Add activation functions if wanted\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m act_fns \u001b[38;5;241m=\u001b[39m [act_fn() \u001b[38;5;28;01mfor\u001b[39;00m act_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[43mact_fn_by_name\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()]\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m# Range on which we want to visualize the activation functions\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m## Plotting\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'act_fn_by_name' is not defined"
     ]
    }
   ],
   "source": [
    "def vis_act_fn(act_fn, ax, x):\n",
    "    # Run activation function and get gradients\n",
    "    !!!!!!!!!!!!!!!\n",
    "    \n",
    "    # Push x, y and gradients back to cpu for plotting\n",
    "    !!!!!!!!!!!!!!!\n",
    "    ## Plotting\n",
    "    !!!!!!!!!!!!!!!\n",
    "    \n",
    "    \n",
    "    \n",
    "# Add activation functions if wanted\n",
    "act_fns = [act_fn() for act_fn in act_fn_by_name.values()]\n",
    "x = torch.linspace(-5, 5, 1000) # Range on which we want to visualize the activation functions\n",
    "## Plotting\n",
    "rows = math.ceil(len(act_fns)/2.0)\n",
    "fig, ax = plt.subplots(rows, 2, figsize=(8, rows*4))\n",
    "for i, act_fn in enumerate(act_fns):\n",
    "    vis_act_fn(act_fn, ax[divmod(i,2)], x)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the effect of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing and visualizing the activation functions, we are aiming to gain insights into their effect. \n",
    "We do this by using a simple neural network trained on [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) and examine various aspects of the model, including the performance and gradient flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's set up a neural network. Implement a network to view the images as 1D tensors and push them through a sequence of linear layers and a specified activation function. I've given the function signature below. Feel free to experiment with other network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            act_fn - Object of the activation function that should be used as non-linearity in the network.\n",
    "            input_size - Size of the input images in pixels\n",
    "            num_classes - Number of classes we want to predict\n",
    "            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create the network based on the specified hidden sizes\n",
    "        !!!!!!!!!!!        \n",
    "        \n",
    "        \n",
    "        # We store all hyperparameters in a dictionary for saving and loading of the model\n",
    "        self.config = {\"act_fn\": act_fn.config, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes} \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # Reshape images to a flat vector\n",
    "        out = self.layers(x)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add functions for loading and saving the model. The hyperparameters are stored in a configuration file (simple json file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _get_config_file(model_path, model_name):\n",
    "    # Name of the file for storing hyperparameter details\n",
    "    return os.path.join(model_path, model_name + \".config\")\n",
    "\n",
    "def _get_model_file(model_path, model_name):\n",
    "    # Name of the file for storing network parameters\n",
    "    return os.path.join(model_path, model_name + \".tar\")\n",
    "\n",
    "def load_model(model_path, model_name, net=None):\n",
    "    \"\"\"\n",
    "    Loads a saved model from disk.\n",
    "    \n",
    "    Inputs:\n",
    "        model_path - Path of the checkpoint directory\n",
    "        model_name - Name of the model (str)\n",
    "        net - (Optional) If given, the state dict is loaded into this model. Otherwise, a new model is created.\n",
    "    \"\"\"\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    assert os.path.isfile(config_file), f\"Could not find the config file \\\"{config_file}\\\". Are you sure this is the correct path and you have your model config stored here?\"\n",
    "    assert os.path.isfile(model_file), f\"Could not find the model file \\\"{model_file}\\\". Are you sure this is the correct path and you have your model stored here?\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    if net is None:\n",
    "        act_fn_name = config_dict[\"act_fn\"].pop(\"name\").lower()\n",
    "        act_fn = act_fn_by_name[act_fn_name](**config_dict.pop(\"act_fn\"))\n",
    "        net = BaseNetwork(act_fn=act_fn, **config_dict)\n",
    "    net.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    return net\n",
    "    \n",
    "def save_model(model, model_path, model_name):\n",
    "    \"\"\"\n",
    "    Given a model, we save the state_dict and hyperparameters.\n",
    "    \n",
    "    Inputs:\n",
    "        model - Network object to save parameters from\n",
    "        model_path - Path of the checkpoint directory\n",
    "        model_name - Name of the model (str)\n",
    "    \"\"\"\n",
    "    config_dict = model.config\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config_dict, f)\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set up the dataset we want to train it on, namely [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). FashionMNIST is a more complex version of MNIST and contains black-and-white images of clothes instead of digits. The 10 classes include trousers, coats, shoes, bags and more. To load this dataset, we will make use of yet another PyTorch package, namely `torchvision` ([documentation](https://pytorch.org/docs/stable/torchvision/index.html)). The `torchvision` package consists of popular datasets, model architectures, and common image transformations for computer vision. We will use the package for many of the notebooks in this course to simplify our dataset handling. \n",
    "\n",
    "Let's load the dataset below, and visualize a few images to get an impression of the data. FashionMNIST already has the training and test sets separated but we need to split the training and validation sets ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformations applied on each image => first make them a tensor, then normalize them in the range -1 to 1\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bin/bash: !!!!!!!!!!!!!!!!!: command not found']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exmp_imgs = [train_set[i][0] for i in range(16)]\n",
    "# Organize the images into a grid for nicer visualization and plot them\n",
    "!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient flow\n",
    "\n",
    "Gradient flow describes how activation functions propagate gradients through the network. Imagine we have a very deep neural network with more than 50 layers. The gradients for the input layer, i.e. the very first layer, have passed >50 times the activation function, but we still want them to be of a reasonable size. If the gradient through the activation function is (in expectation) considerably smaller than 1, our gradients will vanish until they reach the input layer. If the gradient through the activation function is larger than 1, the gradients exponentially increase and might explode.\n",
    "\n",
    "How do you think gradient flow influences choosing a training rate for your network?\n",
    "\n",
    "To get a feeling of how every activation function influences the gradients, plot a freshly initialized network and measure the gradients for each parameter at the activation function layers of your model (which are these?) for a batch of 256 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_gradients(net, color=\"C0\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        net - Object of class BaseNetwork\n",
    "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    \n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    !!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
    "    grads = {name: params.grad.data.view(-1).cpu().clone().numpy() for name, params in net.named_parameters() if \"weight\" in name}\n",
    "    net.zero_grad()\n",
    "    \n",
    "    ## Plotting\n",
    "    columns = len(grads)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3.5, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in grads:\n",
    "        key_ax = ax[fig_index%columns]\n",
    "        sns.histplot(data=grads[key], bins=30, ax=key_ax, color=color, kde=True)\n",
    "        key_ax.set_title(str(key))\n",
    "        key_ax.set_xlabel(\"Grad magnitude\")\n",
    "        fig_index += 1\n",
    "    fig.suptitle(f\"Gradient magnitude distribution for activation function {net.config['act_fn']['name']}\", fontsize=14, y=1.05)\n",
    "    fig.subplots_adjust(wspace=0.45)\n",
    "    plt.show()\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_fn_by_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## Create a plot for every activation function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, act_fn_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mact_fn_by_name\u001b[49m):\n\u001b[1;32m      6\u001b[0m     set_seed(\u001b[38;5;241m42\u001b[39m) \u001b[38;5;66;03m# Setting the seed ensures that we have the same weight initialization for each activation function\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     act_fn \u001b[38;5;241m=\u001b[39m act_fn_by_name[act_fn_name]()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'act_fn_by_name' is not defined"
     ]
    }
   ],
   "source": [
    "# Seaborn prints warnings if histogram has small values. We can ignore them for now\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "## Create a plot for every activation function\n",
    "for i, act_fn_name in enumerate(act_fn_by_name):\n",
    "    set_seed(42) # Setting the seed ensures that we have the same weight initialization for each activation function\n",
    "    act_fn = act_fn_by_name[act_fn_name]()\n",
    "    net_actfn = BaseNetwork(act_fn=act_fn).to(device)\n",
    "    visualize_gradients(net_actfn, color=f\"C{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid activation function shows a clearly undesirable behavior. While the gradients for the output layer are very large with up to 0.1, the input layer has the lowest gradient norm across all activation functions with only 1e-5. This is due to its small maximum gradient of 1/4, and finding a suitable learning rate across all layers is not possible in this setup.\n",
    "All the other activation functions show to have similar gradient norms across all layers. Interestingly, the ReLU activation has a spike around 0 which is caused by its zero-part on the left, and dead neurons (we will take a closer look at this later on).\n",
    "\n",
    "Note that additionally to the activation, the initialization of the weight parameters can be crucial. By default, PyTorch uses the [Kaiming](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) initialization for linear layers optimized for ReLU activations. We could do a whole extra class on initialization, but assume for now that the Kaiming initialization works for all activation functions reasonably well (the Universitet van Amsterdam course has more information on initialisation in lecture 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "Next, we want to train our model with different activation functions on FashionMNIST and compare the gained performance. All in all, our final goal is to achieve the best possible performance on a dataset of our choice. \n",
    "Therefore, you need to write a training loop in the next cell including a validation after every epoch and a final test on the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(net, model_name, max_epochs=50, patience=7, batch_size=256, overwrite=False):\n",
    "    \"\"\"\n",
    "    Train a model on the training set of FashionMNIST\n",
    "    \n",
    "    Inputs:\n",
    "        net - Object of BaseNetwork\n",
    "        model_name - (str) Name of the model, used for creating the checkpoint names\n",
    "        max_epochs - Number of epochs we want to (maximally) train for\n",
    "        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n",
    "        batch_size - Size of batches used in training\n",
    "        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n",
    "    if file_exists and not overwrite:\n",
    "        print(\"Model file already exists. Skipping training...\")\n",
    "    else:\n",
    "        if file_exists:\n",
    "            print(\"Model file exists, but will be overwritten...\")\n",
    "            \n",
    "        # Defining optimizer, loss and data loader\n",
    "        optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) # Default parameters, feel free to change\n",
    "        loss_module = nn.CrossEntropyLoss() \n",
    "        train_loader_local = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "        val_scores = []\n",
    "        best_val_epoch = -1\n",
    "        for epoch in range(max_epochs):\n",
    "            ############\n",
    "            # Training #\n",
    "            ############\n",
    "            net.train()\n",
    "            true_preds, count = 0., 0\n",
    "            for imgs, labels in tqdm(train_loader_local, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "                imgs, labels = imgs.to(device), labels.to(device) # To GPU if we have one\n",
    "                optimizer.zero_grad() # Zero-grad can be placed anywhere before \"loss.backward()\"\n",
    "                \n",
    "                \n",
    "                !!!!!!!!FILL IN THE TRAINING LOOP\n",
    "                \n",
    "                # Record statistics during training\n",
    "                true_preds += (preds.argmax(dim=-1) == labels).sum()\n",
    "                count += labels.shape[0]\n",
    "            train_acc = true_preds / count\n",
    "\n",
    "            ##############\n",
    "            # Validation #\n",
    "            ##############\n",
    "            val_acc = test_model(net, val_loader)\n",
    "            val_scores.append(val_acc)\n",
    "            print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n",
    "\n",
    "            if len(val_scores) == 1 or val_acc > val_scores[best_val_epoch]:\n",
    "                print(\"\\t   (New best performance, saving model...)\")\n",
    "                save_model(net, CHECKPOINT_PATH, model_name)\n",
    "                best_val_epoch = epoch\n",
    "            elif best_val_epoch <= epoch - patience:\n",
    "                print(f\"Early stopping due to no improvement over the last {patience} epochs\")\n",
    "                break\n",
    "\n",
    "        # Plot a curve of the validation accuracy\n",
    "        !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    load_model(CHECKPOINT_PATH, model_name, net=net)\n",
    "    test_acc = test_model(net, test_loader)\n",
    "    print((f\" Test accuracy: {test_acc*100.0:4.2f}% \").center(50, \"=\")+\"\\n\")\n",
    "    return test_acc\n",
    "    \n",
    "\n",
    "def test_model(net, data_loader):\n",
    "    \"\"\"\n",
    "    Test a model on a specified dataset.\n",
    "    \n",
    "    Inputs:\n",
    "        net - Trained model of type BaseNetwork\n",
    "        data_loader - DataLoader object of the dataset to test on (validation or test)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    true_preds, count = 0., 0\n",
    "    for imgs, labels in data_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            !!!!!!!!!!!!WRITE THE TEST FUNCTION\n",
    "    return test_acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train one model for each activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_fn_by_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act_fn_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mact_fn_by_name\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining BaseNetwork with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_fn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m activation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     set_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'act_fn_by_name' is not defined"
     ]
    }
   ],
   "source": [
    "for act_fn_name in act_fn_by_name:\n",
    "    print(f\"Training BaseNetwork with {act_fn_name} activation...\")\n",
    "    set_seed(42)\n",
    "    act_fn = act_fn_by_name[act_fn_name]()\n",
    "    net_actfn = BaseNetwork(act_fn=act_fn).to(device)\n",
    "    train_model(net_actfn, f\"FashionMNIST_{act_fn_name}\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which activation functions perform well? Is this what you were expecting from their gradient flow properties?\n",
    "\n",
    "Not surprisingly, the model using the sigmoid activation function fails and does not improve upon random performance (10 classes => 1/10 for random chance). This is because the gradient of the input layer parameters on the loss function is negligible compared to the other layers.\n",
    "\n",
    "All the other activation functions gain similar performance.\n",
    "To have a more accurate conclusion, we would have to train the models for multiple seeds and look at the averages.\n",
    "However, the \"optimal\" activation function also depends on many other factors (hidden sizes, number of layers, type of layers, task, dataset, optimizer, learning rate, etc.) so that a thorough grid search would not be useful in our case.\n",
    "In the literature, activation functions that have shown to work well with deep networks are all types of ReLU functions we experiment with here, with small gains for specific activation functions in specific networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the activation distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have trained the models, we can look at the actual activation values that our found inside the model. For instance, how many neurons are set to zero in ReLU? Where do we find most values in Tanh?\n",
    "To answer these questions, write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activations inside the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_activations(net, color=\"C0\"):\n",
    "    activations = {}\n",
    "    \n",
    "    !!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    ## Plotting\n",
    "    !!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_fn_by_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, act_fn_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mact_fn_by_name\u001b[49m):\n\u001b[1;32m      2\u001b[0m     net_actfn \u001b[38;5;241m=\u001b[39m load_model(model_path\u001b[38;5;241m=\u001b[39mCHECKPOINT_PATH, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFashionMNIST_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mact_fn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     visualize_activations(net_actfn, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'act_fn_by_name' is not defined"
     ]
    }
   ],
   "source": [
    "for i, act_fn_name in enumerate(act_fn_by_name):\n",
    "    net_actfn = load_model(model_path=CHECKPOINT_PATH, model_name=f\"FashionMNIST_{act_fn_name}\").to(device)\n",
    "    visualize_activations(net_actfn, color=f\"C{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model with sigmoid activation was not able to train properly, the activations are also less informative and all gathered around 0.5 (the activation at input 0).\n",
    "\n",
    "The tanh shows a more diverse behavior. While for the input layer we experience a larger amount of neurons to be close to -1 and 1, where the gradients are close to zero, the activations in the two consecutive layers are closer to zero. This is probably because the input layers look for specific features in the input image, and the consecutive layers combine those together. The activations for the last layer are again more biased to the extreme points because the classification layer can be seen as a weighted average of those values (the gradients push the activations to those extremes).\n",
    "\n",
    "The ReLU has a strong peak at 0, as we initially expected. The effect of having no gradients for negative values is that the network does not have a Gaussian-like distribution after the linear layers, but a longer tail towards the positive values. \n",
    "The LeakyReLU shows a very similar behavior while ELU follows again a more Gaussian-like distribution. \n",
    "The Swish activation seems to lie in between, although it is worth noting that Swish uses significantly higher values than other activation functions (up to 20).\n",
    "\n",
    "As all activation functions show slightly different behavior although obtaining similar performance for our simple network, it becomes apparent that the selection of the \"optimal\" activation function really depends on many factors, and is not the same for all possible networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding dead neurons in ReLU networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One known drawback of the ReLU activation is the occurrence of \"dead neurons\", i.e. neurons with no gradient for any training input.\n",
    "\n",
    "Given our training model is based on finding the gradient of the loss function with respect to each layer's parameters. The dead neurons therefore mean we cannot train the parameters of this neuron in the previous layer to obtain output values besides zero.\n",
    "For dead neurons to happen, the output value of a specific neuron of the linear layer before the ReLU has to be negative for all input images.\n",
    "Considering the large number of neurons we have in a neural network, it is not unlikely for this to happen. \n",
    "\n",
    "To get a better understanding of how much of a problem this is, and when we need to be careful, we will measure how many dead neurons different networks have. For this, you need to implement a function which runs the network on the whole training set and records whether a neuron is exactly 0 for all data points or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def measure_number_dead_neurons(net):\n",
    "\n",
    "    # For each neuron, we create a boolean variable initially set to 1. If it has an activation unequals 0 at any time,\n",
    "    # we set this variable to 0. After running through the whole training set, only dead neurons will have a 1.\n",
    "    neurons_dead = [\n",
    "        torch.ones(layer.weight.shape[0], device=device, dtype=torch.bool) for layer in net.layers[:-1] if isinstance(layer, nn.Linear)\n",
    "    ] # Same shapes as hidden size in BaseNetwork\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(train_loader, leave=False): # Run through whole training set\n",
    "            !!!!!!!!!TEST TO SEE IF ALL ACTIVATIONS ARE ZERO\n",
    "    number_neurons_dead = [t.sum().item() for t in neurons_dead]\n",
    "    print(\"Number of dead neurons:\", number_neurons_dead)\n",
    "    print(\"In percentage:\", \", \".join([f\"{(100.0 * num_dead / tens.shape[0]):4.2f}%\" for tens, num_dead in zip(neurons_dead, number_neurons_dead)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your function to measure the number of dead neurons in an untrained ReLU activation function network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ReLU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m net_relu \u001b[38;5;241m=\u001b[39m BaseNetwork(act_fn\u001b[38;5;241m=\u001b[39m\u001b[43mReLU\u001b[49m())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m measure_number_dead_neurons(net_relu)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ReLU' is not defined"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "net_relu = BaseNetwork(act_fn=ReLU()).to(device)\n",
    "measure_number_dead_neurons(net_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only a minor amount of neurons are dead, but that they increase with the depth of the layer.\n",
    "However, this is not a problem for the small number of dead neurons we have as the input to later layers is changed due to updates to the weights of previous layers. Therefore, dead neurons in later layers can potentially become \"alive\"/active again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try for a different activation function, what are you expecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ELU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m net_elu \u001b[38;5;241m=\u001b[39m BaseNetwork(act_fn\u001b[38;5;241m=\u001b[39m\u001b[43mELU\u001b[49m())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m measure_number_dead_neurons(net_elu)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ELU' is not defined"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "net_elu = BaseNetwork(act_fn=ELU()).to(device)\n",
    "measure_number_dead_neurons(net_elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at this for a trained ReLU network (with the same initialization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'act_fn_by_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net_relu \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFashionMNIST_relu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m measure_number_dead_neurons(net_relu)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_path, model_name, net)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m net \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     act_fn_name \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 25\u001b[0m     act_fn \u001b[38;5;241m=\u001b[39m \u001b[43mact_fn_by_name\u001b[49m[act_fn_name](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     26\u001b[0m     net \u001b[38;5;241m=\u001b[39m BaseNetwork(act_fn\u001b[38;5;241m=\u001b[39mact_fn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m     27\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_file, map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'act_fn_by_name' is not defined"
     ]
    }
   ],
   "source": [
    "net_relu = load_model(model_path=CHECKPOINT_PATH, model_name=\"FashionMNIST_relu\").to(device)\n",
    "measure_number_dead_neurons(net_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dead neurons indeed decreased in the later layers. However, it should be noted that dead neurons are especially problematic in the input layer. As the input does not change over epochs (the training set is kept as it is), training the network cannot turn those neurons back active. Still, the input data has usually a sufficiently high standard deviation to reduce the risk of dead neurons.\n",
    "\n",
    "Finally, check how the number of dead neurons behaves with increasing layer depth. For instance, try a 10-layer neural network with 256 nodes in the first 5 layers and 128 in the last 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SList' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m net_relu \u001b[38;5;241m=\u001b[39m get_ipython()\u001b[38;5;241m.\u001b[39mgetoutput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!!!!!!!!!!!!!!!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmeasure_number_dead_neurons\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_relu\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mmeasure_number_dead_neurons\u001b[0;34m(net)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeasure_number_dead_neurons\u001b[39m(net):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# For each neuron, we create a boolean variable initially set to 1. If it has an activation unequals 0 at any time,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# we set this variable to 0. After running through the whole training set, only dead neurons will have a 1.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     neurons_dead \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 6\u001b[0m         torch\u001b[38;5;241m.\u001b[39mones(layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mLinear)\n\u001b[1;32m      7\u001b[0m     ] \u001b[38;5;66;03m# Same shapes as hidden size in BaseNetwork\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SList' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "net_relu = !!!!!!!!!!!!!!!!\n",
    "measure_number_dead_neurons(net_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dead neurons is significantly higher than before which harms the gradient flow especially in the first iterations. For instance, more than 56% of the neurons in the pre-last layer are dead which creates a considerable bottleneck.\n",
    "Hence, it is advisible to use other nonlinearities like Swish for very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have reviewed a set of six activation functions (sigmoid, tanh, ReLU, LeakyReLU, ELU, and Swish) in neural networks, and discussed how they influence the gradient distribution across layers. Sigmoid tends to fail deep neural networks as the highest gradient it provides is 0.25 leading to vanishing gradients in early layers. All ReLU-based activation functions have shown to perform well, and besides the original ReLU, do not have the issue of dead neurons. When implementing your own neural network, it is recommended to start with a ReLU-based network and select the specific activation function based on the properties of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). [Paper link](https://arxiv.org/abs/1710.05941) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
