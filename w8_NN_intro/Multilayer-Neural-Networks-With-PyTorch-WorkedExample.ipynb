{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae2fbe0",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks with PyTorch - Worked Example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbd152",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [PyTorch](#pytorch)\n",
    "1. [Generating Data and Setting Up](#setup)\n",
    "1. [Revisiting the Single Neuron Binary Classifier](#binary)\n",
    "1. [Training](#training)\n",
    "1. [Multilayer Perceptron](#multi)\n",
    "1. [Training the Multilayer network](#trainmulti)\n",
    "1. [Conclusion](#conc)\n",
    "1. [Appendix](#app)\n",
    "    1. [Coordinate Transformation](#coord)\n",
    "    1. [PyTorch backward() Function](#backward_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707650e3",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# PyTorch [^](#index) <a id='pytorch'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c36f24",
   "metadata": {},
   "source": [
    "Welcome to the wonderful world of PyTorch! \n",
    "\n",
    "There are a lot of sharp edges, but PyTorch is still arguably the most seamless package for building neural networks.\n",
    "\n",
    "[Documentation](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf1035",
   "metadata": {},
   "source": [
    "We will firstly import `torch` and `torch.nn`. Note that module to import is not called pytorch - just torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(31337) # for reproducible runs, manually set the PRNG seed (pseudo-random number generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536444a",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "\n",
    "PyTorch works exclusively with its own datatype similar to a numpy array, known as a **tensor**.\n",
    "Tensors are all encompassing, in the sense that data structures of any dimension can be tensors.\n",
    "\n",
    "- Scalars  - Rank 0 Tensors\n",
    "- Vectors  - Rank 1 Tensors\n",
    "- Matrices - Rank 2 Tensors etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfde88",
   "metadata": {},
   "source": [
    "The simplest way to create a tensor is to use `torch.tensor()` and pass in data in the same way you would with a numpy array.\n",
    "\n",
    "Tensors, similar to numpy arrays, **can only store a single datatype**. This means that the type stored in a tensor is a property of that object.\n",
    "\n",
    "When you create a tensor, you can specify the datatype it stores using the `dtype` keyword argument. The documentation for PyTorch tensors has a [list of datatypes](https://pytorch.org/docs/stable/tensors.html#data-types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c1e5e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "Create a tensor of any shape without specifying datatype. What is the default datatype? Does it depend on the type you use to create the tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa15e4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Check the type of data contained using `.dtype` and the type of tensor using `.type()`. Note the brackets after the type attribute. \n",
    "\n",
    "</div>\n",
    "\n",
    "The documentation linked above also contains details on types of tensors themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1.0,1.0]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a09510",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0)\n",
    "x.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb26602",
   "metadata": {},
   "source": [
    "PyTorch provides methods to create empty, all 0s, all 1s and random-valued tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd02f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = torch.empty(3,5)\n",
    "ran = torch.rand(2,4)\n",
    "zeros = torch.zeros(2,3,4)\n",
    "ones = torch.ones(6)\n",
    "\n",
    "print(f'empty tensor with shape {empty.shape}:\\n\\n{empty}\\n')\n",
    "print(f'random tensor with shape {ran.shape}:\\n\\n{ran}\\n')\n",
    "print(f'zeros tensor with shape {zeros.shape}:\\n\\n{zeros}\\n')\n",
    "print(f'ones tensor with shape {ones.shape}:\\n\\n{ones}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c484ebb",
   "metadata": {},
   "source": [
    "An important keyword we can specify is `requires_grad`. The default value is false. This must be set to true for tensors which are involved in the training process. Recall that the computation of gradients is a key part of the optimization process using gradient descent, or any other optimizer. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086abfd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.tensor([8,9]).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84f7c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "Those who have a GPU on their computer can use the cell below to check whether CUDA is available. Read about CUDA [here](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have GPU acceleration?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d5e3e",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "The first thing we will build is a [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). Data which is fed into a linear layer is linearly transformed, but not passed through an activation function.\n",
    "\n",
    "To do this, we must specify the number of input 'legs' and output 'legs'.\n",
    "\n",
    "`nn.linear` will generate random weights and biases (here specified by our random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=2 # the number of input legs\n",
    "n=1 # the number of output legs\n",
    "\n",
    "linearlayer = nn.Linear(m,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f6dff",
   "metadata": {},
   "source": [
    "We can call the function on a rank 1 tensor of length 2 (since there are 2 input legs), which should produce a rank 0 tensor, a.k.a. a scalar (since there is just 1 output leg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearlayer(torch.Tensor([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4306397",
   "metadata": {},
   "source": [
    "Above, we passed 1 instance of data which has 2 features, from which our linear layer produced 1 output.\n",
    "We'll now pass 20 instances of data, so our output should have shape (20,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fc6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt=torch.randn(20, m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b434946",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearlayer(rt).shape # pass through the linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8248720",
   "metadata": {},
   "source": [
    "### Inspecting the model\n",
    "\n",
    "We can look at the weights and the biases of our model.\n",
    "The model is an object, so has both data and method attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed95366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W\", linearlayer.weight)\n",
    "print(\"b\", linearlayer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f243fc",
   "metadata": {},
   "source": [
    "You will notice the `requires_grad` keyword here. This must be set to True for a tensor if we are going to require the gradient to be calculated at any point in its existence. For weights and biases in a neural network, we definitely need this to be True, as the training process requires computation of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0998f4",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Generating Data and Setting Up [^](#index) <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3beeff",
   "metadata": {},
   "source": [
    "We will import relevant modules, set the plotting parameters for the notebook and generate a toy dataset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.datasets\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [3,3]\n",
    "plt.rcParams['figure.dpi'] = 200 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc3fcd",
   "metadata": {},
   "source": [
    "Generating a toy dataset:\n",
    "\n",
    "(We use a random seed here to ensure reproducible runs, making debugging possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=sklearn.datasets.make_classification(n_features=2, n_redundant=0, n_samples=n_samples,\n",
    "    n_informative=2, random_state=None, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=Y, s=20, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e4f72",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Revisiting the Single Neuron Binary Classifier [^](#index) <a id='binary'></a> \n",
    "\n",
    "\n",
    "We will now recreate the binary classifier we made previously, now we have the full power of Pytorch to train it efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8876b2-24e5-45cd-bae3-3e023490accf",
   "metadata": {},
   "source": [
    "Previously we had to implement all of the training steps ourselves from scratch - this time we will make use of the convenient built-in functionality in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0672a-42d3-4d17-aa7b-484580306053",
   "metadata": {},
   "source": [
    "We need a model which consists of the linear layer from above, but with a Sigmoid activation function. When building a model, `nn.Sequential` allows you to specify all the layers and activation functions in order. E.g., below we pass our linear layer followed by the sigmoid function. The output from one layer becomes the input to the next one. The convenience of this feature becomes more apparent as the networks you build get bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=2 # two dimensional input\n",
    "output_size=1 # one output\n",
    "\n",
    "# Output from one activation function becomes the input for the next\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size,output_size), nn.Sigmoid())\n",
    "\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dce883",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "There is an alternative way to build arbitrary models in \n",
    "PyTorch by constructing a Python class, but this is considerably more complicated, involves a lot of \n",
    "object-orientated boilerplate code, and isn't necessary for \n",
    "simple architectures such as these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6e22c",
   "metadata": {},
   "source": [
    "As before with our linear layer, we input 2 arbitrary values, which should generate a scalar output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor([0.0,0.0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063dc08",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Training [^](#index) <a id='training'></a> \n",
    "\n",
    "We will now train our neural network. For this, we need to define a loss function.\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "See [the documentation from pytorch on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to find an applicable loss function and implement it below.\n",
    "How does this compare to the self-coded loss function from the previous notebook?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e777a",
   "metadata": {},
   "source": [
    "When building a neural network in PyTorch, we need to define an optimizer. Recall that the training process often boils down to a multivariate minimisation problem - there are many pre-built algorithms which can carry this out for us efficiently. We will be using **stochastic gradient descent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) # experiment with different learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75689252",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader\n",
    "\n",
    "Here, we make use of [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) (very commonly used) and `TensorDataset`. You can read about them using the links attached, but the purpose of these objects, particularly the DataLoader, is to combine the various steps associated with loading in, processing and iterating through data.\n",
    "\n",
    "The DataLoader object is an iterator, making it very well suited to the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeba7b3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "Read the docmentation for the DataLoader object, particularly the meaning of each keyword argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cee7d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "For larger scale applications of machine learning which make use of GPUs, the DataLoader object is beneficial as it shuffles data around which means the data is fed more efficiently to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6f4b1",
   "metadata": {},
   "source": [
    "In order to work with our generated datasets using PyTorch, we need them to be Tensor objects so we change the datatype below.\n",
    "\n",
    "The reason why we need to create D using `torch.utils.data.TensorDataset()` is that the DataLoader takes a single dataset as an argument, rather than separate arrays for X and Y, which we started with. It also ensures our data is in tensor form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89405f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtorch = torch.tensor(X).float()\n",
    "Ytorch = torch.tensor(Y).view(len(Y),1).float()\n",
    "D = torch.utils.data.TensorDataset(Xtorch,Ytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f3a96",
   "metadata": {},
   "source": [
    "For stochastic gradient descent, we use **batches** of data. We can specify `batch_size` for DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a6402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(D , batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba22090",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we're ready to train our model. As before, we will define a function which performs one full training loop. We can make this function take the number of epochs, `n`, as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc087d5",
   "metadata": {},
   "source": [
    "In PyTorch, the training process for one batch in one epoch is generally as follows:\n",
    "\n",
    ">1. Obtain batch from the DataLoader\n",
    ">1. Perform a forward pass, i.e., make a prediction on the data using the model\n",
    ">1. Compute the loss (how poor the prediction is) using the loss function\n",
    ">1. Compute gradients (known as the backward pass)\n",
    ">1. Use the gradients to perform a 'step' with the optimizer (hopefully toward the minimum of the loss function) in the direction of the 'most negative' gradient.\n",
    "\n",
    "This is then repeated for all batches (in the case where we are using SGD as our optimizer) and the whole process is performed $n$ times (for $n$ epochs).\n",
    "\n",
    "The best way to see how to achieve this is by observing an example. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4442d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(n): # train for this many epochs\n",
    "    for i in range(n):\n",
    "        totalloss=0\n",
    "        for Xtorch,Ytorch in trainloader: # 1. pull a 'batch' of data from the loader\n",
    "            \n",
    "            y=model(Xtorch) # 2. forward pass through model\n",
    "            loss=loss_function(y, Ytorch) # 3. compute the loss\n",
    "            totalloss+=loss.detach() # detach() tells the machine not to track the gradient of this tensor\n",
    "            \n",
    "            loss.backward() # 4. calculate backward (gradient) pass\n",
    "            optimizer.step() # 5. use gradient info in optimise step\n",
    "        \n",
    "        losses.append(totalloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ba9b6",
   "metadata": {},
   "source": [
    "Again we will visualise the decision boundary, with the training data overlayed, so we can monitor how the model changes during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad850d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a grid of predictions, for plotting the decision boundary\n",
    "N=100\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "predict=predict.reshape( (N,N) ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffebaf8",
   "metadata": {},
   "source": [
    "In the cell below, we run through 25 iterations. In each iteration, we plot the decision boundary (by making our model output predictions for every point in a grid) and then complete one training step.\n",
    "\n",
    "If we did not want to visualise the decision boundary during the training process, we would simply have to run:\n",
    "\n",
    "```python\n",
    "train(25)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68848b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) ).detach()\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    train(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5620",
   "metadata": {},
   "source": [
    "We can plot the loss as a function of the number of epochs. Whilst the above plot shows qualitatively that the model improves over the training process, the plot below shows it quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47226c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec21e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "What fraction of the training data do we correctly predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71534238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\",(Ytorch == torch.round(model(Xtorch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4b34",
   "metadata": {},
   "source": [
    "We can view all of the model's parameters using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d917c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5f26d",
   "metadata": {},
   "source": [
    "## Experiment with Manually Setting Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a960a27",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "The code block below allows us to maually set the weights of the model. Try and do this to correctly describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model[0].weight[0,0]=0\n",
    "    model[0].weight[0,1]=1\n",
    "    model[0].bias[0]=0\n",
    "    \n",
    "for name,param in model.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "# make a grid of predictions, for plotting the decision boundary\n",
    "\n",
    "N=100\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "predict=predict.reshape( (N,N) ).detach()\n",
    "\n",
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)\n",
    "\n",
    "# Accuracy - what fraction of the train data go we correctly predict?\n",
    "print(\"Accuracy\",(Ytorch == torch.round(model(Xtorch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e9d8f-9c16-4651-9019-ae4e3211e9ad",
   "metadata": {},
   "source": [
    "We have now demonstrated the basic process for training a neural network in PyTorch. We have seen that, unlike in the previous notebook, we have not had to do many things 'by hand' - rather, PyTorch performs a lot of these tasks for us 'under the hood'. It may seem that what we had to do to train just a single neuron was quite a lot of work, however building and training a much larger network follows almost the same process with hardly any extra work. This is where PyTorch comes into its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfeac5",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Multilayer Perceptron [^](#index) <a id='multi'></a>\n",
    "\n",
    "From our work thus far, we can see that a single neuron is only capable of linearly separating the input data. \n",
    "\n",
    "Instead we can make a multi-layer perceptron, which goes beyond the capabilities of a single neuron. It also allows us to begin to learn a lower-dimensional representation of the data.\n",
    "\n",
    "This is where the term 'deep' learning comes from, once you have a deep network of many artificial neuron layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1d176",
   "metadata": {},
   "source": [
    "## Backpropagation  <a id='backprop'></a>\n",
    "\n",
    "Now that we have moved onto multilayer networks, it is worth introducing the concept of backpropagation. Previously, when we found the rate of change of the loss function with respect to a parameter of our model, it was straight-forward. Now our network has (an) extra layer(s), the derivative of the loss function with respect to a weight has a less simple form due to the interdependence of neurons in successive layers.\n",
    "\n",
    "Consider the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51546635",
   "metadata": {},
   "source": [
    "<img src=\"https://studymachinelearning.com/wp-content/uploads/2019/12/crop_NN.jpg\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc32f1-2d11-439b-a497-f20fee09f275",
   "metadata": {},
   "source": [
    "The [Image](https://studymachinelearning.com/wp-content/uploads/2019/12/crop_NN.jpg) depicts a neural network with an input layer, a hidden layer and an output layer consisting of 1 neuron. The hidden and output layer have bias terms shown in red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89129c00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Say we want to find the gradient of the loss function with respect to the weight $w_{11}$. Labelling our Loss as $L$, we want to find \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{11}}$$\n",
    "\n",
    "Note that we $a_{ij}$ is defined as the result of passing the output of the $j^{th}$ neuron of the $i^{th}$ layer through the activation function, $\\sigma_i$.\n",
    "\n",
    "i.e., \n",
    "\n",
    "$$a_{ij} = \\sigma_i(z_{ij})$$\n",
    "$$z_{ij} = \\boldsymbol{x} \\cdot \\boldsymbol{w_j} + b_{ij}$$\n",
    "\n",
    "A change in $w_{11}$ does bring about a change in the loss function (provided we don't have a dead neuron, more on this next week), but not in a direct sense. Changing $w_{11}$ causes a change in $z_{11}$, which in turn causes a change in $a_{11}$, which changes $z_{21}$ etc...\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "In order, then, to calculate a gradient for this parameter, we need to use the chain rule.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial a_{21}} \n",
    "                                       \\frac{\\partial a_{21}}{\\partial z_{21}}\n",
    "                                       \\frac{\\partial z_{21}}{\\partial a_{11}}\n",
    "                                       \\frac{\\partial a_{11}}{\\partial z_{11}}\n",
    "                                       \\frac{\\partial z_{11}}{\\partial w_{11}}$$\n",
    "\n",
    "</div> \n",
    "\n",
    "We can see here how we traverse back through the network, hence the name **backpropagation** and the name of the `backward()` method in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a713a",
   "metadata": {},
   "source": [
    "In summary, to get the rate of change of the loss function with respect to a parameter, we need to know how, in turn, it changes with respect to everything which precedes it. This leads to chains of derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3d9c6",
   "metadata": {},
   "source": [
    "See the appendix for how PyTorch calculates gradients using the [backward()](#backward_app) function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e259df7",
   "metadata": {},
   "source": [
    "## Generate New Dataset\n",
    "\n",
    "Now we will generate a dataset which is not linearly separable, and hence requires slightly more 'heavy machinery' than the single layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cba19-8979-47e8-8a60-8ab7b1d9b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_gaussian_quantiles(mean=None, \n",
    "    cov=0.7, n_samples=n_samples, n_features=2, n_classes=2, \n",
    "    shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdca28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=Y, s=20, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a6954-74a3-4577-acf6-2cd1630a369c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "- Construct your new neural network with a 30-neuron hidden layer using the convenient `nn.sequential` functionality, using the tips in the code cell below:\n",
    "    \n",
    "- Print the model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfc4e2f",
   "metadata": {},
   "source": [
    "Note: You are asked below to use another type of activation function called ReLU. It has the form $ f(x) = \\max(0,x) =  \\begin{cases}\n",
    "x, & x > 0 \\\\\n",
    "0, & x \\leq 0\n",
    "\\end{cases}$\n",
    "\n",
    "In PyTorch, we access it using `nn.ReLU`.\n",
    "\n",
    "The specific use cases for ReLU and other activation functions are covered next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=2 # two dimensional input\n",
    "hidden_layer=30 # hidden layer\n",
    "output_size=1 # one output\n",
    "\n",
    "model = nn.Sequential( nn.Linear(input_size, hidden_layer), nn.ReLU(), nn.Linear(hidden_layer,1), nn.Sigmoid())\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521a502",
   "metadata": {},
   "source": [
    "The same as before, we can test that the model gives us outputs of the correct shape. Currently our model is untrained, so the values in the output below do not have much meaning at this stage.\n",
    "\n",
    "Inputting a datapoint with 2 arbitrary features should return a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97835720",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor([0.0,0.0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17728145",
   "metadata": {},
   "source": [
    "# Training the Multilayer Network [^](#index) <a id='trainmulti'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf203d6",
   "metadata": {},
   "source": [
    "A key advantage of PyTorch is that the training process stays largely the same regardless of the size of your network. All of the steps below were necessary for the previous single neuron example, and we have no extra work to do here. Using PyTorch is extremely scalable in this sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816582f6",
   "metadata": {},
   "source": [
    "To be ready to train our network - as we did last time - we need to define our optimizer, prepare our dataloader, implement a training function and visualise our decision boundaries (not necessary to train out model, but we are doing it here).\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Define the same loss function as we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ea8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565c244",
   "metadata": {},
   "source": [
    "Define the optimizer to use for this training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4971c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) #lr = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532959bc",
   "metadata": {},
   "source": [
    "Transform our data into the correct form, then instatiate the DataLoader class, to be used as our iterator in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtorch = torch.tensor(X).float()\n",
    "Ytorch = torch.tensor(Y).view(len(Y),1).float()\n",
    "D=torch.utils.data.TensorDataset(Xtorch,Ytorch)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( D , batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325d31d",
   "metadata": {},
   "source": [
    "Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c688278",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(n): # train for this many epochs\n",
    "    for i in range(n):\n",
    "        totalloss=0\n",
    "        for Xtorch,Ytorch in trainloader: # 1. pull a 'batch' of data from the loader\n",
    "            \n",
    "            y=model(Xtorch) # 2. forward pass through model\n",
    "            loss=loss_function(y, Ytorch) # 3. compute the loss\n",
    "            totalloss+=loss.detach() # detach method tells the machine not to track the gradient of this tensor\n",
    "            \n",
    "            loss.backward() # 4. calculate backward (gradient) pass\n",
    "            optimizer.step() # 5. use gradient info in optimise step\n",
    "        losses.append(totalloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e9068",
   "metadata": {},
   "source": [
    "Make a grid of predictions, for plotting the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d10652",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=400\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) \n",
    "predict=predict.reshape( (N,N) ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb7801",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) ).detach()\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be38c44",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34196dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\",(Ytorch == torch.round(model(Xtorch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df3d29",
   "metadata": {},
   "source": [
    "Again we can inspect the model parameters. Note the difference in number of parameters to our previous, smaller network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9012e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(Xtorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d2880",
   "metadata": {},
   "source": [
    "# Conclusion  <a id='conc'></a> [^](#index)\n",
    "\n",
    "This week we started by building a neuron from scratch, using no machine learning specific modules. We discovered the need for activation functions as part of a neuron because of the non-linearity they create.\n",
    "Moving onto PyTorch, we learned about the fundamental datatype, the tensor, which PyTorch works with. We found that we can just as easily create a single neuron binary classifier, using the PyTorch package. This approach turned out to be easily scalable to larger networks which can deal with more complicated datasets. It is worth noting that, whilst all of this analysis was carried out on 2-dimensional datasets, this was purely for ease of visualisation: real-life use cases tackle problems in many more dimensions. \n",
    "\n",
    "Next week, we look in more detail at the role of activation functions, when to use each type, and why choosing the correct one is very important. In the process, we will be building on what we have learned about PyTorch thus far, and applying it to even larger networks with one or more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce7cc9",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "# Appendix [^](#index) <a id='app'></a>\n",
    "    \n",
    "[Return to Index](#index)\n",
    "\n",
    "In this section, you will find longer pieces of mathematics and code which are **non-examinable**. Please read at your own discretion.\n",
    "\n",
    "- A [Coordinate Transformation](#coord)\n",
    "- B [PyTorch backward() Function](#backward_app) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35b577",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "# A: Coordinate Transformation [^^](#app) <a id='coord'></a>\n",
    "\n",
    "Previously, we saw how we need more neurons, in the form of a hidden layer, in order to deal with classification problems where are training data are less 'nicely separated'.\n",
    "\n",
    "An equally valid way to negotiate this issue is to perform a coordinate transformation on our data. This approach adds only a constant amount to our total computation time: we transform the data before our iterative training process, so there are no time-scaling issues. In fact, a reason to transform our training data would be to speed up the training process. If, in our new coordinate system, the data is again linearly separable, then we require a less complex model with fewer parameters and hence the training process is much less computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df2545a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "As before, we will go through the same process of generating and visualising data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1453c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "np.random.seed(0) # reproducible runs for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data as before\n",
    "\n",
    "X, Y = sklearn.datasets.make_gaussian_quantiles(mean=None, \n",
    "    cov=0.7, n_samples=n_samples, n_features=2, n_classes=2, \n",
    "    shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ffc6d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "Upon inspection, our data is not linearly separable in $(x,y)$ space, but it looks like our data may be linearly separable in $(r,\\theta)$ space: for $r<1$, almost all of the data belongs to the same category and for $r>1$, all belongs to the other category. \n",
    "\n",
    "We will inspect our data, and subsquently visualise it in $(r,\\theta)$ space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c71cb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=Y, s=20, cmap=plt.cm.Spectral)\n",
    "xcirc = np.linspace(1,-1,100)\n",
    "ycirc = np.sqrt(1-xcirc**2)\n",
    "plt.plot(xcirc, ycirc, color='green', label='$r=1$')\n",
    "plt.plot(xcirc, -ycirc, color='green')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00e120",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "The cell below performs the coordinate transformation:\n",
    "    \n",
    "In each row of the array, we have $[r, \\theta]$ for that datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtheta = np.array([[np.sqrt(x**2 + y**2), np.arctan2(y,x)] for x, y in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0c224",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "Now when we plot in the transformed space, we can see that the data looks linearly separable at approximately $r=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a927b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot transformed data\n",
    "plt.scatter(rtheta[:,0],rtheta[:,1], c=Y, s=10, cmap=plt.cm.Spectral)\n",
    "\n",
    "# overlay a line of constant r\n",
    "plt.plot([1,1],[-3.2, 3.2], color='green', label='$r=1$')\n",
    "plt.xlabel(r'$r$')\n",
    "plt.ylabel(r'$\\theta$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045ea07",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "The advantage of transforming our data is that we can revert back to our simple binary classifier consisting of a single neuron. This is much more computationally efficient and makes our model much faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12048d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "We will now see if the single neuron perceptron is capable of achieving what we did previously with a network consisting of a 30-neuron hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee22f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size=2 # two dimensional input\n",
    "output_size=1 # one output\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size,output_size), nn.Sigmoid())\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d71b1a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "Following the exact same process as before, we define our loss function, optimizer and our dataloader. We define a training function and use it in a for loop whilst plotting our updated decision boundary as the number of training epochs increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc026a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) #lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtheta_torch = torch.tensor(rtheta).float()\n",
    "Y_torch = torch.tensor(Y).view(len(Y),1).float()\n",
    "D=torch.utils.data.TensorDataset(rtheta_torch,Y_torch)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( D , batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(n): # train for this many epochs\n",
    "    for i in range(n):\n",
    "        totalloss=0\n",
    "        for rtheta_torch, Y_torch in trainloader: # 1. pull a 'batch' of from the loader\n",
    "            \n",
    "            y=model(rtheta_torch) # 2. forward pass through model\n",
    "            loss=loss_function(y, Y_torch) # 3. compute the loss\n",
    "            totalloss+=loss.detach() # detach method tells the machine not to track the gradient of this tensor\n",
    "            \n",
    "            loss.backward() # 4. calculate backward (gradient) pass\n",
    "            optimizer.step() # 5. use gradient info in optimise step\n",
    "        \n",
    "        losses.append(totalloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063bc2ee",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "Make a grid of predictions, for plotting the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04cda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=400\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) \n",
    "predict=predict.reshape( (N,N) ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e67cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(rtheta[:,0],rtheta[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) ).detach()\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(rtheta[:,0],rtheta[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6b1f5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "It looks like our model has been able to complete the same task as before, but using a much less computationally intensive model, by transforming our data.\n",
    "\n",
    "We will inspect our graph of loss and determine our final accuracy to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99ce27",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "    \n",
    "Finally, we will print the accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d69d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\",(Y_torch == torch.round(model(rtheta_torch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b02693",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "## B. PyTorch backward() Function [^^](#app) <a id='backward_app'></a>\n",
    "    \n",
    "[Return to Backpropagation](#backprop)     \n",
    "    \n",
    "_References: [tutorialspoint](https://www.tutorialspoint.com/how-to-compute-gradients-in-pytorch), PyTorch: [A Gentle Introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), Medium: Abishek Bashyall - [Playing with .backward() method in Pytorch](https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0)_\n",
    "    \n",
    "_For further explanation, click [here](https://stackoverflow.com/questions/57320830/why-torch-sum-before-doing-backward) and [here](https://stackoverflow.com/questions/57248777/backward-function-in-pytorch/57249287#57249287) to view two Stack Overflow discussions_\n",
    "    \n",
    "PyTorch is designed to create neural networks, where calculating the gradient in back propagation is vital. Therefore when we create a Torch tensor, we can specify if it is a variable from which we would like to calculate the derivatives. We do this by setting `requires_grad = True` (it is False by default). \n",
    "    \n",
    "To calculate the gradient of a function wrt this variable, we must then call the `autograd.backward()` (or just `backward()` function on our output. Two such examples are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print (x.grad) # Expect 2*x = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., ) # We would not need this gradient for back propagation\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(5.0, requires_grad = True)\n",
    "\n",
    "y = w * x + b\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Compute gradients by calling backward function for y\n",
    "y.backward()\n",
    "\n",
    "# Access and print the gradients w.r.t x, w, and b\n",
    "dx = x.grad\n",
    "dw = w.grad\n",
    "db = b.grad\n",
    "print(\"x.grad :\", dx) # We did not require a gradient for this variable\n",
    "print(\"w.grad :\", dw) # Expect x = 3\n",
    "print(\"b.grad :\", db) # Expect 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a3845",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "We can see what happens if we have a function act on another, for example by multiplying our input y by 2. Unlike w and b, we will be unable to perform y.grad as y is not a **leaf nodes**; it is calculated from a function of leaf node. Hence we will obtain an error. If we did want to find the gradient wrt y, we would use the `retain_grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15830351",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., ) # We would not need this gradient for back propagation\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(5.0, requires_grad = True)\n",
    "\n",
    "y = w * x + b\n",
    "\n",
    "# Uncomment the line below to retain the gradient\n",
    "#y.retain_grad()\n",
    "\n",
    "z = 2 * y\n",
    "\n",
    "z.backward()\n",
    "\n",
    "db = b.grad\n",
    "dw = w.grad\n",
    "dy = y.grad\n",
    "\n",
    "print(\"w.grad :\", dw) # Expect x = 6\n",
    "print(\"b.grad :\", db) # Expect 2\n",
    "print(\"y.grad :\", dy) # Expect 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd1149",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "By default, `backward()` is called on a scalar tensor since the function is unable to calculate non-scalar derivatives. If the output is not of this form, the code will not run. An example of this is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y_arr= x_arr ** 2 \n",
    "y_arr.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8582c1a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "    \n",
    "One way to get past this would be to simply create a scalar tensor that would make backwards() produce the required gradients - for example the sum of all the elements in our output vector: $\\Sigma y_i^{(j)}$. Since this sum is unweighted, this method can be described as that of _equal gradient flow_. Note that our output elements $y_i^{(j)}$ only depend on $x_i^{(j)}$:\n",
    "    \n",
    "\\begin{equation}    \n",
    "\\frac{\\partial}{\\partial x_k^{(l)}} \\sum y_i^{(j)} = \\sum \\frac{\\partial f(x_i^{(j)})}{\\partial x_k^{(l)}} = \\frac{\\partial f(x_k^{(l)})}{\\partial x_k^{(l)}}\n",
    "\\end{equation}    \n",
    "    \n",
    "`x.grad` would then produce a torch tensor containing all of these derivatives, as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1e953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "print (x)\n",
    "print ()\n",
    "\n",
    "# Imaging that squaring our data is the activation function\n",
    "# This would means the gradient should be given by 2*x\n",
    "out = x**2 \n",
    "out.sum().backward()\n",
    "\n",
    "\n",
    "print(x.grad) # This should be 2*input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1e002",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "    \n",
    "An alternative way to achieve our desired outcome is by setting the gradient argument in the backward() function. Understanding the principles behind why this works requires knowledge of PyTorch's autograd function. \n",
    "    \n",
    "First, imagine our function is of the form $\\boldsymbol{y} = f (\\boldsymbol{x})$, where **x** and **y** have size n and m respectively. \n",
    "   \n",
    "When we call `backward()`, we are actually calling the default `backward(gradient = torch.tensor[1.])`. We will refer to this gradient using $\\boldsymbol{v}$.. Autograd then performs the following calculations, where J is the **Jacobian**:\n",
    "    \n",
    "\\begin{equation}\n",
    "    J^T \\cdot \\boldsymbol{v} =\n",
    "                              \\begin{bmatrix} \n",
    "                              \\frac{\\partial y_1}{\\partial x_1} & \\dots  & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "                                    \\vdots & \\ddots & \\vdots\\\\\n",
    "                              \\frac{\\partial y_1}{\\partial x_n} & \\dots  & \\frac{\\partial y_m}{\\partial x_n} \n",
    "                              \\end{bmatrix} \\boldsymbol{v}\n",
    "\\end{equation}\n",
    "    \n",
    "If **x** and **y** were the same size, and $y_i$ only depended on $x_i$, we would just have a diagonal matrix. By setting **v** to be the same size as **x** and only contain ones, we would get the equation we were after. We can also see how the simple 1D equation works:\n",
    "    \n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix} \n",
    "          \\frac{\\partial y_1}{\\partial x_1} & \\dots  & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\dots  & \\frac{\\partial y_m}{\\partial x_m} \n",
    "    \\end{bmatrix} \n",
    "    \\begin{pmatrix} \n",
    "          1 \\\\ \\vdots \\\\ 1 \n",
    "    \\end{pmatrix} = \n",
    "                    \\begin{pmatrix} \n",
    "                          \\frac{\\partial y_1}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_m} \n",
    "                    \\end{pmatrix}\n",
    "\\end{equation}\n",
    "    \n",
    "If X and Y are matrices rather than vectors, then we can imagine the autograd function working on each column (or row) in turn. The gradient called by backward() must still have the same shape as the matrices X and Y. An example of this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "print (x)\n",
    "print ()\n",
    "\n",
    "out = x**2 \n",
    "out.backward(gradient = torch.ones_like(x))\n",
    "\n",
    "print(x.grad) # This should be 2*input values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('ML_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "dbebadd8b0ae22643bbf3bee6b94e9ec667c3e45a3331e61628f55230064ed19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
