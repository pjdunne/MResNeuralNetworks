{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b6f8e6",
   "metadata": {},
   "source": [
    "# Neural networks - Single Neuron as Binary Classifier (Worked Example) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8619c15",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [Activation Function](#activation)\n",
    "1. [Generating Data](#data)\n",
    "1. [Building a Neuron](#neuron)\n",
    "1. [Building a Training Function](#trainfunc)\n",
    "1. [Training](#training)\n",
    "1. [Visualise What We've Done](#visual)\n",
    "1. [Conclusion](#sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fc90f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Activation Function [^](#index) <a id='activation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70459a",
   "metadata": {},
   "source": [
    "Thus far in the course we have covered classical machine learning algorithms, most of which make use of linear combinations of the data fed to them. The scope of what we are able to achieve is widened when we add non-linearity to our algorithms, using activation functions.\n",
    "\n",
    "As you will discover, neurons in a neural network are arranged in layers. Not unlike in the human brain, a neuron receives signals of varying strength from other neurons, and essentially 'decides' whether this combined received signal is strong enough for the neuron to 'fire', and how strongly.\n",
    "\n",
    "This 'decision' element is replicated by the presence of the activation function in our neuron. All contributions from previous neurons to which it is connected are summed, and passed to the activation function, which then influences how strong a signal our neuron outputs, if any at all.\n",
    "\n",
    "Without this non-linear aspect to our neuron, it would simply output a linear combination of the data passed to it by previous neurons, which in turn also simply contain some linear combination of the data passed to them. We would find that the output of our algorithm is nothing but a linear combination of our input data, and we have therefore achieved nothing special. \n",
    "\n",
    "The real prediction power of neural networks stems precisely from the non-linearity brought about by the presence of activation functions and, as you will find out, there are several choices of activation function we can make, each of which have their own advantages and drawbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c84a25",
   "metadata": {},
   "source": [
    "Run the cell below first - it contains the relevant imports for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e622b7",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "One of the most common activation functions is the sigmoid function. Make your own below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ab0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    s = 1 / (1+np.exp(-v))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e872b",
   "metadata": {},
   "source": [
    "Now plot your sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-10,10)\n",
    "plt.plot(x,sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d5536-9df9-476d-861c-7d3a1b3eb464",
   "metadata": {},
   "source": [
    "You can run the code cell below to perform some basic checks that your sigmoid function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sigmoid(0.0) == 0.5 # zero of sigmoid should be 0.5\n",
    "assert sigmoid(10.0) - 0.9999 < 0.0005\n",
    "assert sigmoid(-10.0) < 0.0005\n",
    "# Does this run? Your sigmoid is hopefully OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aba13c",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Generating Data [^](#index) <a id='data'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdda84",
   "metadata": {},
   "source": [
    "We begin by generating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "n_samples=200\n",
    "\n",
    "\n",
    "X, Y = sklearn.datasets.make_classification(n_features=2, n_redundant=0, n_samples=n_samples,\n",
    "    n_informative=2, random_state=None, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2952673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [3,3] # 3x3 inch is good for a laptop screen; 5x5 for external monitor\n",
    "plt.rcParams['figure.dpi'] = 200 \n",
    "\n",
    "colors = sns.color_palette(\"tab10\", as_cmap=True)\n",
    "plt.scatter(X[:,0],X[:,1], c = Y, s=20, marker = 'x', cmap = colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002eccf1",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Building a Neuron [^](#index) <a id='neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc12063",
   "metadata": {},
   "source": [
    "In this section we will talk through the simplest possible example of a neural network - the single neuron perceptron. Neural networks used in industry and academia can consist of millions of neurons. In notebooks to come, we will begin to look at larger, deeper neural networks, and will gain an appreciation for how neurons fit together and interact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14ca2e",
   "metadata": {},
   "source": [
    "<img src=\"https://static.packt-cdn.com/products/9781788397872/graphics/bc193cf1-aeb4-432e-9f21-e86c1fd45160.png\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde2a21",
   "metadata": {},
   "source": [
    "(Image taken from https://static.packt-cdn.com/products/9781788397872/graphics/bc193cf1-aeb4-432e-9f21-e86c1fd45160.png, depicting a single layer perceptron with 3 input weights. Note that in our example we have only 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e72ba1",
   "metadata": {},
   "source": [
    "A single neuron consists of weight(s), a bias term (generally, but not in this case) and an activation function.\n",
    "Data which is fed into the neuron is multiplied by the weight (dot product).\n",
    "The result of this computation is passed through the activation function and output by the neuron.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Build your own neuron below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fac00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(X, w=[0.0,0.0]):\n",
    "    a=np.matmul(X,w) # product of data and the weights vector\n",
    "    y=sigmoid(a)     # activation function for non-linearity\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f9930",
   "metadata": {},
   "source": [
    "The cell below serves as a basic check that you have implemented your neuron function correctly. If you **don't** see an assertion error, you can move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neuron([0.0], w=[1.0]) == 0.5 # one weight of 1.0 = a neuron that is the sigmoid function\n",
    "assert neuron([0.5,0.5], w=[0.0,0.0])==0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804f21b",
   "metadata": {},
   "source": [
    "Now we can visualise the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8258c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xrange=np.linspace(-5,5,100)\n",
    "\n",
    "plt.plot(Xrange,neuron(np.transpose([Xrange]),w=[1.5])) # edit the weight to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430f718",
   "metadata": {},
   "source": [
    "In the example above, we have just an input layer and a single neuron output layer. We have 2 weights, which we store in a weights vector. In neural networks in general, we can have many layers consisting of many neurons, so higher dimensional objects are required to store the parameters of our model. In general, we would have a **matrix of weights** for each layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990559d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Building a Training Function [^](#index) <a id='trainfunc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acdd0a",
   "metadata": {},
   "source": [
    "Training your model works using the familiar gradient descent method, with the usual hyperparameters. \n",
    "\n",
    "$\\eta=$ learning rate\n",
    "\n",
    "$\\alpha=$ decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.1 # learning rate\n",
    "alpha=0.0 # decay parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e87029",
   "metadata": {},
   "source": [
    "We are using gradient descent, so we are going to require some gradients: that is, the rate of change of the loss function with respect to each parameter of the model. In this case, we have 2 parameters, the 2 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78daeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.array([0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b874ea",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Fill out variable a. a is what is passed into the activation function in the equation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006d7fb",
   "metadata": {},
   "source": [
    "$$y=\\sigma ( Xw ) $$\n",
    "\n",
    "where $\\sigma$ is our activation function, in this case the sigmoid function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ef521",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.matmul(X,w)\n",
    "np.shape(a) # it's always useful to keep track of the shape of your 'tensors' as you progress through the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2787c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "We then need to pass this to our activation function, in this case the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc33ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=sigmoid(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cfb26",
   "metadata": {},
   "source": [
    "We need to define a loss function that we wish to minimise - here we use the **squared error**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0b85c",
   "metadata": {},
   "source": [
    "For a single instance, the loss is given by:\n",
    "$$ l_i=(Y_i-y_i)^2 $$\n",
    "\n",
    "where $y_i$ is the output predicted by our model from datapoint $x_i$ and $Y_i$ is the true value for a given datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd5eff",
   "metadata": {},
   "source": [
    "So our total loss or cost, is given by:\n",
    "\n",
    "$$\\sum_i^n l_i = \\sum_i^n (Y_i-y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339ac8b",
   "metadata": {},
   "source": [
    "Thanks to the way we can manipulate arrays in numpy, we don't have to perform this calculation explicitly for each datapoint.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "Define the loss function. It should be an array called L consisting of the loss for each datapoint.\n",
    "</div>\n",
    "    \n",
    "We will later find the sum of this array as a measure of our 'badness of fit' which we wish to minimise in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8172d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = (Y-y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee61d9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The formula for the gradient is then:\n",
    "\n",
    "$$ gradient = \\begin{bmatrix} \n",
    "    \\frac{\\partial L}{\\partial w_1} \\\\ \n",
    "    \\frac{\\partial L}{\\partial w_2} \n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "            \\sum_i 2x_{i,1} y_i (y_i - Y_i)(1-y_i) \\\\ \n",
    "            \\sum_i 2x_{i,2} y_i (y_i - Y_i)(1-y_i) \\\\\n",
    "         \\end{bmatrix}$$\n",
    "\n",
    "Where $x_{i,1}$ is the $1^{st}$ feature of the $i^{th}$ data point; $y_i$ here represents $\\sigma(\\boldsymbol{x_i} \\cdot \\boldsymbol{w})$\n",
    "    \n",
    "</div>\n",
    "\n",
    "The gradient we want is the rate of change of the loss function with respect to our weights, \n",
    "\n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "         \\end{bmatrix}\n",
    "         \n",
    "It makes sense that our gradient is of the same shape as the weights vector. It is necessarily the case for us, as we will be subtracting multiples of the gradient from the weights vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251e468",
   "metadata": {},
   "source": [
    "Defining a new vector, $\\lambda$, as:\n",
    "\n",
    "$$\\lambda = \\begin{bmatrix}\n",
    "             2 y_1 (y_1 - Y_1)(1-y_1) \\\\\n",
    "             \\vdots \\\\\n",
    "             2  y_n (y_n - Y_n)(1-y_n) \\\\\n",
    "         \\end{bmatrix}$$\n",
    "\n",
    "We can express the gradient in terms of the $\\lambda$ vector and our X matrix:\n",
    "\n",
    "$$ gradient = X^T\\lambda $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa8fe1",
   "metadata": {},
   "source": [
    "**[We can sense-check this dimensionally: multiplying a (2xn) matrix ($X$) by a (nx1) vector ($\\lambda$) will give a (2x1) vector as expected.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb5f68",
   "metadata": {},
   "source": [
    "So we find the gradient and store it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 2*y*(y-Y)*(1-y)\n",
    "gradient = np.matmul(X.T,Lambda)\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff046457",
   "metadata": {},
   "source": [
    "Finally, we update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "w= w - eta * (gradient + alpha * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124e031",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Training [^](#index) <a id='training'></a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c848b",
   "metadata": {},
   "source": [
    "\n",
    "In the cells above, we completed one training step by computing products of the input data and the model weights, then passing to the activation function, calculating the error and updating our parameter values using the gradient. Much more convenient would be to have a function which completes one full training step.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47000c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Implement your training function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79baace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,w, eta=0.02, alpha=0.0):\n",
    "    a=np.matmul(X,w) # product of input data and weights\n",
    "    y=sigmoid(a) #Â activation function\n",
    "    L=(Y-y)**2 # loss function\n",
    "    Lambda = 2*y*(y-Y)*(1-y)\n",
    "    gradient = np.matmul(X.T, Lambda) # gradient of loss w.r.t. parameters # AH change ste back to gradient once done\n",
    "    w = w - eta * (gradient + alpha * w) # gradient descent - take step in direction of negative gradient\n",
    "    loss=sum(L) # find the sum of the array of losses\n",
    "    return(w,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d78a4a",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b141d",
   "metadata": {},
   "source": [
    "We can now use our train function in a for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900b6b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Experiment with different values of the eta and alpha hyperparameters and observe the effect on the training process using the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c4c0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w=np.array([0.1, 0.1]) # initial neuron weights\n",
    "\n",
    "weights=[]\n",
    "loss=[]\n",
    "for i in range(1,100): # run this many training steps\n",
    "    w,L=train(X,Y,w, eta=0.02, alpha=0.5)   # train\n",
    "    weights.append(w)  # keep track of the weights\n",
    "    loss.append(L)     # keep track of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fed04",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Visualise What We've Done [^](#index) <a id='visual'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aded3a",
   "metadata": {},
   "source": [
    "Plot neuron weights and loss function on the same axis, as a function of the training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(weights)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Neuron weights\")\n",
    "\n",
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daded822",
   "metadata": {},
   "source": [
    "Below is the final loss - loss is what we were trying to minimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f50b9",
   "metadata": {},
   "source": [
    "The cell below visualises how the neural decision boundary changes as we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c573597",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.array([0.1, 0.1]) #initial neuron weights\n",
    "\n",
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=neuron(np.transpose(Xgrid2),w) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) )\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    w,loss=train(X,Y,w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5e3a1",
   "metadata": {},
   "source": [
    "In the plots above, we see mostly red and blue, denoting predictions of 1 or 0. The intermediate colours we can see, which are more visible before the model is trained, correspond to different values between 0 and 1 which we can interpret as probabilities in this case. The closer to 1 the value in a certain region is, the higher the probability that a data point which lies in that region belongs to the 1 category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b3d2e",
   "metadata": {},
   "source": [
    "Observe that the decision boundary is not perfect. It may be favourable for us to change the activation function, loss function, shape/size of neural network etc. - it depends on the problem at hand. We discuss these issues in greater detail in the notebooks which follow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d0bbb",
   "metadata": {},
   "source": [
    "# Conclusion [^](#index) <a id='sum'></a>\n",
    "\n",
    "In this notebook, we had our first look at a basic neural network consisting of just an input and an output layer. \n",
    "\n",
    "We were introduced to the importance of activation functions, and how they allow us to achieve more than traditional machine learning techniques (note that these techniques are still widely used and can be extremely powerful).\n",
    "\n",
    "We carried out most of the training steps by hand, with no help from any specialised machine-learning specific modules.\n",
    "\n",
    "In the next notebook, we begin to move onto larger networks, containing what are known as 'hidden layers' - these are the key to building deep neural networks, from which the term deep learning originates. \n",
    "\n",
    "We will achieve more with less code, using the powerful machine learning package, PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
