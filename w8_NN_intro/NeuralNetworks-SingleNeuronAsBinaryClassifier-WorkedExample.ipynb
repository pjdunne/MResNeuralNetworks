{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b6f8e6",
   "metadata": {},
   "source": [
    "# Neural networks - Single neuron as Binary Classifier (Worked Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8619c15",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [Setting Up Our Data](#data)\n",
    "1. [Activation Function](#activation)\n",
    "1. [Building a Neuron](#neuron)\n",
    "1. [Building a Training Function](#trainfunc)\n",
    "1. [Training](#training)\n",
    "1. [Visualise What We've Done](#visual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b028e836",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Setting Up Our Data [^](#index) <a id='data'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125d5ba",
   "metadata": {},
   "source": [
    "We begin by importing the relevant modules and generating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2) # This seed seems to get a nice gently overlapping dataset\n",
    "n_samples=200\n",
    "\n",
    "# Toy dataset with sklearn\n",
    "\n",
    "X, Y = sklearn.datasets.make_classification(n_features=2, n_redundant=0, n_samples=n_samples,\n",
    "    n_informative=2, random_state=None, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097be6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [3,3] # 3x3 inch is good for a laptop screen; 5x5 for external monitor\n",
    "plt.rcParams['figure.dpi'] = 200 \n",
    "\n",
    "colors = sns.color_palette(\"tab10\", as_cmap=True)\n",
    "plt.scatter(X[:,0],X[:,1], c = Y, s=20, marker = 'x', cmap = colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415fc90f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Activation Function [^](#index) <a id='activation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70459a",
   "metadata": {},
   "source": [
    "Thus far in the course we have covered classical machine learning algorithms, most of which make use of linear combinations of the data fed to them. The scope of what we are able to achieve is widened when we add non-linearity to our algorithms, using activation functions.\n",
    "\n",
    "As you will discover, neurons in a neural network are arranged in layers. Not unlike in the human brain, a neuron receives signals of varying strength from other neurons, and essentially 'decides' whether this combined received signal is strong enough for the neuron to 'fire', and how strongly.\n",
    "\n",
    "This 'decision' element is replicated by the presence of the activation function in our neuron. All contributions from previous neurons to which it is connected are summed, and passed to the activation function, which then influences how strong a signal our neuron outputs, if any at all.\n",
    "\n",
    "Without this non-linear aspect to our neuron, it would simply output a linear combination of the data passed to it by previous neurons, which in turn also simply contain some linear combination of the data passed to them. We would find that the output of our algorithm is nothing but a linear combination of our input data, and we have therefore achieved nothing special. \n",
    "\n",
    "The real prediction power of neural networks stems precisely from the non-linearity brought about by the presence of activation functions and, as you will find out, there are several choices of activation function we can make, each of which have their own advantages and drawbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce90a41",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "One of the most common activation functions is the sigmoid function. Make your own below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ab0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    s = 1 / (1+np.exp(-v))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your Sigmoid function \n",
    "x=np.linspace(-10,10)\n",
    "plt.plot(x,sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d5536-9df9-476d-861c-7d3a1b3eb464",
   "metadata": {},
   "source": [
    "You can run the code cell below to perform some basic checks that your sigmoid function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f0138",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sigmoid(0.0) == 0.5 # zero of sigmoid should be 0.5\n",
    "assert sigmoid(10.0) - 0.9999 < 0.0005\n",
    "assert sigmoid(-10.0) < 0.0005\n",
    "# Does this run? Your sigmoid is hopefully OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002eccf1",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Building a Neuron [^](#index) <a id='neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e14ca2e",
   "metadata": {},
   "source": [
    "<img src=\"https://static.packt-cdn.com/products/9781788397872/graphics/bc193cf1-aeb4-432e-9f21-e86c1fd45160.png\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e72ba1",
   "metadata": {},
   "source": [
    "A single neuron consists of weight(s), a bias term (normally but not in this\n",
    "case) and an activation function.\n",
    "Data which is fed into the neuron is multiplied by the weight (dot product).\n",
    "The result of this computation is passed through the activation function and output by the neuron.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Build your own neuron below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fac00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(X, w=[0.0,0.0]):\n",
    "    a=np.matmul(X,w) # product of data and the weights vector\n",
    "    y=sigmoid(a)     # activation function for non-linearity\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neuron([0.0], w=[1.0]) == 0.5 # one weight of 1.0 = a neuron that is the sigmoid function\n",
    "assert neuron([0.5,0.5], w=[0.0,0.0])==0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8258c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the neuron\n",
    "Xrange=np.linspace(-5,5,100)\n",
    "\n",
    "plt.plot(Xrange,neuron(np.transpose([Xrange]),w=[1.5])) # edit the weight to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990559d",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Building a Training Function [^](#index) <a id='trainfunc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35acdd0a",
   "metadata": {},
   "source": [
    "Training your model works using the familiar gradient descent method, hence we require some gradients: that is, the rate of change of the loss function with respect to each parameter of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.1 # learning rate\n",
    "alpha=0.0 # decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78daeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight matrix should look something like...\n",
    "w=np.array([0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b874ea",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Fill out variable a. a is what is passed into the activation function in the equation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006d7fb",
   "metadata": {},
   "source": [
    "$$y=\\sigma ( X*w ) $$\n",
    "\n",
    "where $*$ represents matrix multiplication here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ef521",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.matmul(X,w)\n",
    "np.shape(a) # it's always useful to keep track of the shape of your 'tensors' as you progress through the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2787c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "We then need to pass this to our non-linear function, in this case the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc33ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=sigmoid(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cfb26",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "We need to define an error - here we use the absolute difference (L1 norm) between prediction and observation, given by $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0b85c",
   "metadata": {},
   "source": [
    "$$ e=|Y-y| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8172d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=Y-y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee61d9",
   "metadata": {},
   "source": [
    "The formula for the gradient is then:\n",
    "\n",
    "$$ gradient = - X^T*e $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d6ce1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Implement this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = - np.matmul(np.transpose(X),e)\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff046457",
   "metadata": {},
   "source": [
    "Finally we update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "w= w - eta * (gradient + alpha * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124e031",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Training [^](#index) <a id='training'></a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c848b",
   "metadata": {},
   "source": [
    "In the cells above, we completed one training step by computing dot products with the input data and the model weights, then passing to the activation function, calculating the error and updating our parameter values using the gradient. Much more convenient would be to have a function which completes one full training step.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47000c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Implement your training function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79baace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,w, eta=0.02, alpha=0.0):\n",
    "    a=np.matmul(X,w)\n",
    "    y=sigmoid(a)\n",
    "    e=Y-y\n",
    "    gradient= -np.matmul(np.transpose(X),e)\n",
    "    w=w-eta*(gradient+alpha*w)\n",
    "    loss=sum(abs(e)) # overall absolute loss\n",
    "    return(w,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d78a4a",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b141d",
   "metadata": {},
   "source": [
    "We can now use our train function in a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900b6b6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Experiment with different values of the eta and alpha hyperparameters and observe the effect on the training process using the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.array([0.1, 0.1]) # initial neuron weights\n",
    "\n",
    "weights=[]\n",
    "loss=[]\n",
    "for i in range(1,100): # run this many training steps\n",
    "    w,e=train(X,Y,w, eta=0.02, alpha=0.5)   # train\n",
    "    weights.append(w)  # keep track of the weights\n",
    "    loss.append(e)     # keep track of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fed04",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Visualise What We've Done [^](#index) <a id='visual'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neuron weights, and loss on the same axis, versus training epoch\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(weights)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Neuron weights\")\n",
    "\n",
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced9bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final loss - this is the thing to minimise!\n",
    "loss[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a grid of predictions, for plotting the decision boundary\n",
    "N=100\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=neuron(np.transpose(Xgrid2),w) # re-using our neuron function from earlier\n",
    "predict=predict.reshape( (N,N) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4366d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c573597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function visualises the neural decision boundary as you train\n",
    "w=np.array([0.1, 0.1]) #initial neuron weights\n",
    "\n",
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=neuron(np.transpose(Xgrid2),w) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) )\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    w,loss=train(X,Y,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c07a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
