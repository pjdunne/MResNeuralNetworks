{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b6f8e6",
   "metadata": {},
   "source": [
    "# Neural networks - Single Neuron as Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd733b",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [Activation Function](#activation)\n",
    "1. [Generating Data](#data)\n",
    "1. [Building a Neuron](#neuron)\n",
    "1. [Building a Training Function](#trainfunc)\n",
    "1. [Training](#training)\n",
    "1. [Visualise What We've Done](#visual)\n",
    "1. [Summary](#sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0607d-c6e2-495a-bfa0-0b83f9b0f627",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Activation Function [^](#index) <a id='activation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479d8b8-fc69-4910-b931-ff721aa767a8",
   "metadata": {},
   "source": [
    "Thus far in the course we have covered classical machine learning algorithms, most of which make use of linear combinations of the data fed to them. The scope of what we are able to achieve is widened when we add non-linearity to our algorithms, using activation functions.\n",
    "\n",
    "As you will discover, neurons in a neural network are arranged in layers. Not unlike in the human brain, a neuron receives signals of varying strength from other neurons, and essentially 'decides' whether this combined received signal is strong enough for the neuron to 'fire', and how strongly.\n",
    "\n",
    "This 'decision' element is replicated by the presence of the activation function in our neuron. All contributions from previous neurons to which it is connected are summed, and passed to the activation function, which then influences how strong a signal our neuron outputs, if any at all.\n",
    "\n",
    "Without this non-linear aspect to our neuron, it would simply output a linear combination of the data passed to it by previous neurons, which in turn also simply contain some linear combination of the data passed to them. We would find that the output of our algorithm is nothing but a linear combination of our input data, and we have therefore achieved nothing special. \n",
    "\n",
    "The real prediction power of neural networks stems precisely from the non-linearity brought about by the presence of activation functions and, as you will find out, there are several choices of activation function we can make, each of which have their own advantages and drawbacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609d331-8a93-4dc0-8660-7d3b358e3ebc",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "One of the most common activation functions is the sigmoid function. Make your own below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e19508",
   "metadata": {},
   "source": [
    "Run the cell below first - it contains the relevant imports for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423061d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ab0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(v):\n",
    "    s= ...\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee60d1",
   "metadata": {},
   "source": [
    "Now plot your sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce901981",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-10,10)\n",
    "plt.plot(x,sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0e9b8",
   "metadata": {},
   "source": [
    "You can run the code cell below to perform some basic checks that your sigmoid function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f36a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sigmoid(0.0) == 0.5 # zero of sigmoid should be 0.5\n",
    "assert sigmoid(10.0) - 0.9999 < 0.0005\n",
    "assert sigmoid(-10.0) < 0.0005\n",
    "# Does this run? Your sigmoid is hopefully OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8479f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Generating Data [^](#index) <a id='data'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae697e9",
   "metadata": {},
   "source": [
    "We begin by importing the relevant modules, and generating a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dcbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2) \n",
    "n_samples=200\n",
    "\n",
    "X, Y = sklearn.datasets.make_classification(n_features=2, n_redundant=0, n_samples=n_samples,\n",
    "    n_informative=2, random_state=None, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997068eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [4,2] \n",
    "plt.rcParams['figure.dpi'] = 200 \n",
    "\n",
    "colors = sns.color_palette(\"tab10\", as_cmap=True)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c = Y, s=20, marker = 'x', cmap = colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2817510-782a-48d1-aa59-19ad440264be",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Building a Neuron [^](#index) <a id='neuron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ebc8cb-800f-44ea-b481-301d963a7ea4",
   "metadata": {},
   "source": [
    "<img src=\"https://static.packt-cdn.com/products/9781788397872/graphics/bc193cf1-aeb4-432e-9f21-e86c1fd45160.png\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afa521",
   "metadata": {},
   "source": [
    "(Image taken from https://static.packt-cdn.com/products/9781788397872/graphics/bc193cf1-aeb4-432e-9f21-e86c1fd45160.png, depicting a single layer perceptron with 3 input weights. Note that in our example we have only 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376ac4b-1913-44f5-b9e0-719998776189",
   "metadata": {},
   "source": [
    "A single neuron consists of weight(s), a bias term (generally, but not in this\n",
    "case) and an activation function.\n",
    "Data which is fed into the neuron is multiplied by the weight (dot product).\n",
    "The result of this computation is passed through the activation function and output by the neuron.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Build your own neuron below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(X, w=[0.0,0.0]):\n",
    "    a=... # product of data and the weights vector\n",
    "    y=... # activation function for non-linearity\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85163e07",
   "metadata": {},
   "source": [
    "The cell below serves as a basic check that you have implemented your neuron function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert neuron([0.0], w=[1.0]) == 0.5 # one weight of 1.0 = an neuron that is the sigmoid function\n",
    "assert neuron([0.5,0.5], w=[0.0,0.0])==0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe1c37",
   "metadata": {},
   "source": [
    "Now we can visualise the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f80fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xrange=np.linspace(-5,5,100)\n",
    "\n",
    "plt.plot(Xrange,neuron(np.transpose([Xrange]),w=[1.5])) # edit the weight to see what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a112f8-f396-4005-b093-da96d6967ba2",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Building a Training Function [^](#index) <a id='trainfunc'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5acd39-2b43-4429-b289-24f5e09e1c1e",
   "metadata": {},
   "source": [
    "Training your model works using the familiar gradient descent method, with the usual hyperparameters. \n",
    "\n",
    "$\\eta=$ learning rate\n",
    "\n",
    "$\\alpha=$ decay parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944545a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta=0.1 # learning rate\n",
    "alpha=0.0 # decay parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c46a87",
   "metadata": {},
   "source": [
    "Hence we require some gradients: that is, the rate of change of the loss function with respect to each parameter of the model. In this case, we have 2 parameters, the 2 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.array([0.5,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fcb78-c837-4add-87eb-a98936949819",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Fill out variable a. a is what is passed into the activation function in the equation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260bf6e",
   "metadata": {},
   "source": [
    "$$y=\\sigma ( X * w ) $$\n",
    "\n",
    "where $\\sigma$ is our activation function, in this case the sigmoid function. \n",
    "\n",
    "$*$ represents matrix multiplication here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ef521",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= ...\n",
    "np.shape(a) # it's always useful to keep track of the shape of your 'tensors' as you progress through the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e704a79-f1af-4ebe-835d-a0b844f2d6bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "We then need to pass this to our activation function, in this case the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc33ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c453c-c193-4646-b678-3745b9d84789",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "We need to define a loss function that we wish to minimise - here we use the absolute difference (L1 norm) between prediction and observation, given by $e$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c709a",
   "metadata": {},
   "source": [
    "$$ e=|Y-y| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8172d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "e= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6d6eb",
   "metadata": {},
   "source": [
    "# TO BE CHANGED\n",
    "\n",
    "The formula for the gradient is then:\n",
    "\n",
    "$$ gradient = - X^T*e $$\n",
    "\n",
    "The gradient we want is the rate of change of the loss function with respect to the parameters stored in vector $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c370a-84bc-43ea-8eec-84266221280d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Implement this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient= ...\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c46f2-551e-4c5d-8adb-d63ce2c7e23b",
   "metadata": {},
   "source": [
    "Finally we update our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84236ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - eta * (gradient + alpha * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f16e5-0b75-4cd1-903d-c881f8339f15",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Training [^](#index) <a id='training'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d3db48",
   "metadata": {},
   "source": [
    "In the cells above, we completed one training step by computing products of the input data and the model weights, then passing to the activation function, calculating the error and updating our parameter values using the gradient. Much more convenient would be to have a function which completes one full training step.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba9f66-38df-4c9e-904a-4b1ed0b3c5e1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Implement your training function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79baace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,w, eta=0.05, alpha=0.0):\n",
    "    a= ... # product of input data and weights\n",
    "    y= ... # activation function\n",
    "    e= ... # loss function\n",
    "    gradient= ... # gradient of loss w.r.t. parameters\n",
    "    w= ... # gradient descent - take step in direction of negative gradient\n",
    "    loss=sum(abs(e)) # find the sum of the array of losses\n",
    "    return(w,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6073d0",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e39557-f787-4622-8446-8834869b697b",
   "metadata": {},
   "source": [
    "We can now use our train function in a `for` loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a11eb3-fe5d-4798-87f0-1558185ca04e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\"\n",
    "\n",
    "Experiment with different values of the eta and alpha hyperparameters and observe the effect on the training process using the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.array([0.1, 0.1]) # initial neuron weights\n",
    "\n",
    "weights=[]\n",
    "loss=[]\n",
    "for i in range(1,100): # run this many training steps\n",
    "    w,e=train(X,Y,w)   # train\n",
    "    weights.append(w)  # keep track of the weights\n",
    "    loss.append(e)     # keep track of the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663540e3",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Visualise what we've done [^](#index) <a id='visual'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1875c",
   "metadata": {},
   "source": [
    "Plot neuron weights and loss function on the same axis, as a function of the training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd55313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "\n",
    "plt.plot(weights)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Neuron weights\")\n",
    "\n",
    "plt.subplot(212)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d46762",
   "metadata": {},
   "source": [
    "Below is the final loss - loss is what we were trying to minimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12233628",
   "metadata": {},
   "source": [
    "Below we plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a grid of predictions, for plotting the decision boundary\n",
    "N=100\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=neuron(np.transpose(Xgrid2),w) # re-using our neuron function from earlier\n",
    "predict=predict.reshape( (N,N) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04577e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa288ce8",
   "metadata": {},
   "source": [
    "The cell below visualises how the neural decision boundary changes as we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w=np.array([0.1, 0.1]) #initial neuron weights\n",
    "\n",
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=neuron(np.transpose(Xgrid2),w) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) )\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    w,loss=train(X,Y,w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6f317",
   "metadata": {},
   "source": [
    "# Summary [^](#index) <a id='sum'></a>\n",
    "\n",
    "In this notebook, we had our first look at a basic neural network consisting of just an input and an output layer. \n",
    "\n",
    "We were introduced to the importance of activation functions, and how they allow us to achieve more than traditional machine learning techniques (note that these techniques are still widely used and can be extremely powerful).\n",
    "\n",
    "We carried out most of the training steps by hand, with no help from any specialised machine-learning specific modules.\n",
    "\n",
    "In the next notebook, we begin to move onto larger networks, containing what are known as 'hidden layers' - these are the key to building deep neural networks, from which the term deep learning originates. \n",
    "\n",
    "We will achieve more with less code, using the powerful machine learning package, PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
