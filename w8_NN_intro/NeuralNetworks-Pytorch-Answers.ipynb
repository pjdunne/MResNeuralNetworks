{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae2fbe0",
   "metadata": {},
   "source": [
    "# Neural Networks with PyTorch - Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbd152",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [PyTorch](#pytorch)\n",
    "1. [Generating Data and Setting Up](#setup)\n",
    "1. [Binary Classifier in PyTorch](#binary)\n",
    "1. [Training](#training)\n",
    "1. [Multilayer Perceptron](#multi)\n",
    "1. [Training the Multilayer network](#trainmulti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707650e3",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# PyTorch [^](#index) <a id='pytorch'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c36f24",
   "metadata": {},
   "source": [
    "Welcome to the wonderful world of PyTorch! \n",
    "\n",
    "There are a lot of sharp edges, but PyTorch is still arguably the most seamless package for building neural networks.\n",
    "\n",
    "[Documentation](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf1035",
   "metadata": {},
   "source": [
    "We will firstly import `torch` and `torch.nn`. Note that module to import is not called pytorch - just torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(31337) # for reproducible runs, manually set the PRNG seed (pseudo-random number generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5536444a",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "\n",
    "PyTorch works exclusively with its own datatype similar to a numpy array, known as a **tensor**.\n",
    "Tensors are all encompassing, in the sense that data structures of any dimension can be tensors.\n",
    "\n",
    "- Scalars  - Rank 0 Tensors\n",
    "- Vectors  - Rank 1 Tensors\n",
    "- Matrices - Rank 2 Tensors etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfde88",
   "metadata": {},
   "source": [
    "The simplest way to create a tensor is to use `torch.tensor()` and pass in data in the same way you would with a numpy array.\n",
    "\n",
    "Tensors, similar to numpy arrays, **can only store a single datatype**. This means that the type stored in a tensor is a property of that object.\n",
    "\n",
    "When you create a tensor, you can specify the datatype it stores using the `dtype` keyword argument. The documentation for PyTorch tensors has a [list of datatypes](https://pytorch.org/docs/stable/tensors.html#data-types)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c1e5e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "Create a tensor of any shape without specifying datatype. What is the default datatype? Does it depend on the type you use to create the tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa15e4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Check the type of data contained using `.dtype` and the type of tensor using `.type()`. Note the brackets after the type attribute. \n",
    "\n",
    "</div>\n",
    "\n",
    "The documentation linked above also contains details on types of tensors themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdad75cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.0,1.0]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25a09510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(0)\n",
    "x.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb26602",
   "metadata": {},
   "source": [
    "PyTorch provides methods to create empty, all 0s, all 1s and random-valued tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd02f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = torch.empty(3,5)\n",
    "ran = torch.rand(2,4)\n",
    "zeros = torch.zeros(2,3,4)\n",
    "ones = torch.ones(6)\n",
    "\n",
    "print(f'empty tensor with shape {empty.shape}:\\n\\n{empty}\\n')\n",
    "print(f'random tensor with shape {ran.shape}:\\n\\n{ran}\\n')\n",
    "print(f'zeros tensor with shape {zeros.shape}:\\n\\n{zeros}\\n')\n",
    "print(f'ones tensor with shape {ones.shape}:\\n\\n{ones}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c484ebb",
   "metadata": {},
   "source": [
    "An important keyword we can specify is `requires_grad`. The default value is false. This must be set to true for tensors which are involved in the training process. Recall that the computation of gradients is a key part of the optimization process using gradient descent, or any other optimizer. More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "086abfd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([8,9]).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84f7c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "Those who have a GPU on their computer can use the cell below to check whether CUDA is available. Read about CUDA [here](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have GPU acceleration?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d5e3e",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "The first thing we will build is a [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). Data which is fed into a linear layer is linearly transformed, but not passed through an activation function.\n",
    "\n",
    "To do this, we must specify the number of input 'legs' and output 'legs'.\n",
    "\n",
    "`nn.linear` will generate random weights and biases (here specified by our random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cc8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=2 # the number of input legs\n",
    "n=1 # the number of output legs\n",
    "\n",
    "linearlayer = nn.Linear(m,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f6dff",
   "metadata": {},
   "source": [
    "We can call the function on a rank 1 tensor of length 2 (since there are 2 input legs), which should produce a rank 0 tensor, a.k.a. a scalar (since there is just 1 output leg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearlayer(torch.Tensor([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4306397",
   "metadata": {},
   "source": [
    "Above, we passed 1 instance of data which has 2 features, from which our linear layer produced 1 output.\n",
    "We'll now pass 20 instances of data, so our output should have shape (20,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fc6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt=torch.randn(20, m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b434946",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearlayer(rt).shape # pass through the linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8248720",
   "metadata": {},
   "source": [
    "### Inspecting the model\n",
    "\n",
    "We can look at the weights and the biases of our model.\n",
    "The model is an object, so has both data and method attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed95366",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W\", linearlayer.weight)\n",
    "print(\"b\", linearlayer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f243fc",
   "metadata": {},
   "source": [
    "You will notice the `requires_grad` keyword here. This must be set to True for a tensor if we are going to require the gradient to be calculated at any point in its existence. For weights and biases in a neural network, we definitely need this to be True, as the training process requires computation of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0998f4",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Generating Data and Setting Up [^](#index) <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3beeff",
   "metadata": {},
   "source": [
    "We will again begin by importing relevant modules, setting the plotting parameters for the notebook and generating a toy dataset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn, sklearn.datasets\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "plt.rcParams['figure.dpi'] = 200 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc3fcd",
   "metadata": {},
   "source": [
    "Generating toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "np.random.seed(0) # reproducible runs for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de7294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our 'easy', linearly separable classification\n",
    "X,Y=sklearn.datasets.make_classification(n_features=2, n_redundant=0, n_samples=n_samples,\n",
    "    n_informative=2, random_state=None, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=Y, s=20, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e4f72",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Binary Classifier in PyTorch [^](#index) <a id='binary'></a> \n",
    "\n",
    "\n",
    "We will now recreate the binary classifier we made previously, now we have the full power of Pytorch to train it efficiently.\n",
    "\n",
    "We need a model which consists of the linear layer from above, but with a Sigmoid activation function. When building a model, `nn.Sequential` allows you to specify all the layers and activation functions in order. E.g., below we pass out linear layer followed by the sigmoid function. The output from one layer becomes the input to the next one. The convenience of this feature becomes more apparent as the networks you build get bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=2 # two dimensional input\n",
    "output_size=1 # one output\n",
    "\n",
    "# Output from one activation function becomes the input for the next, chained together by forward() functions\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size,output_size), nn.Sigmoid())\n",
    "\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dce883",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "There is an alternative way to build arbitrary models in \n",
    "PyTorch by constructing a Python class, but this is considerably more complicated, involves a lot of \n",
    "object-orientated boilerplate code, and isn't necessary for \n",
    "simple architectures such as these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6e22c",
   "metadata": {},
   "source": [
    "As before with our linear layer, we input 2 arbitrary values, which should generate a scalar output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor([0.0,0.0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4063dc08",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Training [^](#index) <a id='training'></a> \n",
    "\n",
    "We will now train our neural network. For this, we need to define a loss function.\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "See [the documentation from pytorch on loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to find an applicable loss function and implement it below.\n",
    "How does this compare to the self-coded loss function from the previous notebook?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e777a",
   "metadata": {},
   "source": [
    "When building a neural network in PyTorch, we need to define an optimizer. Recall that the training process often boils down to a multivariate minimisation problem - there are many pre-built algorithms which can carry this out for us efficiently. See the course glossary for a description of schocastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) # experiment with different learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75689252",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader\n",
    "\n",
    "Here, we make use of [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) (very commonly used) and `TensorDataset`. You can read about them using the links attached, but the purpose of these objects, particularly the DataLoader, is to combine the various steps associated with loading in, processing and iterating through data.\n",
    "\n",
    "The DataLoader object is an iterator, making it very well suited to the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeba7b3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "Read the docmentation for the DataLoader object, particularly the meaning of each keyword argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cee7d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "For larger scale applications of machine learning which make use of GPUs, the DataLoader object is beneficial as it shuffles data around which means the data is fed more efficiently to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6f4b1",
   "metadata": {},
   "source": [
    "In order to work with our generated datasets using PyTorch, we need them to be Tensor objects so we change the datatype below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtorch = torch.tensor(X).float()\n",
    "Ytorch = torch.tensor(Y).view(len(Y),1).float()\n",
    "D=torch.utils.data.TensorDataset(Xtorch,Ytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f3a96",
   "metadata": {},
   "source": [
    "For stochastic gradient descent, we use **batches** of data. We can specify `batch_size` for DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a6402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader( D , batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba22090",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now we're ready to train our model. As before, we will define a function which performs one full training loop. We can make this function take the number of epochs, `n`, as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc087d5",
   "metadata": {},
   "source": [
    "In PyTorch, the training process for one batch in one epoch is generally as follows:\n",
    "\n",
    ">1. Obtain batch from the DataLoader\n",
    ">1. Perform a forward pass, i.e., make a prediction on the data using the model\n",
    ">1. Compute the loss (how poor the prediction is) using the loss function\n",
    ">1. Compute gradients (known as the backward pass)\n",
    ">1. Use the gradients to perform a 'step' with the optimizer (hopefully toward the minimum of the loss function)\n",
    "\n",
    "This is then repeated for all batches (in the case where we are using SGD as our optimizer) and the whole process is performed $n$ times for $n$ epochs.\n",
    "\n",
    "The best way to see how to achieve this is by observing an example. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4442d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "losses = []\n",
    "\n",
    "def train(n): # train for this many epochs\n",
    "    for i in range(n):\n",
    "        totalloss=0\n",
    "        for Xtorch,Ytorch in trainloader: # 1. pull a batch from the loader\n",
    "            \n",
    "            y=model(Xtorch) # 2. forward pass through model\n",
    "            loss=loss_function(y, Ytorch) # 3. compute the loss\n",
    "            totalloss+=loss.detach() # detach method tells the machine not to track the gradient of this tensor\n",
    "            \n",
    "            loss.backward() # calculate backward (gradient) pass\n",
    "            optimizer.step() # use gradient info in optimise step\n",
    "        \n",
    "        losses.append(totalloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ba9b6",
   "metadata": {},
   "source": [
    "Again we will visualise the decision boundary, so we can monitor how the model changes during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad850d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a grid of predictions, for plotting the decision boundary\n",
    "N=100\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "predict=predict.reshape( (N,N) ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffebaf8",
   "metadata": {},
   "source": [
    "In the cell below, we run through 25 iterations. In each iteration, we plot the decision boundary (by making our model output predictions for every point in a grid) and then complete one training step.\n",
    "\n",
    "If we did not want to visualise the decision boundary during the training process, we would simply have to run:\n",
    "\n",
    "```python\n",
    "train(25)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68848b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) ).detach()\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    train(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e5620",
   "metadata": {},
   "source": [
    "We can plot the loss as a function of the number of epochs. Whilst the above plot shows qualitatively that the model improves over the training process, the plot below shows it quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47226c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec21e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "What fraction of the training data do we correctly predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71534238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\",(Ytorch == torch.round(model(Xtorch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4b34",
   "metadata": {},
   "source": [
    "We can view all of the model's parameters using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d917c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe all parameters of model\n",
    "for name,param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5f26d",
   "metadata": {},
   "source": [
    "## Experiment with Manually Setting Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a960a27",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "The code block below allows us to maually set the weights of the model. Try and do this to correctly describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model[0].weight[0,0]=0\n",
    "    model[0].weight[0,1]=1\n",
    "    model[0].bias[0]=0\n",
    "    \n",
    "for name,param in model.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "# make a grid of predictions, for plotting the decision boundary\n",
    "N=100\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "predict=predict.reshape( (N,N) ).detach()\n",
    "\n",
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)\n",
    "\n",
    "# Accuracy - what fraction of the train data go we correctly predict?\n",
    "print(\"Accuracy\",(Ytorch == torch.round(model(Xtorch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfeac5",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Multilayer Perceptron [^](#index) <a id='multi'></a>\n",
    "\n",
    "From our work thus far, we can see that a single neuron is only capable of linearly separating the input data. \n",
    "\n",
    "Instead we can make a multi-layer perceptron, which goes beyond the capabilities of a single neuron. It also allows us to begin learning a representation of the data which reduces the dimensionality.\n",
    "\n",
    "This is where the term 'deep' learning comes from, once you have a deep network of many artificial neuron layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1d176",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Now that we have moved onto multilayer networks, it is worth introducing the concept of backpropagation. Previously, when we found the rate of change of the loss function with respect to a parameter of our model, it was straight-forward. Now our network has (an) extra layer(s), the derivative of the loss function with respect to a weight has a less simple form due to the interdependence of neurons in successive layers.\n",
    "\n",
    "Consider the following example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51546635",
   "metadata": {},
   "source": [
    "<img src=\"https://studymachinelearning.com/wp-content/uploads/2019/12/crop_NN.jpg\" width=\"450\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89129c00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Say we want to find the gradient of the loss function with respect to the weight $w_{11}$. Labelling our Loss as $L$, we want to find $$\\frac{\\partial L}{\\partial w_{11}}$$\n",
    "\n",
    "Note that we $a_{ij}$ is defined as the result of passing the output of the $j^{th}$ neuron of the $i^{th}$ layer through the activation function, $\\sigma_i$.\n",
    "\n",
    "i.e., \n",
    "\n",
    "$$a_{ij} = \\sigma_i(z_{ij})$$\n",
    "$$z_{ij} = \\boldsymbol{x} \\cdot \\boldsymbol{w_j} + b_{ij}$$\n",
    "\n",
    "A change in $w_{11}$ does bring about a change in the loss function (provided we don't have a dead neuron, more on this next week), but not in a direct sense. Changing $w_{11}$ causes a change in $z_{11}$, which in turn causes a change in $a_{11}$, which changes $z_{21}$ etc...\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "In order, then, to calculate a gradient for this parameter, we need to use the chain rule.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial a_{21}} \n",
    "                                       \\frac{\\partial a_{21}}{\\partial z_{21}}\n",
    "                                       \\frac{\\partial z_{21}}{\\partial a_{11}}\n",
    "                                       \\frac{\\partial a_{11}}{\\partial z_{11}}\n",
    "                                       \\frac{\\partial z_{11}}{\\partial w_{11}}$$\n",
    "    \n",
    "We can see here how we are almost traversing back, hence the name **backpropagation** and the name of the `backward()` method in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a713a",
   "metadata": {},
   "source": [
    "In summary, to get the rate of change of the loss function with respect to a parameter, we need to know how, in turn, it changes with respect to everything which preceeds it. This leads to chains of derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e259df7",
   "metadata": {},
   "source": [
    "## Generate New Dataset\n",
    "\n",
    "Now we will generate a dataset which is not linearly separable, and hence requires slightly more 'heavy machinery' than the single layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_gaussian_quantiles(mean=None, \n",
    "    cov=0.7, n_samples=n_samples, n_features=2, n_classes=2, \n",
    "    shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdca28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=Y, s=20, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78f39a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Construct your new neural network with a 30-neuron hidden layer using the convenient `nn.sequential` functionality, using the tips in the cell below:\n",
    "    \n",
    "Print the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b623f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=2 # two dimensional input\n",
    "hidden_layer=30 # hidden layer\n",
    "output_size=1 # one output\n",
    "\n",
    "model = nn.Sequential( nn.Linear(input_size, hidden_layer), nn.ReLU(), nn.Linear(30,1), nn.Sigmoid())\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521a502",
   "metadata": {},
   "source": [
    "The same as before, we can test that the model gives us outputs of the correct shape. Currently our model is untrained, so the values in the output below do not have much meaning at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97835720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should work! Just two arb. data points, should generate a scalar output\n",
    "model(torch.tensor([0.0,0.0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17728145",
   "metadata": {},
   "source": [
    "# Training the Multilayer Network [^](#index) <a id='trainmulti'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816582f6",
   "metadata": {},
   "source": [
    "To be ready to train our network - as we did last time - we need to define our optimzer, prepare our dataloader, implement a training function and visualise our decision boundaries (not necessary to train out model, but we are doing it here).\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Define the same loss function as we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ea8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4971c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02) #lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtorch = torch.tensor(X).float()\n",
    "Ytorch = torch.tensor(Y).view(len(Y),1).float()\n",
    "D=torch.utils.data.TensorDataset(Xtorch,Ytorch)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader( D , batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325d31d",
   "metadata": {},
   "source": [
    "Training Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c688278",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(n): # train for this many epochs\n",
    "    for i in range(n):\n",
    "        totalloss=0\n",
    "        for Xtorch,Ytorch in trainloader: # 1. pull a batch from the loader\n",
    "            \n",
    "            y=model(Xtorch) # 2. forward pass through model\n",
    "            loss=loss_function(y, Ytorch) # 3. compute the loss\n",
    "            totalloss+=loss.detach() # detach method tells the machine not to track the gradient of this tensor\n",
    "            \n",
    "            loss.backward() # calculate backward (gradient) pass\n",
    "            optimizer.step() # use gradient info in optimise step\n",
    "        \n",
    "        losses.append(totalloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e9068",
   "metadata": {},
   "source": [
    "Make a grid of predictions, for plotting the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d10652",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=400\n",
    "Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "predict=model(torch.tensor(np.transpose(Xgrid2)).float()) \n",
    "predict=predict.reshape( (N,N) ).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filled contour of the decision boundary\n",
    "plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "# scatter plot of the training data\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb7801",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,26):\n",
    "    ax = plt.subplot(5, 5, i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    \n",
    "    N=25\n",
    "    Xgrid=np.meshgrid(np.linspace(-5, 5, N), np.linspace(-5, 5, N))\n",
    "    Xgrid2=np.array([np.ndarray.flatten(Xgrid[0]), np.ndarray.flatten(Xgrid[1])])\n",
    "    predict=model(torch.tensor(np.transpose(Xgrid2)).float()) # re-using our neuron function from earlier\n",
    "    predict=predict.reshape( (N,N) ).detach()\n",
    "    \n",
    "    plt.contourf(Xgrid[0], Xgrid[1] ,predict, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    # scatter plot of the training data\n",
    "    plt.scatter(X[:,0],X[:,1], c=Y, s=1, cmap=plt.cm.Spectral)\n",
    "    \n",
    "    train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be38c44",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34196dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy\",(Ytorch == torch.round(model(Xtorch))).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df3d29",
   "metadata": {},
   "source": [
    "Again we can inspect the model parameters. Note already the difference in size to our previous, smaller network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9012e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(Xtorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239179af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
