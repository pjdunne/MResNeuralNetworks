{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "In statistics, it is not always the case that you will have knowledge of the exact, or even approximate probability distributions for your data. In this case, its extremely difficult to just write down a likelihood and proceed to determine estimates, confidence/credible intervals for various parameters that you might be interested in. \n",
    "\n",
    "The method of bootstrapping instead, only relies on re-sampling from your existing dataset. Suppose you have a dataset $X_{1},X_{2},...,X_{N}$ of random, *independant observations*. Then you produce a bootstrap sample by randomly selecting $N$ of the observations, replacing them each time. From multiple such samples, you can calculate a quantity (such as an estimator) and use them to determine properties of the distribution of that quantity - eg their variance. \n",
    "\n",
    "We can use python's `random.choice` module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As an example, picking 6  random elements out of a list of 10 integers is as simple as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choices([1,4,2,5,6,3,8,7,9,10],k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `choices` function samples *with replacement* (so we can see the same element more than once in the returned list). To sample *without replacement* in python, we can use `random.sample` instead. \n",
    "\n",
    "Now let's generate some random data from our example of a normal distribution with parameters $\\mu=3,\\sigma=1.5$.  We'll generate 50 observations using the `random` module from numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_true = mu = 3.0\n",
    "sigma_true = sigma = 1.5\n",
    "observations = numpy.random.normal(mu,sigma,50)\n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember  that the sample mean is given by, \n",
    "\n",
    "$$\n",
    "\\bar{x}=\\frac{1}{N}\\sum_{i=1}^{N}X_{i}\n",
    "$$\n",
    "\n",
    "We can estimate the distribution of the mean by using bootstrap samples. Select 50 observations (replacing each time) and in each case calculate the mean from that bootstrap sample. We'll keep track of the values calulated and plot a histogram of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mean(data): \n",
    "    return numpy.mean(data)\n",
    "\n",
    "nsamples = 10000 \n",
    "means = []\n",
    "\n",
    "for s in range(nsamples) : \n",
    "    bootstrap = random.choices(observations,k=50)\n",
    "    means.append(sample_mean(bootstrap))\n",
    "\n",
    "plt.hist(means,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comparing to the mean of the original observations, we can see that the mean of the distribution is indeed similar to the estimate from the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9c/l_hhhml95sz43s5rrrv28k5c0000gn/T/ipykernel_13926/1180060353.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_mean' is not defined"
     ]
    }
   ],
   "source": [
    "print(sample_mean(observations))\n",
    "print(sample_mean(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the standard deviation of the means obtained from the bootstrap samples, we can get some idea about the *uncertainty on the mean estimated from the sample mean of the observations*. \n",
    "\n",
    "Note that this is *not*  the same as the standard deviation of the distribution of $X$ (which we know should be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_std_dev(data):\n",
    "    return numpy.std(data)\n",
    "\n",
    "print(sample_std_dev(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also estimate the variance of the standard deviation estimator with the bootstrapping method. Again we plot a histogram of the std deviation across the bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddevs = []\n",
    "for s in range(nsamples) : \n",
    "    bootstrap = random.choices(observations,k=50)\n",
    "    stddevs.append(sample_std_dev(bootstrap))\n",
    "\n",
    "plt.hist(stddevs,bins=100)\n",
    "plt.show()\n",
    "print(sample_mean(stddevs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be careful when dealing with small lists of observations since theres a limit to the number of unique bootstrap samples that can be obtained. In our example, where we had 50 in our sample this number is 101!/(50!)(49!) so you don't need to worry.\n",
    "\n",
    "Finally, bootstrap estimates are only asymptotically consistent in certain circumstances and so should be used with caution.\n",
    "\n",
    "## Method of moments and bootstrap\n",
    "\n",
    "Now let's apply what we just learned to our method of moments estimators for the gaussian $\\mu$ and $\\sigma$ parameters.  \n",
    "\n",
    "For each value of $n$, we can estimate the variance (or standard deviation) of the distribution of our estimator using bootstrap samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_mu(samples):\n",
    "    nsamples = len(samples)\n",
    "    mus = []\n",
    "    for s in range(nsamples) : \n",
    "      bootstrap = random.choices(samples,k=nsamples)\n",
    "      mus.append(numpy.mean(bootstrap))\n",
    "    return numpy.std(mus)\n",
    "\n",
    "def bootstrap_sigma(samples):\n",
    "    nsamples = len(samples)\n",
    "    sigmas = []\n",
    "    for s in range(nsamples) : \n",
    "      bootstrap = random.choices(samples,k=nsamples)\n",
    "      sigmas.append(numpy.std(bootstrap))\n",
    "    return numpy.std(sigmas)\n",
    "\n",
    "ns_sets = range(10,1000,50)\n",
    "\n",
    "mu_hats = []\n",
    "sigma_hats = []\n",
    "mu_hats_stddev = []\n",
    "sigma_hats_stddev = []\n",
    "\n",
    "for nsamples in ns_sets:\n",
    "    toys = norm.rvs(mu_true,sigma_true,size=nsamples)\n",
    "    mu_hats.append(numpy.mean(toys))\n",
    "    mu_hats_stddev.append(bootstrap_mu(toys))\n",
    "    sigma_hats.append(numpy.std(toys))\n",
    "    sigma_hats_stddev.append(bootstrap_sigma(toys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ns_sets,mu_hats,color='blue',marker=\"o\",label=\"$\\hat{\\mu}$\")\n",
    "plt.plot(ns_sets,sigma_hats,color='red',marker=\"o\",label=\"$\\hat{\\sigma}$\")\n",
    "plt.fill_between(ns_sets, [m-e for m,e in zip(mu_hats,mu_hats_stddev)], [m+e for m,e in zip(mu_hats,mu_hats_stddev)])\n",
    "plt.fill_between(ns_sets, [s-e for s,e in zip(sigma_hats,sigma_hats_stddev)], [s+e for s,e in zip(sigma_hats,sigma_hats_stddev)])\n",
    "\n",
    "plt.xlabel(\"# in sample\")\n",
    "plt.ylabel(\"$\\hat{\\mu}$ or $\\hat{\\sigma}$\")\n",
    "\n",
    "plt.plot([ns_sets[0],ns_sets[-1]],[mu_true,mu_true],color='black',linestyle=\"--\")\n",
    "plt.plot([ns_sets[0],ns_sets[-1]],[sigma_true,sigma_true],color='black',linestyle=\"--\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"moments_estimator.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the standard deviation (the square root of the variance) from the bootstrapping gets narrower as the number in the sample increases. This makes sense since the more data we have, the more accurate we should be about the estimator.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hubble constant \n",
    "\n",
    "Let's take a look at an example where Bootstrapping comes in handy in the \"real world\". Below is a plot of data from Hubble's paper [\"A relation between distance and radial velocity among extra-galactic nebulae\" (1929)](https://www.pnas.org/content/15/3/168), showing the radial velocity ($v$) of extra-galactic nebulae vs their distance from us ($d$). \n",
    "\n",
    "(I borrowed the data from this interesting [blog post](https://medium.datadriveninvestor.com/revisiting-hubbles-law-with-hacker-stats-in-python-9b56604916c1))\n",
    "\n",
    "The data are saved in a `.csv` file with colums \"name\", \"distances\" (in Mpc), \"velocities\" (in km/s). We'll use the `pandas.read_csv` function to read in the data. Pandas is a popular tool to manipulate columnar data (either you've come across this already in the ML practical course or you will do very soon). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "data = pandas.read_csv(\"hubble_data.csv\")\n",
    "data.head(24) # there are 24 rows in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the names of course but we do need the other two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "plt.scatter(data[\"distances\"].tolist(),data[\"velocities\"].tolist())\n",
    "plt.xlabel(\"Distance (Mpc)\")\n",
    "plt.ylabel(\"Radial velocity (km/s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Hubble's law is that the two quantities plotted are related as \n",
    "\n",
    "$$\n",
    "v = H_{0} d\n",
    "$$\n",
    "\n",
    "Where $H_{0}$ is the Hubble constant. \n",
    "\n",
    "We can use a simple linear regression to determine $H_{0}$ from the data. To estimate $H_0$, we optimise the least-squares, \n",
    "\n",
    "$$\n",
    "s = \\sum_{i=1}^{n} (v_{i} - H_{0}d_{i})^{2}\n",
    "$$\n",
    "\n",
    "where we assume the intercept is 0, the value of $H_0$ that minimises $s$ (i.e such that $\\frac{ds}{dH_0}$=0) is given by, \n",
    "\n",
    "$$\n",
    "H_{0} = \\frac{\\sum_i d_i v_i }{\\sum_{i}d_i^{2} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H0(data): \n",
    "    sumdv = sum([d[0]*d[1] for d in data])\n",
    "    sumdd = sum([d[0]*d[0] for d in data])\n",
    "    \n",
    "    return (sumdv)/(sumdd)\n",
    "\n",
    "# let's put the data into a basic form\n",
    "data_basic = [[d,v] for d,v in zip(data[\"distances\"].tolist(),data[\"velocities\"].tolist())]\n",
    "\n",
    "H0_obs = H0(data_basic)\n",
    "print(H0_obs)\n",
    "\n",
    "plt.scatter(data[\"distances\"].tolist(),data[\"velocities\"].tolist())\n",
    "plt.xlabel(\"Distance (Mpc)\")\n",
    "plt.ylabel(\"Radial velocity (km/s)\")\n",
    "plt.plot([0,2],[0,2*H0_obs],color='red')\n",
    "plt.savefig(\"galaxies.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $H_0$ here is about $7\\times$ larger than the measured value today. This is due to the fact that the distances measured were around $7\\times$ smaller than they should be. Let's ignore this though and carry on with these distances.\n",
    "\n",
    "These data don't have any uncertainty associated to them, so we can't use typical error propagation to estimate an uncertainty on $H_{0}$. However, since each of these measurements are independent of one-another, we can estimate an uncertainty on $H_0$ using Bootstrapping. \n",
    "\n",
    "Remember, we select boostrap samples by randomly sampling from the data, with replacement, until we have a new sample with as many entries as the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_H0 = []\n",
    "n = len(data_basic)\n",
    "for i in range(100): \n",
    "  bootstrap = random.choices(data_basic,k=n)\n",
    "  bootstrap_H0.append(H0(bootstrap))\n",
    "  \n",
    "plt.hist(bootstrap_H0)\n",
    "plt.show()\n",
    "\n",
    "print(\"H_0 = %g +/- %g (km/s)/(Mpc)\"%(H0_obs,np.std(bootstrap_H0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the statistical uncertainty is around 10% in this case. However, you might already know that the current value is closer to ~70  (ùëòùëö/ùë†)/(ùëÄùëùùëê) . This is a crucial point of data analysis in that one can only get good results if both the data and model are good. In this case, we already mentioned that the distance measurements were a factor of ~7 off and we haven't accounted for that systematic error in our estimate.\n",
    "\n",
    "Note that there are other forms of Bootstrapping, which may be more appropriate depending on what kind of situation we are dealing with. In the previous examples, we always assumed a fixed number of observations. However, in many scenarious, even the number of observations may be a random variable.\n",
    "\n",
    "For example, if you were studying the energy spectrum of radioactive decay products, you might pick a fixed time-window to make your observations. In this case, the number of observations will be Poisson distributed. To reflect this, we can adjust our bootstrapping algorithm by attaching a weight  ùë§  to each selection  ùë§‚àºPoisson(ùúÜ=1) . If we have 100 entries in the original dataset, we make 100 selections but a single selection may have a weight of  0  (i.e its not included in our sample) or  2  (so its included twice in the sample) or  3,4,5... . The total number therefore may not be 100 in the bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "974ccb57decf0bf63208503b9dff515d9933d9764e6436c4b65384d34512ae6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
