{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions, Gradient Flow and Dead Neurons - Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [Introduction and Setup](#intro)\n",
    "1. [Common Activation Functions](#common-activation-functions)\n",
    "1. [Visualising Activation Functions](#visualising-act)\n",
    "1. [Fashion MNIST Dataset](#fashion)\n",
    "1. [Gradient Flow](#gradient-flow)\n",
    "1. [Training a Model](#training-a-model)\n",
    "1. [Dead Neurons](#dead-neurons)\n",
    "1. [Conclusion](#conclusion)\n",
    "1. [Appendix](#appendix)\n",
    "    1. [PyTorch backwards() Function](#backwards_app) \n",
    "    1. [Backpropagation with Matrices](#matrix_app)\n",
    "    2. [Cross-Entropy](#cross)\n",
    "    3. [Activation Functions Overview](#act_fn_app)\n",
    "\n",
    "This notebook is heavily based on one from Universitaet van Amsterdam's deep learning course written by Phillip Lippe.\n",
    "\n",
    "You can see his original filled notebook at:\n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial3/Activation_Functions.ipynb)\n",
    "\n",
    "**Pre-trained models:** \n",
    "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/saved_models/tree/main/tutorial3)\n",
    "[![GoogleDrive](https://img.shields.io/static/v1.svg?logo=google-drive&logoColor=yellow&label=GDrive&message=Download&color=yellow)](https://drive.google.com/drive/folders/1sFpZUpDJVjiYEvIqISqfkFizfsTnPf4s?usp=sharing)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Introduction and Setup [^](#index) <a id='intro'></a>\n",
    "\n",
    "Last week, you were introduced to the basics of neural networks. In this tutorial we will focus on the **activation function**, which characterises the response at each neuron. \n",
    "\n",
    "As discussed previously in the Single Neuron Notebook,  activation functions are a crucial part of deep learning models because they can add **non-linearity** to the network. Without non-linearity, increasing the number of layers will not effect our network's performance - the output would always be a linear combination of the input variablesï¼Œ and so could have been achieved with just a single layer.\n",
    "\n",
    "There is a great variety of activation functions in the literature, and some are more beneficial than others.\n",
    "The goal of this tutorial is to show the importance of choosing a good activation function (and how to do so), and what problems might occur if we do not.\n",
    "\n",
    "Before we start, we should import our standard libraries and set up basic functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our training reproducible, we will set a seed for the random numbers that we generate from Torch and numPy.\n",
    "\n",
    "If you are using a department provided laptop, or many other types of commercial machines, you will not have access to a GPU. GPUs can increase the speed of the training process and thus are very important when working with more complex models, however simply using a CPU in this course will be **sufficient**.\n",
    "\n",
    "_Unlike the CPU, the same seed on different GPU architectures can give different results. All the prebuilt models linked above here have been trained on an NVIDIA GTX1080Ti._\n",
    "\n",
    "The following cell also defines two file paths: DATASET_PATH and CHECKPOINT_PATH. The dataset path is the directory where we will download datasets used in the notebooks. It is recommended to store all datasets from PyTorch in one joined directory to prevent duplicate downloads. The checkpoint path is the directory where we will store the trained model weights and additional files. The files required will be automatically downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial3\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell downloads all pretrained models we will use in this notebook. These are copies from Philip Lippe's course. The files are stored in a separate [repository](https://github.com/pjdunne/saved_models). Please let me (Patrick) know if an error occurs so it can be fixed for all students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/pjdunne/saved_models/main/tutorial3/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"FashionMNIST_elu.config\", \"FashionMNIST_elu.tar\", \n",
    "                    \"FashionMNIST_leakyrelu.config\", \"FashionMNIST_leakyrelu.tar\",\n",
    "                    \"FashionMNIST_relu.config\", \"FashionMNIST_relu.tar\",\n",
    "                    \"FashionMNIST_sigmoid.config\", \"FashionMNIST_sigmoid.tar\",\n",
    "                    \"FashionMNIST_swish.config\", \"FashionMNIST_swish.tar\",\n",
    "                    \"FashionMNIST_tanh.config\", \"FashionMNIST_tanh.tar\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Common Activation Functions [^](#index) <a id='common-activation-functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first implement some common activation functions, writing our own functions to gain a better understanding and insight into how they operate. Most of these functions can also be found in the `torch.nn` package (see the [documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for an overview).\n",
    "\n",
    "_The following code uses Object Orientated Programming (OOP). If you are unfamiliar or out of practice with OOP, you may wish to read the 2nd Bootcamp Notebook for some guidance._\n",
    "\n",
    "\n",
    "To make it easier to compare various activation functions, we will start by defining a base class from which all our future modules will inherit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ActivationFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = self.__class__.__name__\n",
    "        self.config = {\"name\": self.name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every activation function will be an `nn.Module` so that we can integrate them nicely in a network. We will use the `config` dictionary to store adjustable parameters for some activation functions.\n",
    "\n",
    "Next, we will implement two of the \"oldest\" activation functions that are still commonly used for various tasks: sigmoid and tanh. Both the sigmoid and tanh activation can be also found as PyTorch functions (`torch.sigmoid`, `torch.tanh`) or as modules (`nn.Sigmoid`, `nn.Tanh`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "    \n",
    "Below, you should write the code for the `forward` function of two activation functions. \n",
    "     \n",
    "If you are unsure of the form of either of the two functions, feel free to look them up online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "##############################   \n",
    "    \n",
    "class Tanh(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_exp, neg_x_exp = torch.exp(x), torch.exp(-x)\n",
    "        return (x_exp - neg_x_exp) / (x_exp + neg_x_exp)\n",
    "    \n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Activation Functions\n",
    "\n",
    "Another popular activation function that has allowed the training of deeper networks is the **Rectified Linear Unit (ReLU)**. This function returns the input for inputs greater than 0, and returns 0 for inputs less than or equal to 0. Despite its simplicity, ReLU has one major benefit compared to sigmoid and tanh: a strong, stable gradient for a large range of values.\n",
    "\n",
    "Based on this idea, many variations of ReLU have been proposed. We will implement the following three:\n",
    "\n",
    "**LeakyReLU:** In the negative region of the input space, LeakyReLU has the form of a linear function with a small gradient (alpha), rather than just being 0. This non-zero gradient means that gradients can also 'flow' in this part of the input. We'll see what this means later.\n",
    "\n",
    "**ELU:** This is similar to the leaky ReLU but replaces the function in the negative input space with an exponential decay.\n",
    "\n",
    "**Swish:** This is the most recently proposed of these activation functions, and is actually the result of a large experiment with the purpose of finding the \"optimal\" activation function. Swish returns the input multiplied by the sigmoid of the input.\n",
    "Compared to the other activation functions, Swish is both smooth and non-monotonic (i.e. Swish's gradient  has a change of sign). Swish is designed to solve a problem called 'dead neurons' which can occur with standard ReLU. We'll see what this is later, but it is a particular problem with deep networks.\n",
    "If interested, a more detailed discussion of the benefits of Swish can be found in [this paper](https://arxiv.org/abs/1710.05941) [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "    \n",
    "Given the desciptions above, implement these four activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * (x > 0).float()\n",
    "\n",
    "##############################\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    \n",
    "    def __init__(self, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.config[\"alpha\"] = alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.where(x > 0, x, self.config[\"alpha\"] * x)\n",
    "\n",
    "##############################\n",
    "    \n",
    "class ELU(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.where(x > 0, x, torch.exp(x)-1)\n",
    "\n",
    "##############################\n",
    "    \n",
    "class Swish(ActivationFunction):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarise all our activation functions in a dictionary, which maps the name of the function to the class object. If you wish to implement a new activation function, add it to this dictionary to include it in future comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act_fn_by_name = {\n",
    "    \"sigmoid\": Sigmoid,\n",
    "    \"tanh\": Tanh,\n",
    "    \"relu\": ReLU,\n",
    "    \"leakyrelu\": LeakyReLU,\n",
    "    \"elu\": ELU,\n",
    "    \"swish\": Swish\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Visualising Activation Functions [^](#index) <a id='visualising-act'></a>\n",
    "\n",
    "To understand what each activation function actually does, it may be useful to visualise both the function and its **gradient**. Understanding the gradient of the activation function is crucial for optimising the neural network. \n",
    "\n",
    "PyTorch allows us to compute the gradients by calling the `backward` function. The cell below demonstrates how this can be implemented. For more information on the backwards and how it operates function, please see the Appendix section [PyTorch backwards() Function](#backwards_app).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_grads(act_fn, x):\n",
    "    \"\"\"\n",
    "    Computes the gradients of an activation function at specified positions.\n",
    "    \n",
    "    Inputs:\n",
    "        act_fn - An object of the class \"ActivationFunction\" with an implemented forward pass.\n",
    "        x - 1D input tensor. \n",
    "    Output:\n",
    "        A tensor with the same size of x containing the gradients of act_fn at x.\n",
    "    \"\"\"\n",
    "    x = x.clone().requires_grad_() # Mark the input as tensor for which we want to store gradients\n",
    "    out = act_fn(x)\n",
    "    out.sum().backward() # Summing results in an equal gradient flow to each element in x\n",
    "    return x.grad # Accessing the gradients of x by \"x.grad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualise all our activation functions including their gradients.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "\n",
    "Complete the following functions in order to generate nice plots of the activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def vis_act_fn(act_fn, ax, x):\n",
    "    # Run activation function\n",
    "    y = act_fn(x)\n",
    "    y_grads = get_grads(act_fn, x)\n",
    "    # Push x, y and gradients back to cpu for plotting\n",
    "    x, y, y_grads = x.cpu().numpy(), y.cpu().numpy(), y_grads.cpu().numpy()\n",
    "    ## Plotting\n",
    "    ax.plot(x, y, linewidth=2, label=\"ActFn\")\n",
    "    ax.plot(x, y_grads, linewidth=2, label=\"Gradient\")\n",
    "    ax.set_title(act_fn.name)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(-1.5, x.max())\n",
    "\n",
    "# Add activation functions if wanted\n",
    "act_fns = [act_fn() for act_fn in act_fn_by_name.values()]\n",
    "x = torch.linspace(-5, 5, 1000) # Range on which we want to visualize the activation functions\n",
    "## Plotting\n",
    "rows = math.ceil(len(act_fns)/2.0)\n",
    "fig, ax = plt.subplots(rows, 2, figsize=(8, rows*4))\n",
    "for i, act_fn in enumerate(act_fns):\n",
    "    vis_act_fn(act_fn, ax[divmod(i,2)], x)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Fashion MNIST Dataset [^](#index) <a id='fashion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the impact each activation function has on training our neural network, we will look at a simple example NN trained on [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "FashionMNIST is a more complex version of MNIST and contains black-and-white images of clothes instead of digits. The 10 classes include trousers, coats, shoes, bags and more. \n",
    "\n",
    "To load this dataset, we will make use of the PyTorch package `torchvision` ([documentation](https://pytorch.org/docs/stable/torchvision/index.html)). The `torchvision` package consists of popular datasets, model architectures, and common image transformations for computer vision. We will use the package for many of the notebooks in this course to simplify our dataset handling. \n",
    "\n",
    "FashionMNIST already has the training and test sets separated but we need to split the training and validation sets ourselves. Below we load in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformations applied on each image => first make them a tensor, then normalize them in the range -1 to 1\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
    "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\n",
    "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "    \n",
    "Visualize a few of the MNIST images to get an impression of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exmp_imgs = [train_set[i][0] for i in range(16)]\n",
    "# Organize the images into a grid for nicer visualization\n",
    "img_grid = torchvision.utils.make_grid(torch.stack(exmp_imgs, dim=0), nrow=4, normalize=True, pad_value=0.5)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"FashionMNIST examples\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Setup\n",
    "\n",
    "The code below contains the framework to implement the required network: we must view the Fashion MNIST images as 1D tensors and then push them through a sequence of linear layers and a specified activation function. \n",
    "\n",
    "I've given the function signature below. Feel free to experiment with other network architectures.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "\n",
    "Complete the cell below by adding the code required to create the hidden layers.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            act_fn - Object of the activation function that should be used as non-linearity in the network.\n",
    "            input_size - Size of the input images in pixels\n",
    "            num_classes - Number of classes we want to predict\n",
    "            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create the network based on the specified hidden sizes\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + hidden_sizes\n",
    "        for layer_index in range(1, len(layer_sizes)):\n",
    "            layers += [nn.Linear(layer_sizes[layer_index-1], layer_sizes[layer_index]),\n",
    "                       act_fn]\n",
    "        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n",
    "        self.layers = nn.Sequential(*layers) # nn.Sequential summarizes a list of modules into a single module, applying them in sequence\n",
    "        \n",
    "        # We store all hyperparameters in a dictionary for saving and loading of the model\n",
    "        self.config = {\"act_fn\": act_fn.config, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes} \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # Reshape images to a flat vector\n",
    "        out = self.layers(x)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add functions for loading and saving the model. The hyperparameters are stored in a configuration file (simple json file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _get_config_file(model_path, model_name):\n",
    "    # Name of the file for storing hyperparameter details\n",
    "    return os.path.join(model_path, model_name + \".config\")\n",
    "\n",
    "def _get_model_file(model_path, model_name):\n",
    "    # Name of the file for storing network parameters\n",
    "    return os.path.join(model_path, model_name + \".tar\")\n",
    "\n",
    "def load_model(model_path, model_name, net=None):\n",
    "    \"\"\"\n",
    "    Loads a saved model from disk.\n",
    "    \n",
    "    Inputs:\n",
    "        model_path - Path of the checkpoint directory\n",
    "        model_name - Name of the model (str)\n",
    "        net - (Optional) If given, the state dict is loaded into this model. Otherwise, a new model is created.\n",
    "    \"\"\"\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    assert os.path.isfile(config_file), f\"Could not find the config file \\\"{config_file}\\\". Are you sure this is the correct path and you have your model config stored here?\"\n",
    "    assert os.path.isfile(model_file), f\"Could not find the model file \\\"{model_file}\\\". Are you sure this is the correct path and you have your model stored here?\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config_dict = json.load(f)\n",
    "    if net is None:\n",
    "        act_fn_name = config_dict[\"act_fn\"].pop(\"name\").lower()\n",
    "        act_fn = act_fn_by_name[act_fn_name](**config_dict.pop(\"act_fn\"))\n",
    "        net = BaseNetwork(act_fn=act_fn, **config_dict)\n",
    "    net.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    return net\n",
    "    \n",
    "def save_model(model, model_path, model_name):\n",
    "    \"\"\"\n",
    "    Given a model, we save the state_dict and hyperparameters.\n",
    "    \n",
    "    Inputs:\n",
    "        model - Network object to save parameters from\n",
    "        model_path - Path of the checkpoint directory\n",
    "        model_name - Name of the model (str)\n",
    "    \"\"\"\n",
    "    config_dict = model.config\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config_dict, f)\n",
    "    torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Gradient flow [^](#index)  <a id='gradient-flow'></a>\n",
    "\n",
    "An important aspect of neural network training are the **gradients** of each parameter (i.e. the partial derivative of the loss function wrt that parameter). Each activation function results in different gradients, and it is important to understand what these differences mean for the training process.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "\n",
    "Plot a freshly initialised network and measure the gradients for each parameter at the activation function layers of your model for a batch of 256 images. This should give you a feeling about how each activation function influences the gradients.\n",
    "\n",
    "For your loss function, use **cross entropy** (please read the Appendix section [Cross-Entropy](#cross) or see the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_gradients(net, color=\"C0\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        net - Object of class BaseNetwork\n",
    "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=256, shuffle=False)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "    \n",
    "    # Pass one batch through the network, and calculate the gradients for the weights\n",
    "    \n",
    "    net.zero_grad()\n",
    "    preds =net(imgs)\n",
    "    loss =F.cross_entropy(preds, labels)\n",
    "    loss.backward()\n",
    "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
    "    \n",
    "    grads = {name: params.grad.data.view(-1).cpu().clone().numpy() for name, params in net.named_parameters() if \"weight\" in name}\n",
    "    net.zero_grad()\n",
    "    \n",
    "    ## Plotting\n",
    "    columns = len(grads)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3.5, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in grads:\n",
    "        key_ax = ax[fig_index%columns]\n",
    "        sns.histplot(data=grads[key], bins=30, ax=key_ax, color=color, kde=True)  \n",
    "        key_ax.set_title(str(key))\n",
    "        key_ax.set_xlabel(\"Grad magnitude\")\n",
    "        fig_index += 1\n",
    "    fig.suptitle(f\"Gradient magnitude distribution for activation function {net.config['act_fn']['name']}\", fontsize=14, y=1.05)\n",
    "    fig.subplots_adjust(wspace=0.45)\n",
    "    plt.show()\n",
    "    # The initialisation of the weight parameters can also be crucial to creating an optimal model and avoiding problems with vanishing\n",
    "    # gradient. By default, PyTorch uses the [Kaiming](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) \n",
    "    # initialisation for linear layers optimised for ReLU activations. We could do a whole extra class on initialisation, \n",
    "    # but assume for now that the Kaiming initialisation works for all activation functions reasonably well (the Universitet van Amsterdam\n",
    "    # course has more information on initialisation in [Lecture 4](https://github.com/phlippe/uvadlc_notebooks/tree/master/docs/tutorial_notebooks/tutorial4)).\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn prints warnings if histogram has small values. We can ignore them for now\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "## Create a plot for every activation function\n",
    "for i, act_fn_name in enumerate(act_fn_by_name):\n",
    "    set_seed(42) # Setting the seed ensures that we have the same weight initialization for each activation function\n",
    "    act_fn = act_fn_by_name[act_fn_name]()\n",
    "    net_actfn = BaseNetwork(act_fn=act_fn).to(device)\n",
    "    visualize_gradients(net_actfn, color=f\"C{i}\")\n",
    "    \n",
    "#N.b. The number indicating the number of layers refers to both the linear layers and those containing the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of these plots, we must first understand the principles behind **gradient flow**.\n",
    "\n",
    "**Gradient flow** describes how activation functions propagate the gradients of the loss function (referred to here as gradients) through the network. First, we should remind ourselves about how a multi-layered NN trains:\n",
    "\n",
    "1. **Forward Pass:** The input data is fed into the neural network and passes through the successive layers. \n",
    "1. **Loss Calculation:** Using a specified loss function, the error of our predicted values compared to the true values is calculated. By training our model, we want to minimise this error.\n",
    "1. **Backward Pass (Backpropagation):** This is the key concept in gradient flow, and was discussed in more detail in the Neural Networks notebook. \n",
    "- Just like in the linear models we considered in ML Basics, we can minimise the loss by moving in the direction of the negative gradient of the loss function. Unlike in linear models, the relationship between loss function and the weights and bias is often not direct (there may be many NN layers with activation function in between).\n",
    "- To solve this, we calculate the gradients by using the chain rule and moving **backwards** through our network. \n",
    "- Consider the image below, where $\\phi$ represents our activation function and the term in brackets is $z$ (z will have the same indexing notation as the other terms), as we saw before. From the chain rule, we can calculate the effects of weights in the final layer using:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{(2)}_{11}} = \\frac{\\partial L}{\\partial \\phi  (z^{(2)} )}\n",
    "                                              \\frac{\\partial \\phi (z^{(2)})}{\\partial z^{(2)}}\n",
    "                                              \\frac{\\partial z^{(2)}}{\\partial w^{(2)}_{11}}\n",
    "                                              = \\frac{\\partial L}{\\partial y}\n",
    "                                              \\frac{\\partial y}{\\partial w^{(2)}_{11}}$$\n",
    "                                                                                           \n",
    "\n",
    "- Working backwards allows us to increase the efficiency of our model. Since we have already calculated $\\frac{\\partial L}{\\partial y}$, we can use the result in further chain rules. We can repeat this with $\\frac{\\partial L}{\\partial a_1}$, and so on. \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w^{(1)}_{11}} = \\frac{\\partial L}{\\partial y}\n",
    "                                              \\frac{\\partial y}{\\partial \\phi  (z_1^{(1)} )}\n",
    "                                              \\frac{\\partial \\phi (z_1^{(1)} )}{\\partial z_1^{(1)}}\n",
    "                                              \\frac{\\partial z_1^{(1)}}{\\partial w^{(2)}_{11}} \n",
    "                                              = \\frac{\\partial L}{\\partial a_1}\n",
    "                                              \\frac{\\partial a_1}{\\partial w^{(2)}_{11}}$$\n",
    "                                            \n",
    "<img src=\"weight-and-bias.png\" width=\"600\" />\n",
    "\n",
    "- For more complex networks, we would also have to sum the different partial differentials from each branch. [JeremyJordan](https://www.jeremyjordan.me/neural-networks-training/) includes these terms written out in full, as well as explaining how matrix calculations can decrease the complexity of the many derivatives required. To read a brief description of how these matrices operate, and the use of the backward() function, please see Appendix section [Backpropagation with Matrices](#matrix_app).\n",
    "\n",
    "4. **Update:** Using these gradient calculations, we can then update our weights and biases. Using a gradient descent method. Note that the learning rate $\\alpha$ for a particular variable is the same for all layers.\n",
    "5. **Repeat:** The process repeats until a stopping condition is met (such as maximum number of iterations reached).\n",
    "\n",
    "### Issues with Gradient Flow\n",
    "\n",
    "Imagine we have a very deep neural network with more than 50 layers. Each layer we move back through the network, we will introduce a further term of $\\frac{\\partial \\phi (z_j^{(i)} )}{\\partial z_j^{(i)}}$. So to calculate the effects of the weights from the input terms to the first nodes, our equation would have 50 multiples of this term.\n",
    "\n",
    "**Vanishing Gradient Problem** \n",
    "\n",
    "If the expectation of this gradient term is considerably smaller than 1, the multiplication effect will mean that $\\frac{\\partial L}{\\partial w}$ will become extremely close to 0 for any weights far from the output. This can not be resolved through the learning rate either, since in general it kept constant for all layers. The weights are updated in proportion to this gradient term, so a gradient close to 0 will mean that these earlier weights barely update as we repeat each iteration. This results in **slow** or **stalled** learning.\n",
    "\n",
    "Using an activation function that will not have a 0 gradient (such as ReLU) will help to resolve this problem\n",
    "\n",
    "**Exploding Gradient**\n",
    "\n",
    "The opposite effect is if the expectation of $\\frac{\\partial \\phi (z )}{\\partial z}$ is much greater than one. In this instance, backpropogation will cause $\\frac{\\partial L}{\\partial w}$ for earlier terms to be extremely large, and thus cause wild fluctuations in weights that make it difficult for the model to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretting our Gradient Plots\n",
    "\n",
    "The sigmoid activation function shows a clearly undesirable behavior. While the gradients for the output layer are very large with values up to 0.1, the input layer has the lowest gradient norm across all activation functions (only 1e-5). This is due to its small maximum gradient of 1/4, and thus finding a suitable learning rate across all layers is not possible with this setup.\n",
    "All the other activation functions show similar gradient norms across all layers. Interestingly, the ReLU activation has a spike around 0. This is due to the function returning 0 for negative inputs, and dead neurons (we will take a closer look at this later on).\n",
    "\n",
    "The initialisation of the weight parameters can also be crucial to creating an optimal model and avoiding problems with vanishing gradient. By default, PyTorch uses the [Kaiming](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_) initialisation for linear layers optimised for ReLU activations. We could do a whole extra class on initialisation, but assume for now that the Kaiming initialisation works for all activation functions reasonably well (the Universitet van Amsterdam course has more information on initialisation in [Lecture 4](https://github.com/phlippe/uvadlc_notebooks/tree/master/docs/tutorial_notebooks/tutorial4))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Training a model [^](#index) <a id='training-a-model'></a>\n",
    "\n",
    "We now want to train our model with different activation functions on the FashionMNIST dataset and compare the performances. Our ultimate goal is to achieve the best possible performance on a dataset of our choice.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "    \n",
    "Write a training loop in the next cell, including a validation after every epoch and a final test on the best model. Then try answering the following questions before continuing:\n",
    "- Which activation functions perform well? \n",
    "- Is this what you were expecting from their gradient flow properties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(net, model_name, max_epochs=50, patience=7, batch_size=256, overwrite=False):\n",
    "    \"\"\"\n",
    "    Train a model on the training set of FashionMNIST\n",
    "    \n",
    "    Inputs:\n",
    "        net - Object of BaseNetwork\n",
    "        model_name - (str) Name of the model, used for creating the checkpoint names\n",
    "        max_epochs - Number of epochs we want to (maximally) train for\n",
    "        patience - If the performance on the validation set has not improved for #patience epochs, \n",
    "                    we stop training early\n",
    "        batch_size - Size of batches used in training\n",
    "        overwrite - Determines how to handle the case when there already exists a checkpoint.\n",
    "                    If True, it will be overwritten. Otherwise, we skip training.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n",
    "    if file_exists and not overwrite:\n",
    "        print(\"Model file already exists. Skipping training...\")\n",
    "    else:\n",
    "        if file_exists:\n",
    "            print(\"Model file exists, but will be overwritten...\")\n",
    "            \n",
    "        # Defining optimizer, loss and data loader\n",
    "        optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) # Default parameters, feel free to change\n",
    "        loss_module = nn.CrossEntropyLoss() \n",
    "        train_loader_local = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
    "\n",
    "        val_scores = []\n",
    "        best_val_epoch = -1\n",
    "        for epoch in range(max_epochs):\n",
    "            ############\n",
    "            # Training #\n",
    "            ############\n",
    "            net.train()\n",
    "            true_preds, count = 0., 0\n",
    "            for imgs, labels in tqdm(train_loader_local, desc=f\"Epoch {epoch+1}\", leave=False):\n",
    "                imgs, labels = imgs.to(device), labels.to(device) # To GPU if we have one\n",
    "                optimizer.zero_grad() # Zero-grad can be placed anywhere before \"loss.backward()\"\n",
    "                preds = net(imgs)\n",
    "                loss = loss_module(preds, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # Record statistics during training\n",
    "                true_preds += (preds.argmax(dim=-1) == labels).sum()\n",
    "                count += labels.shape[0]\n",
    "            train_acc = true_preds / count\n",
    "\n",
    "            ##############\n",
    "            # Validation #\n",
    "            ##############\n",
    "            val_acc = test_model(net, val_loader)\n",
    "            val_scores.append(val_acc)\n",
    "            print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n",
    "\n",
    "            if len(val_scores) == 1 or val_acc > val_scores[best_val_epoch]:\n",
    "                print(\"\\t   (New best performance, saving model...)\")\n",
    "                save_model(net, CHECKPOINT_PATH, model_name)\n",
    "                best_val_epoch = epoch\n",
    "            elif best_val_epoch <= epoch - patience:\n",
    "                print(f\"Early stopping due to no improvement over the last {patience} epochs\")\n",
    "                break\n",
    "\n",
    "        # Plot a curve of the validation accuracy\n",
    "        plt.plot([i for i in range(1,len(val_scores)+1)], val_scores)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Validation accuracy\")\n",
    "        plt.title(f\"Validation performance of {model_name}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    load_model(CHECKPOINT_PATH, model_name, net=net)\n",
    "    test_acc = test_model(net, test_loader)\n",
    "    print((f\" Test accuracy: {test_acc*100.0:4.2f}% \").center(50, \"=\")+\"\\n\")\n",
    "    return test_acc\n",
    "    \n",
    "\n",
    "def test_model(net, data_loader):\n",
    "    \"\"\"\n",
    "    Test a model on a specified dataset.\n",
    "    \n",
    "    Inputs:\n",
    "        net - Trained model of type BaseNetwork\n",
    "        data_loader - DataLoader object of the dataset to test on (validation or test)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    true_preds, count = 0., 0\n",
    "    for imgs, labels in data_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = net(imgs).argmax(dim=-1) #returns the indices of the maximum value of all elements\n",
    "            # dim = the dimension to reduce\n",
    "            true_preds += (preds == labels).sum().item()\n",
    "            count += labels.shape[0]\n",
    "    test_acc = true_preds / count\n",
    "    return test_acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train one model for each activation function.\n",
    "\n",
    "*Runtime Warning - this may take over 15 minutes to run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for act_fn_name in act_fn_by_name:\n",
    "    print(f\"Training BaseNetwork with {act_fn_name} activation...\")\n",
    "    set_seed(42)\n",
    "    act_fn = act_fn_by_name[act_fn_name]()\n",
    "    net_actfn = BaseNetwork(act_fn=act_fn).to(device)\n",
    "    train_model(net_actfn, f\"FashionMNIST_{act_fn_name}\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the model using the sigmoid activation function fails and does not improve upon random performance (10 classes => 1/10 for random chance). This is because the gradient of the input layer parameters on the loss function is negligible compared to the other layers.\n",
    "All the other activation functions gain similar performance. To have a more accurate conclusion, we would have to train the models for multiple seeds and look at the averages. However, the \"optimal\" activation function also depends on many other factors (hidden sizes, number of layers, type of layers, task, dataset, optimizer, learning rate, etc.), meaning a thorough grid search would not be useful in our case.\n",
    "In the literature, activation functions that have shown to work well with deep networks are all types of ReLU functions that we experimented with here, with small gains for specific activation functions in specific networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the activation distribution \n",
    "\n",
    "After we have trained the models, we can look at the actual activation values that are found inside the model. This may provide further insight into the model's performance.\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "    \n",
    "Write a simple function which takes a trained model, applies it to a batch of images, and plots the histogram of the activation function outputs inside the network. Use the histograms to answer the following questions:\n",
    "    \n",
    "- How many neurons are set to zero in ReLU? \n",
    "- Where do we find most values in Tanh?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_activations(net, color=\"C0\"):\n",
    "    activations = {}\n",
    "    \n",
    "    net.eval()\n",
    "    small_loader = data.DataLoader(train_set, batch_size=1024)\n",
    "    imgs, labels = next(iter(small_loader))\n",
    "    with torch.no_grad():\n",
    "        layer_index = 0\n",
    "        imgs = imgs.to(device)\n",
    "        imgs = imgs.view(imgs.size(0), -1)\n",
    "        # We need to manually loop through the layers to save all activations\n",
    "        for layer_index, layer in enumerate(net.layers[:-1]):\n",
    "            imgs = layer(imgs)\n",
    "            activations[layer_index] = imgs.view(-1).cpu().numpy() #reshapes tensor\n",
    "    \n",
    "    ## Plotting\n",
    "    columns = 4\n",
    "    rows = math.ceil(len(activations)/columns)\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(columns*2.7, rows*2.5))\n",
    "    fig_index = 0\n",
    "    for key in activations:\n",
    "        key_ax = ax[fig_index//columns][fig_index%columns]\n",
    "        sns.histplot(data=activations[key], bins=50, ax=key_ax, color=color, kde=True, stat=\"density\")\n",
    "        key_ax.set_title(f\"Layer {key} - {net.layers[key].__class__.__name__}\")\n",
    "        fig_index += 1\n",
    "    fig.suptitle(f\"Activation distribution for activation function {net.config['act_fn']['name']}\", fontsize=14)\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    plt.show()\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, act_fn_name in enumerate(act_fn_by_name):\n",
    "    net_actfn = load_model(model_path=CHECKPOINT_PATH, model_name=f\"FashionMNIST_{act_fn_name}\").to(device)\n",
    "    visualize_activations(net_actfn, color=f\"C{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model with **sigmoid** activation was not able to train properly, the activation outputs (or 'activations') are also less informative and are all gathered around 0.5 (this is what the activation function returns for an input of 0).\n",
    "\n",
    "**Tanh** shows more diverse behavior. \n",
    "- In the input layer, we see a larger number of neuron outputs close to -1 and 1 (the expected outputs when the gradients are close to 0), whereas the values in the two consecutive layers are closer to zero. This is probably because the input layers look for specific features in the input image, and the consecutive layers combine those together. \n",
    "- The activations for the last layer are again more biased to the extreme values. This is because neurons in the middle layers are simply identifying features and weighting their importance, and there can be many such features. However, the last layer is the true classifier layer. Each neuron must produce the probability that a data instance belongs to a particular set on not, and thus returning extreme values is more instructive.\n",
    "\n",
    "**ReLU** has a strong peak at 0, as we initially expected. The effect of having no gradients for negative values is that the network does not have a Gaussian-like distribution after the linear layers, but instead has a longer tail towards the positive values. \n",
    "\n",
    "**LeakyReLU** shows very similar behaviour, whereas **ELU** follows a more Gaussian-like distribution. The **Swish activation** seems to lie in between, although it is worth noting that Swish uses significantly higher activations than other activation functions (up to a value of 20).\n",
    "\n",
    "We see that all activation functions show slightly different behaviuor, although obtaining similar performances for our simple network. Thus it is apparent that the selection of the \"optimal\" activation function really depends on many factors, and is not the same for all possible networks.\n",
    "\n",
    "\n",
    "The tanh shows a more diverse behaviour. While for the input layer we see a larger amount of neurons (neuron outputs?) to be close to -1 and 1 (where the gradients are close to zero), the values in the two consecutive layers are closer to zero. This is probably because the input layers look for specific features in the input image, and the consecutive layers combine those together. The activations for the last layer are again more biased to the extreme points. \n",
    "\n",
    "\n",
    "because the classification layer can be seen as a weighted average of those values (the gradients push the activations to those extremes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Dead Neurons [^](#index) <a id='dead-neurons'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One known drawback of the ReLU activation is the occurrence of \"dead neurons\". These are neurons with **no gradient** (or only a small gradient) for any training input. For ReLU, this is true if the neuron returns 0 for all the given input data.\n",
    "\n",
    "A neural network trains by calculating the gradient of the loss function wrt each layer's parameters. If the gradient calculated when backpropogating is close to 0, the associated parameters will update very slowly (or not at all). This an cause the training to slow or to stall.\n",
    "\n",
    "A neuron first calculates `'z'`, the output of the linear function, before passing this value to the activation function. If this `'z'` is negative, the ReLU function will then always return 0, resulting in a dead neuron.\n",
    "\n",
    "Considering the large number of neurons we have in a neural network, it is not unlikely for this to occur. \n",
    "\n",
    "To get a better understanding of how much of a problem this is, and when we need to be careful, we will measure how many dead neurons different networks have. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "    \n",
    "Implement a function which runs the network on the whole training set and records whether a neuron is exactly 0 for all data points or not.\n",
    "\n",
    "We will then use this function to measure the number of dead neurons in an untrained ReLU activation function network:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def measure_number_dead_neurons(net):\n",
    "\n",
    "    # For each neuron, we create a boolean variable initially set to 1. If it has an activation unequals 0 at any time,\n",
    "    # we set this variable to 0. After running through the whole training set, only dead neurons will have a 1.\n",
    "    neurons_dead = [\n",
    "        torch.ones(layer.weight.shape[0], device=device, dtype=torch.bool) for layer in net.layers[:-1] if isinstance(layer, nn.Linear)\n",
    "    ] # Same shapes as hidden size in BaseNetwork\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(train_loader, leave=False): # Run through whole training set\n",
    "            layer_index = 0\n",
    "            imgs = imgs.to(device)\n",
    "            imgs = imgs.view(imgs.size(0), -1)\n",
    "            for layer in net.layers[:-1]:\n",
    "                imgs = layer(imgs)\n",
    "                if isinstance(layer, ActivationFunction):\n",
    "                    \n",
    "                    # Are all activations == 0 in the batch, and we did not record the opposite in the last batches?\n",
    "                    neurons_dead[layer_index] = torch.logical_and(neurons_dead[layer_index], (imgs == 0).all(dim=0))\n",
    "                    layer_index += 1\n",
    "    number_neurons_dead = [t.sum().item() for t in neurons_dead]\n",
    "    print(\"Number of dead neurons:\", number_neurons_dead)\n",
    "    print(\"In percentage:\", \", \".join([f\"{(100.0 * num_dead / tens.shape[0]):4.2f}%\" for tens, num_dead in zip(neurons_dead, number_neurons_dead)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "net_relu = BaseNetwork(act_fn=ReLU()).to(device)\n",
    "measure_number_dead_neurons(net_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only a minor amount of neurons are dead, but that the number of dead neurons increases with the depth of the layer.\n",
    "\n",
    "This is not too great of an issue if we only have a small number of dead neurons, such as in this case. As weights are updated in the training process, the inputs to these later layers will change too. These new inputs could allow the dead neuron to become \"alive\"/active again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "    \n",
    "### Exercise\n",
    "\n",
    "Measure the number of dead neurons for different activation functions. Are the results what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "net_elu = BaseNetwork(act_fn=ELU()).to(device)\n",
    "measure_number_dead_neurons(net_elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at this for a **trained** ReLU network (with the same initialisation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_relu = load_model(model_path=CHECKPOINT_PATH, model_name=\"FashionMNIST_relu\").to(device)\n",
    "measure_number_dead_neurons(net_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dead neurons has indeed decreased in the later layers. However, dead neurons are especially problematic in the input layer. The training data is fixed and thus the inputs to the input layer will not change over epochs. Training the network **cannot** make these neurons become active.\n",
    "\n",
    "Usually, the input data has a sufficiently high standard deviation to reduce the risk of dead neurons (the range of inputs is quite varied).\n",
    "\n",
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "    \n",
    "Investigate how the number of dead neurons changes with increasing layer depth. For instance, try building a 10-layer neural network with 256 nodes in the first 5 layers and 128 nodes in the last 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "net_relu = BaseNetwork(act_fn=ReLU(), hidden_sizes=[256, 256, 256, 256, 256, 128, 128, 128, 128, 128]).to(device)\n",
    "measure_number_dead_neurons(net_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dead neurons is significantly higher than before which harms the gradient flow, especially in the first iterations. For instance, more than 56% of the neurons in the pre-last layer are dead which creates a considerable bottleneck.\n",
    "It is therefore advisible to use other nonlinear activation functions like Swish for very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Conclusion [^](#index) <a id='conclusion'></a>\n",
    "\n",
    "In this notebook, we have reviewed a set of six activation functions in neural networks (sigmoid, tanh, ReLU, LeakyReLU, ELU, and Swish), and discussed how they influence the gradient distribution across layers.\n",
    "\n",
    "Sigmoid tends to fail in deep neural networks since the highest gradient it provides is 0.25. This leads to vanishing gradients in early layers. \n",
    "\n",
    "All ReLU-based activation functions performed well and, besides the original ReLU, do not have the issue of dead neurons. \n",
    "\n",
    "When implementing your own neural network, it is recommended to start with a ReLU-based network and select the specific activation function based on the properties of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "# Appendix <a id='appendix'></a>\n",
    "\n",
    "[Return to Index](#index)\n",
    "\n",
    "In this section, you will find longer pieces of mathematics and code which are **non-examinable**. Please read at your own discretion\n",
    "\n",
    "- A. [PyTorch backwards() Function](#backwards_app)\n",
    "- B. [Backpropagation with Matrices](#matrix_app)\n",
    "- C. [Cross-Entropy](#cross)\n",
    "- D. [Activation Functions Overview](#act_fn_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "## A. PyTorch backwards() Function [^^](#appendix) <a id='backwards_app'></a>\n",
    "    \n",
    "[Return to Visualising Activation Functions](#visualising-act)     \n",
    "    \n",
    "_References: [tutorialspoint](https://www.tutorialspoint.com/how-to-compute-gradients-in-pytorch), PyTorch: [A Gentle Introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html), Medium: Abishek Bashyall - [Playing with .backward() method in Pytorch](https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0)_\n",
    "    \n",
    "_For further explanation, click [here](https://stackoverflow.com/questions/57320830/why-torch-sum-before-doing-backward) and [here](https://stackoverflow.com/questions/57248777/backward-function-in-pytorch/57249287#57249287) to view two Stack Overflow discussions_\n",
    "    \n",
    "PyTorch is designed to create neural networks, where calculating the gradient in back propagation is vital. Therefore when we create a Torch tensor, we can specify if it is a variable from which we would like to calculate the derivatives. We do this by setting `requires_grad = True` (it is False by default). \n",
    "    \n",
    "To calculate the gradient of a function wrt this variable, we must then call the `autograd.backward()` (or just `backward()` function on our output. Two such examples are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print (x.grad) # Expect 2*x = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., ) # We would not need this gradient for back propagation\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(5.0, requires_grad = True)\n",
    "\n",
    "y = w * x + b\n",
    "print(\"y:\", y)\n",
    "\n",
    "# Compute gradients by calling backward function for y\n",
    "y.backward()\n",
    "\n",
    "# Access and print the gradients w.r.t x, w, and b\n",
    "dx = x.grad\n",
    "dw = w.grad\n",
    "db = b.grad\n",
    "print(\"x.grad :\", dx) # We did not require a gradient for this variable\n",
    "print(\"w.grad :\", dw) # Expect x = 3\n",
    "print(\"b.grad :\", db) # Expect 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "We can see what happens if we have a function act on another, for example by multiplying our input y by 2. Unlike w and b, we will be unable to perform y.grad as y is not a **leaf nodes**; it is calculated from a function of leaf node. Hence we will obtain an error. If we did want to find the gradient wrt y, we would use the `retain_grad()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3., ) # We would not need this gradient for back propagation\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(5.0, requires_grad = True)\n",
    "\n",
    "y = w * x + b\n",
    "\n",
    "# Uncomment the line below to retain the gradient\n",
    "#y.retain_grad()\n",
    "\n",
    "z = 2 * y\n",
    "\n",
    "z.backward()\n",
    "\n",
    "db = b.grad\n",
    "dw = w.grad\n",
    "dy = y.grad\n",
    "\n",
    "print(\"w.grad :\", dw) # Expect x = 6\n",
    "print(\"b.grad :\", db) # Expect 2\n",
    "print(\"y.grad :\", dy) # Expect 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "By default, `backward()` is called on a scalar tensor since the function is unable to calculate non-scalar derivatives. If the output is not of this form, the code will not run. An example of this is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y_arr= x_arr ** 2 \n",
    "y_arr.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "    \n",
    "One way to get past this would be to simply create a scalar tensor that would make backwards() produce the required gradients - for example the sum of all the elements in our output vector: $\\Sigma y_i^{(j)}$. Since this sum is unweighted, this method can be described as that of _equal gradient flow_. Note that our output elements $y_i^{(j)}$ only depend on $x_i^{(j)}$:\n",
    "    \n",
    "\\begin{equation}    \n",
    "\\frac{\\partial \\Sigma y_i^{(j)}}{\\partial x_k^{(l)}} = \\Sigma \\frac{\\partial f(x_i^{(j)})}{\\partial x_k^{(l)}} = \\frac{\\partial f(x_k^{(l)})}{\\partial x_k^{(l)}}\n",
    "\\end{equation}    \n",
    "    \n",
    "`x.grad` would then produce a torch tensor containing all of these derivatives, as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "print (x)\n",
    "print ()\n",
    "\n",
    "# Imaging that squaring our data is the activation function\n",
    "# This would means the gradient should be given by 2*x\n",
    "out = x**2 \n",
    "out.sum().backward()\n",
    "\n",
    "\n",
    "print(x.grad) # This should be 2*input values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "    \n",
    "An alternative way to achieve our desired outcome is by setting the gradient argument in the backward() function. Understanding the principles behind why this works requires knowledge of PyTorch's autograd function. \n",
    "    \n",
    "First, imagine our function is of the form $\\boldsymbol{y} = f (\\boldsymbol{x})$, where **x** and **y** have size n and m respectively. \n",
    "   \n",
    "When we call `backward()`, we are actually calling the default `backward(gradient = torch.tensor[1.])`. We will refer to this gradient using $\\boldsymbol{v}$.. Autograd then performs the following calculations, where J is the **Jacobian**:\n",
    "    \n",
    "\\begin{equation}\n",
    "    J^T \\cdot \\boldsymbol{v} =\n",
    "                              \\begin{bmatrix} \n",
    "                              \\frac{\\partial y_1}{\\partial x_1} & \\dots  & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "                                    \\vdots & \\ddots & \\vdots\\\\\n",
    "                              \\frac{\\partial y_1}{\\partial x_n} & \\dots  & \\frac{\\partial y_m}{\\partial x_n} \n",
    "                              \\end{bmatrix} \\boldsymbol{v}\n",
    "\\end{equation}\n",
    "    \n",
    "If **x** and **y** were the same size, and $y_i$ only depended on $x_i$, we would just have a diagonal matrix. By setting **v** to be the same size as **x** and only contain ones, we would get the equation we were after. We can also see how the simple 1D equation works:\n",
    "    \n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "      \\frac{\\partial y_1}{\\partial x_1} & \\dots  & 0 \\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      0 & \\dots  & \\frac{\\partial y_m}{\\partial x_m} \n",
    "\\end{bmatrix} \n",
    "\n",
    "\\begin{pmatrix} \n",
    "      1 \\\\ \\vdots \\\\ 1 \n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix} \n",
    "      \\frac{\\partial y_1}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_m} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "    \n",
    "If X and Y are matrices rather than vectors, then we can imagine the autograd function working on each column (or row) in turn. The gradient called by backward() must still have the same shape as the matrices X and Y. An example of this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.,3.],[4.,5.,6.]], requires_grad=True)\n",
    "print (x)\n",
    "print ()\n",
    "\n",
    "out = x**2 \n",
    "out.backward(gradient = torch.ones_like(x))\n",
    "\n",
    "print(x.grad) # This should be 2*input values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "## B. Backpropagation with Matrices [^^](#appendix) <a id='matrix_app'></a>\n",
    "    \n",
    "[Return to Gradient Flow](#gradient-flow) \n",
    "    \n",
    "_References: Machine Learning Mastery: [A Gentle Introduction to the Jacobian](https://machinelearningmastery.com/a-gentle-introduction-to-the-jacobian/), PyTorch: [A Gentle Introduction to torch.autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)_\n",
    "    \n",
    "We will use the previously considered backward() function to briefly examine how to use matrices for the chain rule.\n",
    "\n",
    "In most instances, we will call backward() on your loss function in order to calculate the gradients required for optimisation. \n",
    "\n",
    "In this section, we will consider how autograd behaves if **v** is the gradient of some **scalar** function $l$: $v = \\nabla l(\\boldsymbol{y})$, with grad calculated with respect to **y**. Therefore:\n",
    "\n",
    "$$\n",
    "\n",
    "J^T \\cdot \\boldsymbol{v} =\n",
    "                        \\begin{bmatrix} \n",
    "                        \\frac{\\partial y_1}{\\partial x_1} & \\dots  & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "                              \\vdots & \\ddots & \\vdots\\\\\n",
    "                        \\frac{\\partial y_1}{\\partial x_n} & \\dots  & \\frac{\\partial y_m}{\\partial x_n} \n",
    "                        \\end{bmatrix}\n",
    "\n",
    ",\n",
    "\n",
    "\\begin{pmatrix}  \n",
    "      \\frac{\\partial l}{\\partial y_1} \\\\ \\vdots \\\\  \\frac{\\partial l}{\\partial y_m} \\end{pmatrix}  = \\begin{pmatrix}  \\frac{\\partial l}{\\partial x_1} \\\\ \\vdots \\\\  \\frac{\\partial l}{\\partial x_m} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This means that for a loss function $l$, we can calculate all the partial derivatives with respect to the elements of **x** (which could be the weights, the biases etc) by calculating the Jacobian of **y** wrt **x**, and having previously calculated how $l$ depends on **y**. We can then use this differential to calculate the next gradient, and so on - this is a key principle of backpropagation. If **v** contained only ones, as we saw previously, this would mean that a change to any element in **y** would have an equal impact on the loss function - this is why described this ones tensor as having _equal gradient flow_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "## C. Cross-Entropy [^^](#appendix) <a id='cross'></a>\n",
    "\n",
    "[Return to Gradient Flow](#gradient-flow)   \n",
    "\n",
    "Cross-Entropy is a commonly used loss function for classification problems.\n",
    "\n",
    "### Binary Cross-Entropy/Log Loss\n",
    "\n",
    "If we have a binary classification problem, the cross entropy loss function is **the same** as that of the log loss. \n",
    "\n",
    "When this type of model makes a prediction about a data instance's classification, it is actually calculating the **probability** of the instance belonging to each category, and assigning it to the category where the probability is greatest. We use this probability value in our loss function calculations.\n",
    "\n",
    "Imagine we assign a label, $y_i$, to our data points such that belonging to the category has a label value of 1, and not belonging has a label value of 0. The model's predicted probability of a data point belonging to the category is given by $p(x_i)$.\n",
    "\n",
    "The formula for log loss is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    H = - \\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log( p(x_i)) + (1 - y_i) \\cdot \\log(1 - p(x_i))\n",
    "\\end{equation}\n",
    "\n",
    "The label $y_i$ can only be 0 or 1, so for each instance only one of the log probability terms will be included in the sum. Also note that since this a binary classification problem, 1 - p(x_i) is the model's predicted probability that the instance is not in the category.\n",
    "\n",
    "-log(a) tends to infinity as a tends to 0, and -log(1) = 0. Therefore worse prediction values will contribute a larger amount to the loss function, and more accurate predictions will contribute amounts closer to 0. This is what we require for a loss function.\n",
    "\n",
    "### More categories\n",
    "\n",
    "It is easier to work with a binary model than when there are multiple outputs. Thus we choose to convert a multiple category model to a binary type. \n",
    "\n",
    "Let there be m possible categories for our dataset to fall into. We now give each data instance m labels, where the label $y_i^{(k)}$ is 1 or 0 depending on whether the instance is in the $k^{th}$ category or not. The categories are discrete, so only one label value will be non-zero. This data engineering method is called **one-hot encoding**. $p_k(x_i)$ denotes the model's predicted probability of the data instance belonging to category k.\n",
    "\n",
    "To calculate the cross-entropy, we can then simply sum over each respective category:\n",
    "\n",
    "\\begin{equation}\n",
    "    H = - \\frac{1}{N} \\sum_{j=1}^m \\sum_{i=1}^N y_i^{(m)} \\cdot \\log( p_k(x_i)) + (1 - y_i^{(m)})) \\cdot \\log(1 - p_k(x_i))\n",
    "\\end{equation}\n",
    "### Connection to Entropy\n",
    "\n",
    "If you have previously studied thermodynamics or information theory, you will be able to see why this loss function is related to **entropy**. For example, consider the similarities of the loss function to the equation for Gibbs entropy: $S_G = -\\sum p_i \\log(p_i)$, where $p_i$ is the probability of the system being in a given microstate.\n",
    "\n",
    "_References: Medium: Daniel Godoy - [Understanding binary cross-entopy/log loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a), Medium: Vlastimil Martinek - [Cross-entropy for Classification](https://towardsdatascience.com/cross-entropy-for-classification-d98e7f974451)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "## D. Activation Functions Overview [^^](#appendix) <a id='act_fn_app'></a>\n",
    "\n",
    " \n",
    "### Sigmoid\n",
    "\n",
    "The sigmoid function has the form: $$f(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x + 1}$$\n",
    "\n",
    "\n",
    "### Tanh\n",
    "\n",
    "The tanh function has the form: \n",
    "    \n",
    "$$ f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "### ReLU\n",
    "\n",
    "ReLU stands for \"Rectified Linear Unit\", and has the form: \n",
    "    \n",
    "$$ f(x) = \\max(0,x) =  \\begin{cases}\n",
    "x, & x > 0 \\\\\n",
    "0, & x \\leq 0\n",
    "\\end{cases}$$\n",
    "    \n",
    "### Leaky ReLU\n",
    "\n",
    "The Leaky ReLU function had the form: \n",
    "    \n",
    "$$ f(x) =  \\begin{cases}\n",
    "  x, & x > 0 \\\\\n",
    "  ax, & x \\leq 0\n",
    " \\end{cases}, \\textrm{ } (a<1).$$\n",
    " \n",
    " \n",
    "### ELU\n",
    "\n",
    "ELU stands for \"Exponential Linear Unit\". Notice that when $\\alpha=0$, it reduces to the ReLU function. ELU has the form:\n",
    "    \n",
    "$$ f(x) = \\begin{cases}\n",
    "  x, & x > 0 \\\\\n",
    "  \\alpha(e^x-1), & x \\leq 0\n",
    " \\end{cases}$$\n",
    "    \n",
    "\n",
    " \n",
    " \n",
    "### Swish\n",
    "\n",
    "The Swish function is the sigmoid function multipied by $x$. It has the form:\n",
    "$$f(x) = \\frac{x}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "[1] Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). [Paper link](https://arxiv.org/abs/1710.05941) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
