{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "1. Intended learning outcomes\n",
    "2. Histograms\n",
    "3. Plotting error bars\n",
    "4. Line fitting\n",
    "5. Optional: other fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intended learning outcomes<a id=\"outcomes\"></a>\n",
    "After this session, you should be able to:\n",
    "- plot a histogram of a 1D data set\n",
    "- plot error bars on scatter plots\n",
    "- fit a linear model to your data and plot both fit and data\n",
    "- find the uncertainties on your fit parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: cyan\"> Do you have any questions about the last Computing session? This is a good moment to talk with your demonstrator if you are still unsure about any of the concepts.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Histograms<a id=\"histograms\"></a>\n",
    "In this section you will practise the array statistics you learnt last session, as well as learn to create histograms. Histograms are frequently used when displaying a set of repeated measurements, and are therefore an incredible useful tool to use for your labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise 1: before you move on, import the relevant packages for plotting and scientific calculations and below. Which will you need?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the matplotlib pyplot package it is straightforward to create a histogram of a data array. For the following example, we use the \"Dataset.txt\" file, which you can find in the 'Data' folder. This data file contains 20 measurements of the speed of light (in units of $10^8\\mathrm{ms}^{-1}$). The code below creates a histogram of the data. Inspect the code and subsequently run the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('Data/Dataset.txt')# read the data from file\n",
    "plt.ylabel(\"Number of measurements\")# set the y-label\n",
    "plt.xlabel(\"Speed (m/s)\")# set the x-label\n",
    "plt.hist(data)# create a histogram of the data\n",
    "plt.show()# show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately see that most measurements are clustered around $c = 3.0 \\times 10^8 \\,\\rm m/s$, but there appears to be one outlier at $\\sim 2.5 \\times 10^8 \\,\\rm m/s$. It should be clear that it is much easier to spot outliers when you plot a histogram than by looking at a table of individual measurements.\n",
    "\n",
    "The question is whether this outlier is a mistake. If it is, we can disregard it when we calculate the mean and standard error of the mean. \n",
    "<p>\n",
    "<div style=\"background-color: #00FF00\">\n",
    "\n",
    "**Exercise 2: inspect the data and calculate the mean and standard deviation of the entire sample to get an idea of how far away the point is. Would it be reasonable to assume a mistake was made in this measurement?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python to make a new data sample without this data point using the NumPy <span style=\"color:blue\">delete()</span> function (remember that the elements in the array start counting at 0). Note however that we do not 'delete' the data point from our data set altogether - we still record the outlier and keep it in our data file (it would be terribly bad practice to simply altogether discard data that doesn't match our expectations!). We use the <span style=\"color:blue\">delete()</span> function to create a new array without the outlier, so we can do further statistics on this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=np.delete(data,5)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise 3: recalculate the mean and standard deviation, and display a histogram of the new data sample. Has the result changed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "We often would like to compare two datasets or plots visually; for this purpose it is helpful to plot two (or more) graphs side by side. This can be done using the <span style=\"color:blue\">plt.subplot()</span> function to present them in a grid structure that contains $n \\times m$ plots, where $n$ is the number of rows and $m$ is the number of columns. You call the <span style=\"color:blue\">plt.subplot()</span> function like this:\n",
    "\n",
    "```python\n",
    "plt.subplot(n,m,k)\n",
    "```\n",
    "\n",
    "where n is the number of rows you want, m is the number of columns and k is the number of the plot you are creating at this moment in time.\n",
    "\n",
    "**Exercise: Use the <span style=\"color:blue\">plt.subplot()</span> function to plot the two histograms you created above side by side.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "As physicists you will take many measurements in labs and research projects, so you need to become skilled data analysts. For example, you will need to have a good enough working knowledge of statistics to be able to decide how many measurements you need to take to obtain a satisfactory uncertainty on your estimate of the physical quantity you are measuring. The example below helps you see the effect of working with small and large data sets in practice.\n",
    "<p>\n",
    "Below, we will analyze five sets of data, each having 20 points. These numbers have been generated using a random number generator, and are all drawn from a normal distribution with the same mean and standard deviation. We will plot the histograms of the data sets and calculate their mean, standard deviation, and standard error of the mean to see if they are what we would expect.\n",
    "<p>\n",
    "    \n",
    "**Exercise: read the data stored in the file \"five_datasets.txt\" (in the 'Data' folder) into Python. Next, for each dataset, plot a histogram and calculate the mean and standard deviation. Do the means of each set of data come out close to each other?  Are the standard deviations of each set the same?  What is the standard error of the mean of each set?  Are the differences between the means of all the sets similar to the values of the calculated standard errors?**\n",
    "\n",
    "Tip: again use the <span style=\"color:blue\">plt.subplot()</span> function to plot your histograms in a grid for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "Now we will investigate what happens when we combine the five datasets into one dataset of 100 measurements.\n",
    "<p>\n",
    "    \n",
    "**Exercise: plot a histogram of the combined dataset (including all 100 measurements) and calculate the mean, standard deviation, and standard error of the mean. Compare this to the results from the individual datasets comprising 20 measurements. What is the difference? Write your combined dataset to a file (i.e. one column with 100 data points) and save your final histogram. Remember to save all your output in the 'Output' folder!**\n",
    "\n",
    "Hint: you can flatten a 2D-array into a 1D-array by using the <span style=\"color:blue\">flatten()</span> function. For example, the following line of code would flatten a (pre-existing) 2D array called 'data_array' and store it into a 1D array called 'new_1Darray':\n",
    "\n",
    "```python\n",
    "new_1Darray = data_array.flatten()\n",
    "```\n",
    "<span style=\"color:#FFF8C6\">.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plotting error bars<a id=\"errorbars\"></a>\n",
    "So far we have learnt to plot our data using linear plots, scatter plots, and histograms. However, normally all data we take will have errors associated with it. Our plots should include these errors in the form of error bars. Fortunately this is straightforward with matplotlib as it includes the function <span style=\"color:blue\">errorbar()</span> which creates a plot with error bars for us. \n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise 4: have a look at the help for the <span style=\"color:blue\">errorbar()</span> function to see which input arguments it takes. Pay particular attention to the keywords <span style=\"color:blue\">yerr</span>, which takes an array that includes the y-error bars, and <span style=\"color:blue\">fmt</span>, with which you specify the plotting symbol.**\n",
    "\n",
    "**Exercise 5: load the resistivity data we used last session (\"Resistivity.txt\" - included in this session's 'Data' folder), create an array containing 5% errors on the resistivity data, and plot a scatter plot of the data including error bars.**\n",
    "\n",
    "Note: the exercise above includes little guidance - this is to help you get used to finding programming solutions to problems you have not encountered before. Start by looking through the help file, and possibly googling example uses of the function. Also feel free to discuss with your fellow students! If you still find you have trouble getting started (don't worry - this is normal for novice coders), skip ahead to the blue box below and after showing your previous results ask your demonstrator to help you on your way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "You can customise many features on your error bar plot. For example, can you find out how to put caps on your error bars (so they are displayed as **T** rather than **I**)? Also, your independent variable may have error bars too, which you would need to add to your plot. \n",
    "\n",
    "**Exercise: add fixed temperature error bars of 2K to your plot. Check the <span style=\"color:blue\">errorbar()</span> documentation for this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "### Error bars on histograms\n",
    "Sometimes we want to plot error bars on histograms. For example, if your histogram displays a number of counts of a certain value, you may want to add Poisson errors to each bin ($\\sigma_{\\rm Poisson} = \\sqrt{N}$, where $N$ is the number of counts in the bin).\n",
    "\n",
    "**Exercise (challenging): below, recreate the histogram you made in the previous section of the full dataset, and add Poisson error bars to each bin.** \n",
    "\n",
    "Tip: besides plotting a histogram, the matplotlib function <span style=\"color:blue\">hist()</span> can return values too. Look this up, and use it to calculate and create the error bars with the <span style=\"color:blue\">errorbar()</span> function. You will need to think carefully about the $x$-positions of the error bars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: cyan\"> Show your histograms and error bar plot and discuss your findings with a demonstrator - and don't forget to note down your results in your Computing Notebook!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Line fitting<a id=\"linefitting\"></a>\n",
    "In the last session we had noticed that the resistivity data appears to show a linear relationship with temperature. In order to find this relationship, we want to fit a straight line to our data. We will do this by using the Numpy routine <span style=\"color:blue\">polyfit()</span>. This routine takes as arguments an array of the x values of the data, an array of the y values, and the *order* of the polynomial (what power of x) - in this case 1 for a straight line. We can then tell it to weight each data point by the inverse of its error with the <span style=\"color:blue\">w</span> keyword, and ask it to return the uncertainty of the fit parameters by setting <span style=\"color:blue\">cov=True</span>. Look at the documentation to see all of the available keyword arguments.\n",
    "\n",
    "The routine returns an array which contains the best fit values for the coefficients of the polynomial ($P[0]$ and $P[1]$) \n",
    "\n",
    "$$ f(x) = P[1] + P[0] x $$\n",
    "\n",
    "and a *covariance matrix* which contains the information on the uncertainties on the fit parameters *i.e.* how well we have measured the slope ($P[0]$) and intercept ($P[1]$).\n",
    "\n",
    "$$ \\left( \n",
    "\\begin{array}\n",
    "\\ C_{00} & C_{10} \\\\\n",
    " C_{01} & C_{11} \n",
    "\\end{array}\\right)$$\n",
    "\n",
    "We are interested in the diagonal elements of this matrix, where for instance the uncertainty on fit parameter P[1] (the intercept) :\n",
    "\n",
    "$$\\sigma_{P[1]}=\\sqrt{C_{11}}$$\n",
    "\n",
    "The other two elements of the covariance matrix describe the covariance between the two different parameters, which is something we do not need to use for our error analysis.\n",
    "\n",
    "The code below returns the linear fit to the Aluminium resistivity data. Carefully look through this code and make sure you understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T,R_Cu,R_Al = np.loadtxt('Data/Resistivity.txt',unpack=True)# Read in the data\n",
    "errors_Al = 0.05*R_Al# Calculate 5% errors\n",
    "errors_Cu = 0.05*R_Cu\n",
    "\n",
    "# The line below stores the fit coefficients in the fit_Al variable, and the covariance matrix in the cov_Al variable.\n",
    "# Note that the input arguments for polyfit() below are:\n",
    "# (1) the independent variable (T)\n",
    "# (2) the dependent variable (R_Al)\n",
    "# (3) the order of the polynomial to be fitted (1)\n",
    "# (4) the weights of each data point (w = 1/errors_Al)\n",
    "# (5) whether or not to return the covariance matrix (cov = True)\n",
    "fit_Al,cov_Al = np.polyfit(T,R_Al,1,w=1/errors_Al,cov=True)\n",
    "print('Aluminium fit coefficients')\n",
    "print(fit_Al)\n",
    "print('covariance matrix')\n",
    "print(cov_Al)\n",
    "\n",
    "sig_0 = np.sqrt(cov_Al[0,0]) #The uncertainty in the slope\n",
    "sig_1 = np.sqrt(cov_Al[1,1]) #The uncertainty in the intercept\n",
    "\n",
    "print('Slope = %.3e +/- %.3e' %(fit_Al[0],sig_0))# Note the %.3e forces the values to be printed in scientific notation with 3 decimal places.\n",
    "print('Intercept = %.3e +/- %.3e' %(fit_Al[1],sig_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the convenient <span style=\"color:blue\">poly1d()</span> routine from the NumPy package, which takes the fit parameters returned by <span style=\"color:blue\">polyfit()</span> and returns a function which calculates the corresponding fit values at any given point. We then plot the linear fit on top of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the fit \n",
    "pAl=np.poly1d(fit_Al)\n",
    "print('Aluminium polynomial')\n",
    "print(pAl)\n",
    "\n",
    "# Create the original data figure with error bars\n",
    "plt.grid()\n",
    "plt.xlabel(\"Temperature (K)\") \n",
    "plt.ylabel(\"Resistivity (Ohm m)\") \n",
    "plt.title(\"Resistivity Plot\") \n",
    "plt.errorbar(T,R_Cu,yerr=errors_Cu, fmt='o', mew=2, ms=3, capsize=4)\n",
    "plt.errorbar(T,R_Al,yerr=errors_Al, fmt='o', mew=2, ms=3, capsize=4)\n",
    "plt.legend([\"Copper\", \"Aluminium\"], loc=2 ) \n",
    "plt.xticks(np.arange(200, 400, 50))\n",
    "\n",
    "# Overlay the linear fit\n",
    "# Note that we create the y-coordinates for the fit data points by calling pAl \n",
    "# (which was the return value of poly1Dfit), with the x-coordinates stored in T as the input argument.\n",
    "plt.plot(T,pAl(T))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is quite a complicated piece of code. It is therefore important to understand what exactly is going on. Answer the below questions to help you gain a better understanding - make sure to note the answers down in your Computing Notebook.\n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise 6: To understand what exactly is returned by the function <span style=\"color:blue\">polyfit()</span>, find out the data types of <span style=\"color:blue\">fit_Al</span> and <span style=\"color:blue\">cov_Al.</span>**\n",
    "\n",
    "**Next, to check what the command <span style=\"color:blue\">pAl(T)</span>  does, plot the linear fit with crosses as symbols, rather than a line. In order to do this, edit the following line in the code cell above:**\n",
    "```python\n",
    "plt.plot(T,pAl(T))\n",
    "```\n",
    "**Use the <span style=\"color:blue\">pAl()</span> command to calculate the predicted value of the Resistivity at 250 K and 400 K. Check that your answer is sensible by inspecting the above plot of the fit together with the data.**\n",
    "\n",
    "**Now explain the <span style=\"color:blue\">pAl(T)</span> command to a neighbour and/or a demonstrator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise 7: now we have created and plotted the linear fit to the Aluminium resistivity data, can you do the same for the Copper resistivity data? Your final output should be a scatter plot of the data including error bars and both linear fits.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "### A further investigation into the impact of measurement uncertainties.\n",
    "The data we have used so far has been nearly ideal: there is little variance around the linear fit. In practice, data is often a lot noisier, i.e. the measured data points will show more scatter around the fit. To illustrate this, have a look at the file \"noisy_data.txt\" (in the 'Data' folder), which includes a noisier set of measurements of the Aluminium resistivity. The file contains three columns: the temperature, the resistivity measurement, and an error on the resistivity measurement. Upon inspection of the file, you will notice that the measurement errors are variable; below we will investigate the impact of the size of the errors on the linear fit.\n",
    "\n",
    "**Exercise: start by reading in the noisy data set, and plotting it (with error bars) below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "As you can see, the data points are clearly more scattered, although the linear relationship is still apparent. \n",
    "<p>\n",
    "    \n",
    "**Exercise: below, create two fits to the data: one that weights each data point by the inverse of its error, and one that does not take the errors into account. Overplot both on your data plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "You should now see that the unweighted fit tries to take all data points into account equally. However, the weighted fit takes into account that the third and last data points have large error bars and therefore creates a steeper fit to accommodate the points with small error bars. \n",
    "<p>\n",
    "    \n",
    "**Exercise: investigate this further by manually adjusting the size of the uncertainties of individual data points. Can you predict how the fit will change?**\n",
    "\n",
    "The work you have just done highlights the importance of assigning realistic uncertainty estimates to your datapoints. Note however that in reality you should *never* manually adjust the size of the error bars just to create the fit you want! You should always be able to justify the size of your uncertainty estimate - if all measurements were taken in a similar way they will usually have a similar absolute or fractional error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical example\n",
    "\n",
    "Let's consider an experiment where the resistance of a pair of identical resistors is to be found. In this experiment, a measurement is made of the voltage difference across the two resistors and the current running through them is also measured. The resistance of each resistor can be described by the equation:<p>\n",
    "$$R= \\frac{1}{2}\\frac{V_1 −V_2}{I},$$<p>\n",
    "where $V_1$ and $V_2$ are the voltages at the two ends of the resistors and $I$ is the current through them.\n",
    "\n",
    "Consider two approaches to finding the value of $R$:\n",
    "\n",
    "1. We take one measurement of each of $V_1$, $V_2$ and $I$ and accept the equipment manufacturer’s error estimates giving the following values: $V_1 = 6.9 \\pm 0.5 \\rm\\, V$, $V_2 = 0.7 \\pm 0.1 \\rm \\,V$ and $I = 0.43 \\pm 0.03\\rm\\, A$. Find a value for $R$ and its error $\\sigma_R$ using the appropriate methods for combining errors. <p>\n",
    "\n",
    "2. We take a series of measurements of $V_1$, $V_2$ and $I$ with results as given in [Resistors.csv](https://cclewley.github.io/ComputingYr1/Data2/Resistors.csv). Plot $(V_1 − V_2)$ against $I$ and use a linear fit to find $R$ and $\\sigma_R$.\n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise 8: use both methods to calculate the resistance $R$ and its associated error with Python. Do the two approaches give the same results?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: cyan\"> Show your linear fits and error bar plots to a demonstrator - and don't forget to note down your findings in your Computing Notebook!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "## 5. Optional: other fitting models<a id=\"nonlinear\"></a>\n",
    "### Higher-order polynomial fits\n",
    "So far we have only considered fitting straight lines to a dataset. More complex relationships might require the relationship to be represented by e.g. a higher-order polynomial or an exponential function. In the following exercise we will try to fit a second-order polynomial to the atmospheric CO$_2$ concentration, which can be found in \"CO2_data.csv\" (in the 'Data' folder).\n",
    "\n",
    "**Exercise: read in the CO$_2$ data set and plot it.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "The CO$_2$ concentration varies periodically; this is caused by the change in uptake of CO$_2$ by vegetation during the seasons. However, there is also a year-on-year increase in the CO$_2$ concentration. \n",
    "\n",
    "\n",
    "**Exercise: fit the trend with a straight line, and overplot the result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "It doesn't look like a straight line is the appropriate fit to the trend. This is even more clearly seen when a residuals plot is created, which shows the data minus the fit. \n",
    "\n",
    "**Exercise: below, subtract the fitted values from the measured data points and show the residuals plot. Important note: store your residuals in a variable called 'residuals', for further use later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "This shows there is clearly further structure in the trend. We can use the <span style=\"color:blue\">polyfit()</span> function to fit a second order polynomial to the CO$_2$ concentration data. This will give a fit of the form:\n",
    "\n",
    "$$ f(x) = P[2] + P[1] x + P[0] x^2. $$\n",
    "\n",
    "Here $P[0]$, $P[1]$, and $P[2]$ are the fit parameters. A third order fit would give:\n",
    "\n",
    "$$ f(x) = P[3] + P[2] x + P[1] x^2 + p[0] x^3. $$\n",
    "\n",
    "The <span style=\"color:blue\">polyfit()</span> function can create arbitrarily high orders of polynomial fits; however it is best to use the lowest order of polynomial that gives a good fit in order to avoid 'overfitting'.\n",
    "\n",
    "**Exercise: fit a polynomial of a higher order to the data and recreate your data plot with the fit overplotted. Also recreate the residual plot. Which order of polynomial would you pick to fit the trend?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "### Non-linear fits\n",
    "\n",
    "If you plot your residuals as a line plot, you can see the periodic variability in the data. To test whether this really is a yearly cycle (as posited earlier), we are going to fit a sine function to the residuals, of the form:\n",
    "\n",
    "$$ f(t) = A\\sin\\Big({\\frac{2\\pi}{T}t+\\phi}\\Big) $$\n",
    "\n",
    "Here $A$ is the amplitude, $T$ is the period, $t$ is time, and $\\phi$ is the phase offset of the sine function. This is an example of a non-linear function because one of the coefficients, in this case $T$, is within the sine function (see the Advanced worksheet for a full definition of linear vs non-linear functions). \n",
    "\n",
    "There is no pre-built routine that fits a sine function; instead we have to use the generic <span style=\"color:blue\">curve_fit()</span> function which is in the scipy.optimize package. The <span style=\"color:blue\">curve_fit()</span> function allows us to define our own fit function. Below is the example code to create our sine function fit. Carefully read through the code before running it; note how the fit function is defined in a function that we have called <span style=\"color:blue\">my_sin</span>, and we have to specificy an initial guess for the fit parameters. We do this because the fit is non-linear; all non-linear fits have to be done via iteration, where an initial guess is refined until it convergences onto an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit# Import the function curve_fit from the optimize package in Scipy\n",
    "\n",
    "year,CO2=np.loadtxt('Data/CO2_data.csv',skiprows=2, delimiter=',',unpack=True)# Load the data\n",
    "\n",
    "# This is the function we want to fit - you will learn how to create a function from scratch in the next session.\n",
    "def my_sin(t, period, amplitude, phase):\n",
    "    return amplitude*np.sin(t * 2*np.pi/period + phase)\n",
    "\n",
    "# Our initial guess for the parameters\n",
    "guess_period = 1# Period in years\n",
    "guess_amplitude = 2\n",
    "guess_phase = np.pi\n",
    "\n",
    "p0=[guess_period, guess_amplitude, guess_phase]# Array of initial parameter values\n",
    "\n",
    "# now do the fit\n",
    "# curve_fit arguments: \n",
    "# 1. the name of the function to fit (my_sin)\n",
    "# 2. the independent function values (year)\n",
    "# 3. the dependent function values to be fitted \n",
    "#    (note that here we are fitting to the data stored in a variable named 'residuals' which you evaluated earlier)\n",
    "# 4. an array with the initial parameter values (p0 = p0)\n",
    "fit = curve_fit(my_sin, year, residuals, p0=p0)\n",
    "# The fit variable contains the optimized parameters as its first element, and the covariance matrix as its second element.\n",
    "print('The fit parameters are: ',fit[0])\n",
    "\n",
    "# recreate the fitted curve using the optimized parameters\n",
    "# The *fit[0] notation 'unpacks' the first element of the fit. It is the same as saying: \n",
    "# fit[0][0], fit[0][1], fit[0][2] (each of which contains one of the optimized variables)\n",
    "data_fit = my_sin(year, *fit[0])\n",
    "\n",
    "# Create a plot of the fit with the residuals overlaid.\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Residuals')\n",
    "plt.plot(year,data_fit)\n",
    "plt.plot(year,residuals)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "You can see that the peaks and troughs in the residuals are well matched by the fit (although our fit has a fixed amplitude whereas the residuals clearly are more variable). If you pay attention to the printed fit parameters, you will see that the fitted period is very close to 1, i.e. there is indeed a yearly cycle in the data. Our initial guess of a period of 1 year was very good, which allowed the fitting routine to find the optimized parameters rapidly and accurately. It is very important to have good starting values, otherwise the fit might make no sense at all. You often only notice this when you actually plot the fit on top of the data! \n",
    "<p>\n",
    "    \n",
    "**Exercise: our particular fit function is extremely sensitive to the starting value of the period: try and change the initial guess for the period in the code above and see what happens. What happens when you change the starting guess for the other fit parameters?**\n",
    "<p>\n",
    "We have now fitted the CO$_2$ concentration data with a sine function to represent the yearly variability and a polynomial to represent the overall trend. The two fits added together are our best fit for the data as a whole. \n",
    "<p>\n",
    "    \n",
    "**Exercise: plot a graph of the full CO$_2$ concentration data with the total fit overlaid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "Normally when we try and fit a function, we first decide what kind of function makes physical sense. When you expect a linear relationship, it is sensible to fit a straight line. In the data above however, we fitted a polynomial of arbitrary order to the trend, without a theoretical reason for it. In the case of the atmospheric CO$_2$ concentration, it is more common to fit an exponential curve (a good fit would then indicate \"exponential growth\"). \n",
    "<p>\n",
    "    \n",
    "**Exercise: create an exponential fit to the trend by using the <span style=\"color:blue\">curve_fit()</span> function. To do this, first copy the <span style=\"color:blue\">my_sin</span> function from the code cell above, change its name to <span style=\"color:blue\">my_exp</span> and alter it to return a function of the form:**\n",
    "\n",
    "$$f(t) = P[2] + P[1]e^{P[0]t}$$\n",
    "\n",
    "**Here $P[0]$, $P[1]$, and $P[2]$ are the fit parameters, and $t$ is the time in years. Make sure your <span style=\"color:blue\">curve_fit()</span> function calls your new <span style=\"color:blue\">my_exp</span> function instead of <span style=\"color:blue\">my_sin</span>!**\n",
    "\n",
    "Hint: subtract the starting year from your time array so that it starts at 1 instead of around 1960. Also, as you have seen when we tried to fit a sine function, the initial guess of the fit is very important. To make an educated guess of the initial parameters, first plot the data and your guess by hand so you can choose an offset and exponent that are at least of the right order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: cyan\"> Show your CO2 fits and plots to a demonstrator and discuss your findings.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "### Example: scanning experiment fitting\n",
    "The data below, contains what we think to be a Gaussian “feature” in a scanning experiment (it could be a spectral line in optics or a mass scan in particle physics or many other physical situations). This “feature” sits on top of a background. You can assume the background is linear, but not necessarily flat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([ 100.,  101.,  102.,  103.,  104.,  105.,  106.,  107.,  108.,\n",
    "        109.,  110.,  111.,  112.,  113.,  114.,  115.,  116.,  117.,\n",
    "        118.,  119.,  120.,  121.,  122.,  123.,  124.,  125.,  126.,\n",
    "        127.,  128.,  129.,  130.,  131.,  132.,  133.,  134.,  135.,\n",
    "        136.,  137.,  138.,  139.,  140.,  141.,  142.,  143.,  144.,\n",
    "        145.,  146.,  147.,  148.,  149.])\n",
    "y = np.array([ 49.62351587,  53.28471702,  70.91469338,  59.347993  ,\n",
    "        68.14021339,  47.96831052,  46.27111237,  47.75670421,\n",
    "        54.27684285,  49.79369344,  41.06072665,  43.95823631,\n",
    "        47.32189537,  57.59658829,  45.18881389,  56.58866369,\n",
    "        50.1240723 ,  42.6683265 ,  41.92068127,  25.85132307,\n",
    "        46.60808952,  38.07772697,  56.7941344 ,  72.65808868,\n",
    "        54.94790233,  84.97532661,  82.91663574,  54.97400141,\n",
    "        33.05466879,  31.07916615,  30.20075973,  23.87661428,\n",
    "        21.81024528,  28.83273888,  17.92903416,  27.57351836,\n",
    "        25.05685919,  27.4866161 ,  31.13142432,  20.26710321,\n",
    "        16.80327091,  14.1787744 ,  20.52320243,  22.65700337,\n",
    "        16.23164708,  11.78551076,  15.21441115,  12.58749491,\n",
    "        12.88956241,   6.24572757])\n",
    "y_error = np.array([ 7.74596669,  7.68114575,  7.61577311,  7.54983444,  7.48331477,\n",
    "        7.41619849,  7.34846923,  7.28010989,  7.21110255,  7.14142843,\n",
    "        7.07106781,  7.        ,  6.92820323,  6.85565464,  6.78233068,\n",
    "        6.70821365,  6.63335528,  6.55833372,  6.48664491,  6.43341394,\n",
    "        6.44498198,  6.61337546,  7.02586908,  7.63076491,  8.17847123,\n",
    "        8.36660027,  8.05527104,  7.36400523,  6.58504642,  5.97802099,\n",
    "        5.61585191,  5.42114516,  5.29873213,  5.1972821 ,  5.09915702,\n",
    "        5.00001304,  4.89898045,  4.79583158,  4.69041576,  4.5825757 ,\n",
    "        4.47213596,  4.35889894,  4.24264069,  4.12310563,  4.        ,\n",
    "        3.87298335,  3.74165739,  3.60555128,  3.46410162,  3.31662479])\n",
    "\n",
    "plt.errorbar(x, y, y_error, fmt='o',capsize=2)\n",
    "plt.xlabel('scan value')\n",
    "plt.ylabel('signal')\n",
    "plt.title('Scan experiment data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "    \n",
    "**Exercise: use the <span style=\"color:blue\">curve_fit()</span> function to fit a straight line for the background and a Gaussian function for the feature. Hence, find the amplitude and width of the Gaussian feature. Note that in this case it is not a good idea to fit the line first and afterwards the Gaussian from the residuals, as the Gaussian will influence the line fit. Instead, fit both background and the Gaussian feature at once.**\n",
    "\n",
    "This last exercise is challenging and integrates everything you have learnt about fitting so far. First write down in your Computing Notebook what the function you want to fit would look like. Which are the parameters you need to fit? Remember to make a good first guess of the fitting parameters by closely the inspecting the plot above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: cyan\"> Show your scanning experiment fit and plots to a demonstrator - and don't forget to note down your findings in your Computing Notebook!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please complete the [Mentimeter Poll](https://www.menti.com/h8cqct33vx) for this session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
