{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Array Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the core worksheet for session 1 we restricted ourselves to working with Numpy array data structures. As physicists this was natural; arrays are a n-dimensional structure that can hold integers or floats and allow statistics to be performed on them. Python however offered multiple types of data structures that each serve an unique and useful purpose when programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists\n",
    "\n",
    "Lists in Python are defined using square brackets surrounding zero or more comma separated literals: \n",
    "```python\n",
    "some_primes = [2,3,5,7,11,13]\n",
    "names_of_cats = [\"Ginger\", \"Princess\", \"Zorxo the Clawful\"]\n",
    "```\n",
    "\n",
    "Lists don't even have to be of the same type:\n",
    "\n",
    "```python\n",
    "Mixed_list = [2,\"Python\",16.5]\n",
    "```\n",
    "\n",
    "is an allowed list. Moreover lists don't need to contain any elements, they can be initalised as an empty list:\n",
    "\n",
    "```python\n",
    "Empty_list=[]\n",
    "```\n",
    "This is particularly useful when you will be adding elements to the list as your code progresses. To add elements you can use the <span style=\"color:blue\">.append</span> method to a list:\n",
    "\n",
    "```python\n",
    "Empty_list.append(2)\n",
    "```\n",
    "\n",
    "\n",
    "### Tuples\n",
    "Tuples behave very similarly to lists, but are immutable (i.e. they cannot be changed). Tuple literals are created by a writing a sequence of items separated by commas, optionally surrounded by parentheses. To get a tuple with only one element, you need to have a comma after the element.<br>\n",
    "```python\n",
    "my_tuple = 1,2,3\n",
    "my_tuple = (1,2,3)        # equivalent\n",
    "not_a_tuple = 1           # same as: not_a_tuple=1\n",
    "a_tuple = 1,\n",
    "a_tuple = (\"first!\",)     # here the first and only element of the tuple is \"first!\".\n",
    "```\n",
    "\n",
    "Many aspects of Python are implicit tuples. For instance, the assignment operator = will happily assign tuples of names to tuples of values:<br>\n",
    "```python\n",
    "A,B,C = 1,2,3\n",
    "```\n",
    "which is the same as:\n",
    "```python\n",
    "(A,B,C) = (1,2,3)\n",
    "```\n",
    "which is the same as:\n",
    "```python\n",
    "A = 1\n",
    "B = 2\n",
    "C = 3\n",
    "```\n",
    "\n",
    "This behaviour can be easily used to swap the names of data:<br>\n",
    "```python\n",
    "A,B = 1,2\n",
    "A,B = B,A\n",
    "print(A,B)   # prints 2,1\n",
    "```\n",
    "\n",
    "### Dictionaries\n",
    "The third most common collection type used in Python is the Dictionary, or dict, which store mappings from keys to values. For every key, there is a value, which can be almost any Python object. Keys are usually strings, but it is possible to use certain other objects as keys. Dictionary literals are written as a comma-separated list of key:value pairs, with a colon separating key from value, surrounded by (curly) braces. Dict items are accessed using the key within square brackets.<br>\n",
    "```python\n",
    "student_grades = {\"Simon\": 60, \"Jenny\":68, \"Laura\":112}\n",
    "student_grades[\"Laura\"] = 100 # Change Laura's grade.\n",
    "student_grades[\"Pug\"] = 58    # New student!\n",
    "print(student_grades[\"Jenny\"])\n",
    "68\n",
    "```\n",
    "<br>\n",
    "\n",
    "### When should you use each data type?\n",
    "\n",
    "An important question to ask yourself when first thinking about a problem is what data structure is the best for you to use? There is no universal answer for us and will really depend on the situation. When to use a dictionary is pretty straightforward, they are used when you want to link one set of values to another. A phonebook which links a person name to their number is a good use of a dictionary.\n",
    "\n",
    "When to use a list or tuple can be harder to understand. The key difference between between lists and tuple is that tuples are immutable, i.e once created they cannot be altered. Therefore a sequence of tuples can be used if you don't expect the sequence (or indeed) want them to change. However if you want to add or remove elements from a sequence during your code execution then lists are the data structure to go with.\n",
    "\n",
    "For most physics applications, we will be dealing with 2D, 3D or even higher dimensionality data that we need to operate on. This is best achieved using arrays contained within the numpy module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objects and methods\n",
    "\n",
    "When looking at lists I used a word <span style=\"color:blue\">append</span> to add an element onto a list, this was called a method. What are methods, how do they work, how are they called and what are the implications of them? To understand this we need to dig a little deeper about how data structures are actually defined in Python.\n",
    "\n",
    "In Python, the data structures are you encounter are stored as objects. Objects are a way of storing definitions, methods, variables and many other things in a package that is modular, i.e. it is self-contained. \n",
    "\n",
    "As an example, imagine we has an object square, that is the shape square. Within that object square, there would need to be variables that define the length of the square. Additionally there can be methods that would calculate its area or perimeter. As another example consider an object bike, which can have variables that define the colour, the number of gears and methods that calculate the maximum speed that could be achieved.\n",
    "\n",
    "Coming back to data structures, the data structure objects contain definitions that define how data is stored within them, and then methods that allow you to manipulate that structure. This course will not consider objects explicitly but will merely make use of them. In next year's Computing course you will start to create your own objects and associated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data types and data structures\n",
    "\n",
    "In section 4, it was shown that variables can be converted from one data type to another. The exact same can be done with data structures, whether it is to change the data types of the structure, or change the actual structure itself. The first thing to consider is: What is my data structure type and what do I want it to be? For physicists the most common change will be to go between a list and an array. To find the type of a data structure, use the <span style=\"color:blue\">type</span> command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Array=np.array([1,2,3,4])\n",
    "print(type(Array))\n",
    "List=[1,2,3,4]\n",
    "print(type(List))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the data structure is crucial to ensuring you handle your data in the most efficient way. For example the Numpy library is optimised to run operations on arrays. We can convert a list to an array but calling the <span style=\"color:blue\">np.array</span> function and passing it the list as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array_of_list=np.array(List)\n",
    "print(Array_of_list,type(Array_of_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go the other way, we use the <span style=\"color:blue\">np.ndarray.tolist</span> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_array=np.ndarray.tolist(Array)\n",
    "print(List_of_array,type(List_of_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For arrays, we can also look to see the type of data that is stored within it by appending our array with a <span style=\"color:blue\">.dtype</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Array.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data type is int32, which is a 32-bit integer (the 32-bit refers to how it is stored). Once the data type of an array is known, it can be converted by applying <span style=\"color:blue\">.astype</span> to the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array_float=Array.astype(float)\n",
    "print(Array_float.dtype)\n",
    "Array_complex=Array.astype(complex)\n",
    "print(Array_complex.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00FF00\">\n",
    "\n",
    "**Exercise: the following data structures are either tuples,lists or arrays, query their data types. For each data structure, convert them to the other data structures: i.e. convert the array to a list and tuple, the list to an array and list, and the list to an array and tuple:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=(4,5,6)\n",
    "c=[7,8,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Structures - The Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading files in session 1, all of the data that was loaded in had some fundamental drawbacks: they all had to the same data type and there was no information on what the data itself represented. For small and simple data sets the np.loadtxt() will normally be sufficient, but when text files contain different data types or if you want to open for example an Excel spreadsheet, then the loadtxt function is insufficient. The Python library Pandas allows you to open and manipulate complex data in an intuitive fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening and examing a file with Pandas\n",
    "\n",
    "The first thing to is to import the pandas library using:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "To open a file with pandas, the most common function is the read_csv command where csv stands for comma separated variable. To open the resistivity data used in session 1, we use:\n",
    "\n",
    "```python\n",
    "df=pd.read_csv('Data/Resistivity2.csv')\n",
    "```\n",
    "\n",
    "(Note that this is a slightly different .csv file specifically used in this advanced worksheet).\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise: open the Resistivity2.csv data contained with the name df using the pd.read_csv command. Use the type command and print what data structure has been generated, and then print the data itself. What does this return?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the data you are reading in might not be in a comma separated variable format, there may be whitespaces or semicolons. To ensure that the data is read in properly you will need to pass the delim_whitespace keyword to tell the function how the data is separated. Look at the read_csv documentation to learn about the other options available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrame\n",
    "\n",
    "The data strucure that has been generated by the read_csv is called a DataFrame, this is at the heart of how Pandas stores and manipulates data. A DataFrame is a 2D data structure that is composed of the following components:\n",
    "\n",
    "1) The data\n",
    "\n",
    "2) The index\n",
    "\n",
    "3) The columns\n",
    "\n",
    "Looking as the DataFrame printed above the data should be obvious, it is all of the numbers that was contained in the Resistivity2.csv file. The index, which is the row number of the DataFrame, defines all of the instances when data was taken. In this case data was taken every 20 K between 200-360 K. The columns contain the data that was taken at each index, which for this data was the restivity of copper and aluminium.\n",
    "\n",
    "There are two important notes about the DataFrame that should be noted now. The top of the DataFrame contains what was in the top of the file that was loaded in, in this instance it was the labels of the resistivity data. These are known as headers, and will allow you to access your data without needing to use indices. You should be careful when loading in data that it has header data, or else Pandas will place your first data row into that slot. \n",
    "\n",
    "The first column of the DataFrame contains the numbers 0-9. These are the index labels that can be used to access rows of data. These indices were generated automatically because we did not tell it what to use. Looking at our data however, we can see a more convenient index to use: The temperature of our data! To set the indices of our DataFrame to be the temperature, we can use the index_col keyword argument when reading in our data or use the set_index function on our DataFrame. Look up these two methods to understand.\n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise: using the set_index function, make a new DataFrame df2 that has the temperature column as the index. Then, create a new DataFrame df3 that sets the index column to temperature during read in. Print these two DataFrames to look at their structure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger data sets, it can be disadvantageous to print the whole DataFrame to the screen. We can look at only parts of the data using the .head() and .tail() functions on the DataFrames.\n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise: use the .head() and .tail() functions to look at the select parts of the DataFrame df2. How many rows does it print out by default? How can you change that? Look at the documentation to help you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sometimes don't need to look at the information in the DataFrame itself, we only want a top level summary of the data. This is achieved using the DataFrame.info() command.\n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise: use the DataFrame.info() command to generate a top level summary of the DataFrames df2/df3. What do you see?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame\n",
    "Often, we are dealing with data that is not in a format that can immediately be turned into a DataFrame as it may be missing headers or an index. It is then down to the user to create the necessary information to turn the data into a DataFrame compatible format. To do this we use the pd.DataFrame function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as sp\n",
    "\n",
    "d=[[2,3,'e',5],[4,3,'f',5],[5,3,'g',4]]\n",
    "Headers=['a','b','c','d']\n",
    "df=pd.DataFrame(data=d,columns=Headers)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one of the column values are strings and not numbers; this is one of the big advantages of using a DataFrame to store your data.\n",
    "\n",
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "    \n",
    "**Exercise: create a DataFrame from the following array:**\n",
    "\n",
    "```python\n",
    "\n",
    "Array=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\n",
    "\n",
    "```\n",
    "\n",
    "**with the following header:**\n",
    "\n",
    "```python\n",
    "header=['a','b','c','d']\n",
    "```\n",
    "<p>\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting with Pandas\n",
    "\n",
    "Being able to store heterogeneous data in a single data structure is useful, but the real power of Pandas comes when it comes to plotting using a DataFrame. This is accomplished by the following line of code:\n",
    "\n",
    "```python\n",
    "df['column name'].plot()\n",
    "```\n",
    "\n",
    "Alternatively you can use:\n",
    "\n",
    "```python\n",
    "df.plot('x column name','y column name')\n",
    "```\n",
    "Note how we only reference the name of the column, we don't need to know its index. For the first method we didn't set an x-axis; with that plotting nomenclature Pandas will use whatever the index is as an x-axis. Let's look at these methods of plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # this needs to be imported to show the plot\n",
    "\n",
    "df=pd.read_csv('Data/Resistivity2.csv')\n",
    "df['Resistivity Cu (ohm/m)'].plot()\n",
    "df.plot('Temperature (K)','Resistivity Cu (ohm/m)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second case we had to use the DataFrame df, which didn't have temperature set as the index. In the first plotting style it has taken the DataFrame index, in this case temperature, and has set it to be the x-axis. Plotting data without having to reference column numbers is more intuitive and will make your code easier to understand.\n",
    "\n",
    "**It however relies on the programmer to be careful when making the column names into something sensible, so take care !!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #00FF00\">\n",
    "\n",
    "**Exercise - Plot the Al resisitivty data using both styles of plotting. Be sure to include a title and y label in your plot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a more general note, we can access columns by querying their column name, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Temperature (K)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering DataFrames\n",
    "\n",
    "So far we have made use of the whole DataFrame. A powerful feature of the DataFrame is when you have a large amount of data and want to analyse only a small subset of it based on a condition that is already within the data. As an example, we can filter the above DataFrame df to only contain temperatures below 300 eV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df[df['Temperature (K)'] < 300]\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that compared to the original DataFrame, we have 4 less values. This becomes powerful when we have a large 2D DataFrame that contains lots of data that are grouped by the value contained in a certain column. Look at the following DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df=pd.read_csv('Data/Car_Data.csv')\n",
    "print(car_df.info())\n",
    "print('')\n",
    "print(car_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame contains information about various aspects of 392 cars. There is a lot of information here, however we will focus only focus on a few columns to illustrate some key features of DataFrames. Let's plot the weight of the car versus the mpg, the miles per gallon it can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df.plot.scatter(x='weight',y='mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used the scatter plot function and not just the plot function. This is because the DataFrame is not sorted in ascending weight. Looking at the plot, we can see a clear trend that is not suprising: heavier cars get worse mileage. Looking at the summary of the DataFrame, we see that there is an origin column that says where the car was made. To see all the different values contained in the origin column, we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(car_df.origin.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using the command:\n",
    "\n",
    "```python\n",
    "print(car_df.mpg.unique())\n",
    "```\n",
    "\n",
    "would be a bad idea as there are lots of different values of mpg; we can see this from the graph we plotted! Just as we filtered our resistivity DataFrame based on temperature, we can filter our car data based on region. The syntax for this is a little different from above, and looks like:\n",
    "\n",
    "```python\n",
    "New_df=Old_df[old_df['column_name'].str.contains('condition')]\n",
    "```\n",
    "\n",
    "So to extract the US car data from the DataFrame, we would use:\n",
    "\n",
    "```python\n",
    "car_df_US=car_df[car_df['origin'].str.contains('US')]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00FF00\">\n",
    "\n",
    "**Exercise: extract the data for each origin into a separate DataFrame, like the code snippet above. Then plot the weight vs mpg of each car region on the same graph in different colours to answer the question: which region produces cars with the worst mpg?**\n",
    "\n",
    "**Hint: To get each scatter plot on a common axis, you will need to use the ax keyword argument. Consult Google, the Pandas documentation, and Stack Overflow about how to go about this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
