{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reinforcement Learning \n",
                "\n",
                "Reinforcement Learning is the cutting-edge approach in artificial intelligence that empowers machines to learn by interacting with their environment. Just like a skillful player mastering a game, this innovative technique enables AI to make smart decisions and improve performance through trial and error. By rewarding positive outcomes and penalising mistakes, Reinforcement Learning paves the way for autonomous agents that learn to navigate complex challenges and conquer the unknown."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<hr style=\"border:2px solid gray\">\n",
                "\n",
                "## Index: <a id='index'></a>\n",
                "1. [Reward System](#reward-system)\n",
                "1. [Reinforcement Learning in Action](#RL)\n",
                "1. [Q-Learn](#QL)\n",
                "1. [Q-Value and Hyper-Parameters](#QV)\n",
                "1. [Using Dictionary](#Dic)\n",
                "1. [Reinforcement Learning - \"a Game of Rock, Paper, Scissors\"](#Game)\n",
                "1. [Exercise: Write Your Own Game](#write)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Real life example:\n",
                "AlphaGo, the revolutionary AI developed by DeepMind, employs Reinforcement Learning as a crucial component of its strategy. Through a combination of supervised learning from human expert games and reinforcement learning by playing against itself, AlphaGo hones its skills and adapts its gameplay. This reinforcement learning process allows AlphaGo to refine its moves, prioritise winning strategies, and continuously evolve, eventually achieving superhuman proficiency in the intricate game of Go. The result is a monumental breakthrough in the world of AI and a testament to the power of Reinforcement Learning in conquering complex challenges. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## In this notebook...\n",
                "\n",
                "we will go through a simple example of reinforcement learning.\n",
                "\n",
                "A game of rock paper scissors is designed, you may try to write your own example to implement a simple game.\n",
                "\n",
                "What we want is to let you the player choose an option and the computer to also do the same thing, then we decide if you or the player has won the game:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"background-color: #FFF8C6\">\n",
                "\n",
                "## Exercise: Write your version of this implementation..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The set of code below is a version of implementation:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "#### This is how a basic one to one game of rock paper and scissors work\n",
                "def users_choice():\n",
                "    print(\"Let's play rock, paper, scissors\")\n",
                "    while True:\n",
                "        choice = input(\"Enter your choice (rock, paper, or scissors): \")\n",
                "        if choice in [\"rock\", \"paper\", \"scissors\"]:\n",
                "            return choice\n",
                "        else:\n",
                "            print(\"Invalid choice. Try again.\")\n",
                "\n",
                "\n",
                "def comp_trial():\n",
                "    options = [\"rock\", \"paper\", \"scissors\"]\n",
                "    choice = random.choice(options)\n",
                "    return choice\n",
                "\n",
                "\n",
                "def rule(user_choice, comp_choice):\n",
                "    '''\n",
                "    This shows all the outcomes of the game,\n",
                "    the if statements can be shortened to three\n",
                "    '''\n",
                "    if user_choice == comp_choice:\n",
                "        return \"draw\"\n",
                "    elif user_choice == \"rock\" and comp_choice == \"scissors\":\n",
                "        return \"user wins\"\n",
                "    elif user_choice == \"scissors\" and comp_choice == \"rock\":\n",
                "        return \"computer wins\"\n",
                "    elif user_choice == \"paper\" and comp_choice == \"rock\":\n",
                "        return \"user wins\"\n",
                "    elif user_choice == \"rock\" and comp_choice == \"paper\":\n",
                "        return \"computer wins\"\n",
                "    elif user_choice == \"scissors\" and comp_choice == \"paper\":\n",
                "        return \"user wins\"\n",
                "    elif user_choice == \"paper\" and comp_choice == \"scissors\":\n",
                "        return \"computer wins\"\n",
                "\n",
                "\n",
                "comp_choice = comp_trial()\n",
                "user_choice = users_choice()\n",
                "\n",
                "\n",
                "print(\"Your choice:\", user_choice)\n",
                "print(\"I will choose:\", comp_choice)\n",
                "\n",
                "result = rule(user_choice, comp_choice)\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Adding a Reward System  [^](#index)\n",
                "<a id='reward-system'></a>\n",
                "\n",
                "The idea of reinforcement learning is about letting the machine know what outcomes is the outcome that we want to see. To do this we can set up a reward system:\n",
                "\n",
                "Here I want the player to win the game and I will let the code know this by adding in a reward system. In this section we are implementing a reward system and the machine will append these values to be assessed.\n",
                "\n",
                "The game is still being played randomly. How should we then use the statistics to train the model to play the way we want. In this case, play to let the player win."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "def users_choice():\n",
                "\n",
                "    print(\"Let's play rock, paper, scissors\")\n",
                "    while True:\n",
                "        choice = input(\"Enter your choice (rock, paper, or scissors): \")\n",
                "        if choice in [\"rock\", \"paper\", \"scissors\"]:\n",
                "            return choice\n",
                "        else:\n",
                "            print(\"Invalid choice. Try again.\")\n",
                "\n",
                "\n",
                "def comp_trial():\n",
                "    options = [\"rock\", \"paper\", \"scissors\"]\n",
                "    choice = random.choice(options)\n",
                "    return choice\n",
                "\n",
                "\n",
                "def get_reward(user_choice, comp_choice):\n",
                "    if user_choice == comp_choice:\n",
                "        return 0  # Draw\n",
                "    elif (\n",
                "        (user_choice == \"rock\" and comp_choice == \"scissors\")\n",
                "        or (user_choice == \"scissors\" and comp_choice == \"paper\")\n",
                "        or (user_choice == \"paper\" and comp_choice == \"rock\")\n",
                "    ):\n",
                "        return 1  # Win\n",
                "    else:\n",
                "        return -1  # Lose\n",
                "\n",
                "\n",
                "def play_game():\n",
                "    user_choice = users_choice()\n",
                "    comp_choice = comp_trial()\n",
                "\n",
                "    print(\"Your choice:\", user_choice)\n",
                "    print(\"I will choose:\", comp_choice)\n",
                "\n",
                "    reward = get_reward(user_choice, comp_choice)\n",
                "    return reward\n",
                "\n",
                "\n",
                "num_episodes = 15\n",
                "total_reward = 0\n",
                "\n",
                "for episode in range(num_episodes):\n",
                "    reward = play_game()\n",
                "    total_reward += reward\n",
                "\n",
                "# visualise reward, so far we are not using the reward yet\n",
                "average_reward = total_reward / num_episodes\n",
                "print(\"Average Reward over {} episodes: {}\".format(num_episodes, average_reward))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reinforcement Learning in Action <a id='RL'></a>[^](#index)\n",
                "\n",
                "\n",
                "\n",
                "Let's discuss the concept of reinforcement learning further using this game.\n",
                "\n",
                "In this game, the game can be considered as the **environment**. The hand you choose to play: `input(\"Enter your choice (rock, paper, or scissors): \")` is the **state**.\n",
                "\n",
                "The computer who plays against you the player is the **agent** and the **state** represents the current situation or configuration of the **environment** that the **agent** observes. In the case of the rock-paper-scissors game, the state is not just the hand you choose to play `[\"rock, \"paper, \"scissors\"]`, but it also includes the computer's hand, as it influences the outcome of the game. So, the **state** is a combination of both your hand and the opponent's hand.\n",
                "\n",
                "After this transition, the **agent** receives a penalty or reward - with winning bringing a `+1` reward, losing bringing a `-1` penalty and drawing being a neutral action.\n",
                "\n",
                "The **policy** is then the strategy of choosing an action that gives better outcomes considering the reward system. It's a mapping from states to actions, indicating what action the agent should take in a given state. The policy can be deterministic, meaning it always chooses the same action in a specific state, or it can be stochastic, where it selects actions probabilistically. \n",
                "\n",
                "How willing the code is to selecting actions randomly/exploring different routes, would be determined by **Epsilon-Greedy exploration** - a technique used to balance exploration and exploitation during the agent's learning process. The agent uses an exploration rate (`epsilon`) to decide whether to explore a new action randomly or exploit the current best action according to the Q-values.\n",
                "\n",
                "<img src=\"https://www.learndatasci.com/documents/14/Reinforcement-Learning-Animation.gif\" alt=\"Reinforcement Learning Animation\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Q-Learning [^](#index)\n",
                "<a id='QL'></a>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Here, we implement Q-learning to enable the **agent (player)** to learn the best actions to take in different states. The agent uses the environment's rewards to update the Q-values over time.\n",
                "\n",
                "The Q-table is a dictionary that maps a `(state, action)` combination to the corresponding Q-value. Each Q-value represents the \"quality\" of the action taken from a specific state. Higher Q-values imply better chances of obtaining greater rewards from that action.\n",
                "\n",
                "For example, the Q-table will have entries like, with the reward system implemented:\n",
                "\n",
                "<center>\n",
                "\n",
                "|          State         |  Action  |  Q-Value  |\n",
                "|------------------------|----------|----------|\n",
                "|    Rock, Opponent=Rock  |  Rock    |   0.0    |\n",
                "|    Rock, Opponent=Rock  |  Paper   |  -1.0    |\n",
                "|    Rock, Opponent=Rock  | Scissors |   1.0    |\n",
                "|    Rock, Opponent=Paper |  Rock    |   1.0    |\n",
                "|    Rock, Opponent=Paper |  Paper   |   0.0    |\n",
                "|    Rock, Opponent=Paper | Scissors |  -1.0    |\n",
                "| Rock, Opponent=Scissors |  Rock    |  -1.0    |\n",
                "| Rock, Opponent=Scissors |  Paper   |   1.0    |\n",
                "| Rock, Opponent=Scissors | Scissors |   0.0    |\n",
                "|           ...          |   ...    |   ...    |\n",
                "\n",
                "</center>\n",
                "\n",
                "\n",
                "In this table, the rows represent different states (e.g., \"Rock, Opponent=Rock\" indicating that the agent chose Rock, and the opponent also chose Rock), the columns represent the available actions (Rock, Paper, Scissors), and the values represent the corresponding Q-values.\n",
                "\n",
                "As the agent explores and interacts with the environment, it updates the Q-values based on the rewards obtained, and future actions are influenced by these Q-values, guiding the agent towards making better decisions in the game.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Q-values are updated using the following equation: <a id='QV'></a>[^](#index)\n",
                "\n",
                "\n",
                "$$\n",
                "Q(s, a) = Q(s, a) + α * [R(s, a) + γ * max(Q(s', a')) - Q(s, a)]\n",
                "$$\n",
                "\n",
                "- `Q(s, a)` is the Q-value of the (state, action) pair.\n",
                "α (alpha) is the learning rate, controlling the impact of new information on the Q-value updates ($0≤α≤1$).\n",
                "- `R(s, a)` is the immediate reward obtained when taking action a in state s.\n",
                "- `γ (gamma)` is the discount factor, determining the importance of future rewards ($0≤γ≤1$).\n",
                "- `max(Q(s', a'))` is the maximum Q-value among all possible actions `a'` in the next state `s'`.\n",
                "- `s'` is the next state after taking action a in state `s`.\n",
                "\n",
                "This Q-value update equation is fundamental to the Q-learning algorithm, allowing the agent to iteratively adjust its Q-values based on the rewards received and the expected maximum future reward from the next state. As the agent explores and interacts with the environment, the Q-values converge to optimal values, guiding the agent towards making better decisions and maximizing cumulative rewards. The learning rate `(α)` and discount factor `(γ)` are hyper-parameters that can be tuned to control the learning process in different environments."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction to Dictionaries in Python <a id='Dic'></a>[^](#index)\n",
                "\n",
                "\n",
                "In Python, a dictionary is a powerful and flexible data structure that allows you to store key-value pairs. It is denoted by curly braces `{}` and consists of keys separated from their corresponding values by a colon `:`. Each key-value pair represents an item in the dictionary. Dictionaries are particularly useful when dealing with data that requires fast and efficient lookup based on unique keys.\n",
                "\n",
                "## Q-Table for the Rock-Paper-Scissors Game\n",
                "\n",
                "In Python dictionaries, including the Q-table, there is no inherent order to the keys. Dictionaries are implemented as hash tables, which are data structures optimised for fast lookup based on keys rather than maintaining a specific order.\n",
                "\n",
                "In the context of the Rock-Paper-Scissors game, we use a dictionary to represent the Q-table. The Q-table maps each `(state, action)` combination to its corresponding Q-value. Here's how we can create the Q-table using a dictionary:\n",
                "\n",
                "```python\n",
                "q_table = {\n",
                "    (state, action): q_value\n",
                "}\n",
                "```\n",
                "\n",
                "We can create the the Q_table into a class\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Rock, Paper and Scissors<a id='game'></a> [^](#index)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class QTable:\n",
                "    def __init__(self, states, actions):\n",
                "        '''\n",
                "        This initialises the q_table as a 3*3 with all the states and actions\n",
                "        A Q-table is created as a dictionary with the key being the state and action and the value being the q_value\n",
                "        '''\n",
                "        self.states = states\n",
                "        self.actions = actions\n",
                "        self.q_table = {\n",
                "            (state, action): 0 for state in self.states for action in self.actions\n",
                "        }\n",
                "\n",
                "    def get_q_value(self, state, action):\n",
                "        '''\n",
                "        The `get_q_value` method is a function that takes in a state and an action as parameters and returns the \n",
                "        corresponding q-value from the q_table. If the q-value does not exist in the q_table, it returns 0 as a \n",
                "        default value.\n",
                "        '''\n",
                "        return self.q_table.get((state, action), 0)\n",
                "\n",
                "\n",
                "    def update_qvalue(self, state, action, reward, alpha, gamma):\n",
                "        '''\n",
                "        The `update_qvalue` method in the `QTable` class is used to update the Q-value in the Q-table based on the\n",
                "        given state, action, reward, learning rate (alpha), and discount factor (gamma).\n",
                "        '''\n",
                "        old_q_value = self.get_q_value(state, action) # access the old q_value\n",
                "        next_max = max(\n",
                "            [self.get_q_value(state, next_action) for next_action in self.actions]\n",
                "        ) # find the next max q_value - method to find the most optimal action\n",
                "        new_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_max) # q-table update equation\n",
                "        self.q_table[(state, action)] = new_q_value\n",
                "\n",
                "\n",
                "states = [\"rock\", \"paper\", \"scissors\"]\n",
                "actions = [\"rock\", \"paper\", \"scissors\"]\n",
                "q_table = QTable(states, actions)\n",
                "num_episodes = int(1000) # loop the game 1000 times"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The user function plays the hand randomly, so we can train the computer to play for a large number episode."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def user_choice():\n",
                "    choice = random.choice(states)\n",
                "    return choice\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we introduce epsilon, and the epsilon greedy value.\n",
                "\n",
                "In the `comp_choice()` function, we access the hyperparameter `epsilon`. The first if statement generates a random number between 0 and 1, and if the value generated is below the epsilon value, we prompt the computer to make a random move so the q_table's q_value can explored further. Else, if the randomly generated number is greater than epsilon, the computer uses its learning outcome from the q_table and play the best possible hand."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def comp_choice(state, epsilon):\n",
                "    '''\n",
                "    This code is implementing the epsilon-greedy strategy for selecting an action.\n",
                "    '''\n",
                "    if random.uniform(0, 1) < epsilon:\n",
                "        action = random.choice(actions)\n",
                "    else:\n",
                "        q_values = [q_table.get_q_value(state, action) for action in actions]\n",
                "        action = actions[np.argmax(q_values)]\n",
                "    return action\n",
                "\n",
                "\n",
                "def get_state(user_choice, comp_choice):\n",
                "    return (user_choice, comp_choice)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Document Reward System\n",
                "\n",
                "Here, a dictionary is used to document successes and failures. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_dict = {\n",
                "    (\"rock\", \"rock\"): 0.0,\n",
                "    (\"rock\", \"paper\"): -1.0,\n",
                "    (\"rock\", \"scissors\"): 1.0,\n",
                "    (\"paper\", \"rock\"): 1.0,\n",
                "    (\"paper\", \"paper\"): 0.0,\n",
                "    (\"paper\", \"scissors\"): -1.0,\n",
                "    (\"scissors\", \"rock\"): -1.0,\n",
                "    (\"scissors\", \"paper\"): 1.0,\n",
                "    (\"scissors\", \"scissors\"): 0.0,\n",
                "}\n",
                "\n",
                "document_result = []\n",
                "\n",
                "\n",
                "def reward(user_choice, comp_choice):\n",
                "    result = results_dict[(user_choice, comp_choice)]\n",
                "    document_result.append(result)\n",
                "    return result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we loop the game for the num_episodes to train the computer to let the player win the game."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "list_of_user_input = []\n",
                "list_of_comp_input = []\n",
                "contin_reward = []\n",
                "\n",
                "\n",
                "def game_loop(alpha, gamma, epsilon, num_episodes):\n",
                "    total_reward = 0\n",
                "    for episode in range(num_episodes):\n",
                "        # Decay the hyperparameter(s)\n",
                "        epsilon *= np.exp(-e_decay_rate * episode)\n",
                "\n",
                "        state = user_choice()\n",
                "        action = comp_choice(state, epsilon)\n",
                "\n",
                "        # visualising results\n",
                "        list_of_user_input.append(state)\n",
                "        list_of_comp_input.append(action)\n",
                "        reward_val = reward(state, action)\n",
                "        total_reward += reward_val\n",
                "        contin_reward.append(total_reward)\n",
                "\n",
                "        q_table.update_qvalue(state, action, reward_val, alpha, gamma)\n",
                "    return contin_reward"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hyper-parameters\n",
                "\n",
                "Q_table usually takes 3 hyper-parameters: `alpha`, `gamma` and `epsilon`, as explained above. \n",
                "\n",
                "A decay factor is a simple way to change the hyperparameter according to the progress of learning. Here we start by making half of the computer's decisions random, by setting `epsilon = 0.5`, then by making the `epsilon` value decay to approach 0, the computer can minimise random decisions makings as the q_table updates.\n",
                "\n",
                "As the `e_decay_rate` decreases exponentially it can also be considered as a hyper-parameter (it's not a constant).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "alpha = 0.3  # learning rate\n",
                "gamma = 0.9  # discount factor\n",
                "epsilon = 0.5  # exploration rate\n",
                "\n",
                "e_decay_rate = 1 / num_episodes\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualising Result\n",
                "\n",
                "Plotting diagrams and displaying tables can aid your understanding on how your code is performing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "game_loop(alpha, gamma, epsilon, num_episodes)\n",
                "\n",
                "# The code snippet is creating a plot to visualize the build-up of the reward score over the iterations of the game.\n",
                "x_var = np.linspace(0, num_episodes, num_episodes)\n",
                "plt.plot(x_var, contin_reward)\n",
                "plt.title(\"Reward Score Build-Up\")\n",
                "plt.xlabel(\"Number of Iterations\")\n",
                "plt.ylabel(\"Reward\")\n",
                "plt.show()\n",
                "\n",
                "record = pd.DataFrame(\n",
                "    {\n",
                "        \"Player's Choices\": list_of_user_input,\n",
                "        \"Computer's Choices\": list_of_comp_input,\n",
                "        \"Result\": document_result,\n",
                "    }\n",
                ")\n",
                "slice_result = record.iloc[-10:, :]\n",
                "print(slice_result) # print the last 10 results of the game\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"background-color: #FFF8C6\">\n",
                "\n",
                "## Exercise: Tweak the Hyper-parameters and Decay_Rate\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<div style=\"background-color: #FFF8C6\">\n",
                "\n",
                "## Write Your Own Game: <a id='write'></a> [^](#index)\n",
                "\n",
                "In real life scenarios, the reward system isn't always as obvious as a game of Rock, Paper and Scissors. Then you may need to put more considerations on how to implement the learning system.\n",
                "\n",
                "We can try to consider the code structure for a game of noughts and crosses, where the outcome of the game isn't instantly obvious.\n",
                "\n",
                "Try to keep the code as general as possible so it can easily be altered to accommodate for any scenario.\n",
                "\n",
                "<center>\n",
                "\n",
                "1|2|3\n",
                "---|---|---\n",
                "4|5|6\n",
                "7|8|9\n",
                "\n",
                "If I play in the top left corner, where can the computer go?\n",
                "\n",
                "X|2|3\n",
                "---|---|---\n",
                "4|5|6\n",
                "7|8|9\n",
                "\n",
                "The computer then can choose options = [2,3,4,5,6,7,9]\n",
                "</center>\n",
                "\n",
                "This is a suggestion, think about the most general way to achieve this, and consider the symmetry of the game."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# How would you initialise the Q-table for a 3x3 grid \"Noughts and Crosses\"?\n",
                "class QTable:\n",
                "    def __init__(self, states, actions):\n",
                "        self.states = states\n",
                "        self.actions = actions\n",
                "        self.q_table = {\n",
                "            (state, action): 0 for state in self.states for action in self.actions\n",
                "        }\n",
                "\n",
                "    def get_q_value(self, state, action):\n",
                "        return self.q_table.get((state, action), 0)\n",
                "\n",
                "    def update_qvalue(self, state, action, reward, alpha, gamma):\n",
                "        ### START CODE HERE ###\n",
                "        # Access the old q_value\n",
                "        # Find the next max q_value\n",
                "        # Update the q_value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# suggestion for the structure of the game\n",
                "\n",
                "def user_choice():\n",
                "    # Ask the user where they want to play\n",
                "    return state\n",
                "\n",
                "\n",
                "def comp_choice(state, epsilon):\n",
                "    # Implement the epsilon-greedy strategy for selecting an action\n",
                "    return action\n",
                "\n",
                "\n",
                "def get_state(user_choice, comp_choice):\n",
                "    return (user_choice, comp_choice)\n",
                "\n",
                "\n",
                "def steps(action, state):\n",
                "    state = user_choice()\n",
                "    action = comp_choice(state, epsilon)\n",
                "    # What are the possible steps from the current state\n",
                "    # e.g. if I play in the top left corner, what are the possible next moves?\n",
                "    return status\n",
                "\n",
                "\n",
                "def result(action, state, status):\n",
                "    # Check if the action is a winning move\n",
                "    # if user/comp wins makes a line across the grid, return 1\n",
                "    # if no one lines are made, return 0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "alpha = ##\n",
                "gamma = ##\n",
                "epsilon = ##\n",
                "num_episodes = ##\n",
                "\n",
                "def game_loop(state, action, alpha, gamma, epsilon, num_episodes):\n",
                "    # Loop through the game\n",
                "    # Decay the hyperparameter(s)\n",
                "    # Update the q_table\n",
                "    return results"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.17",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.17"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "4d56cc16d861913ffbcda9b6300d2a9b1f4537e9ddc8b105371ad79de78aa931"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
