{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8c0b81",
   "metadata": {},
   "source": [
    "# Dimension Reduction and unsupervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fd2ef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "**Recommend reading** for extra information on dimension analysis: https://web.stanford.edu/class/cs246/slides/06-dim_red.pdf and Hands on Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fb983",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "1. [Dimension Reduction](#DR)\n",
    "1. [Principal Component Analysis](#PCA)\n",
    "1. [SVD](#SVD)\n",
    "1. [Projecting down to d-Dimensions](#PDD)\n",
    "1. [Looking at MNIST](#MNIST)\n",
    "1. [Variance Ratio](#VR)\n",
    "1. [How many dimensions](#DC)\n",
    "1. [Exercise 1](#Exercise)\n",
    "1. [Other sorts of PCA](#oPCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fedbf7",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction [^](#index)\n",
    "<a id='DR'></a>\n",
    "\n",
    "We have only examined datasets containing a relatively limited number of features, with MNIST being the largest, consisting of 784 features. However, it's common to encounter datasets with thousands or even millions of features. As you have observed, models trained on datasets with numerous features are significantly slower compared to those built on datasets with only a few features - consider the contrast between the iris data and the MNIST data. Consequently, when dealing with datasets containing a large number of features, the computational speed can become considerably slower.\n",
    "\n",
    "There are further reasons to use dimensionality reduction. Even if you transform all your data (so that each feature has a value range between 0 and 1), adding features will add more dimensions to the hyperspace that you need to fill and characterise/model. If you pick two points at random from a unit square (i.e. a two-featured space), their separation is, on average, approximately 0.52; if you go to a 3D cube, then it grows to approximately 0.66. For a million features, i.e. a 1,000,000D hypercube, this has grown to approximately 408.25. This means that your model is now training on very sparse data which may not be representative. The obvious answer is to use more data to train the model; however, you would need more data than there are atoms in the observable universe to have an average separation of 0.1 for just 100 dimensions (not sure how they calculated the actual number of atoms in the observable universe). This is sometimes called [*the curse of dimensionality*](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "\n",
    "So some form of dimensionality reduction can often be very helpful. However, in any such reduction you will loose some information and you want to minimise this. A good way to consider this is to try to retain the maximum variance in your reduced data set. That way you will (generally) loose the smallest amount of discrimination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efee9e",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) [^](#index)\n",
    "<a id='PCA'></a>\n",
    "\n",
    "While there are many different forms of dimensionality reduction, PCA is by far the most common and so it is the only one that we will really cover, in PCA your data are projected along axes that retain the greatest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7cac5",
   "metadata": {},
   "source": [
    "Consider the diagrams below - don't worry about code that generates them, this is taken directly from {homl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first some basics\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for rotation and stretch\n",
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "# Set seed for reproducibility of random data\n",
    "np.random.seed(3)\n",
    "\n",
    "# Generate random 2D data and scale it down\n",
    "X = np.random.randn(m, 2) / 10\n",
    "\n",
    "# Stretch the data in the direction of the x-axis\n",
    "X = X.dot(np.array([[stretch, 0],[0, 1]]))\n",
    "\n",
    "# Rotate the data by the defined angle\n",
    "X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "# Define three unit vectors for projection in different directions\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])\n",
    "u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])\n",
    "\n",
    "# Project the data onto these three vectors\n",
    "X_proj1 = X.dot(u1.reshape(-1, 1))\n",
    "X_proj2 = X.dot(u2.reshape(-1, 1))\n",
    "X_proj3 = X.dot(u3.reshape(-1, 1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# First subplot: original 2D data with the unit vectors\n",
    "plt.subplot2grid((3,2), (0, 0), rowspan=3)\n",
    "# Plot unit vectors and their respective lines\n",
    "plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], \"k-\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], \"k--\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], \"k:\", linewidth=2)\n",
    "\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "\n",
    "# Plot arrows representing unit vectors\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", fontsize=22)\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", fontsize=22)\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# Second, third, and fourth subplots: projections onto each unit vector\n",
    "for idx, X_proj in enumerate([X_proj1, X_proj2, X_proj3], start=1):\n",
    "    plt.subplot2grid((3,2), (idx-1, 1))\n",
    "    plt.plot([-2, 2], [0, 0], \"k-\" if idx==1 else \"k--\" if idx==2 else \"k:\", linewidth=1)\n",
    "    plt.plot(X_proj[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "    plt.gca().get_yaxis().set_ticks([])\n",
    "    plt.gca().get_xaxis().set_ticklabels([])\n",
    "    plt.axis([-2, 2, -1, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "# Add x-axis label to the last subplot\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb95318",
   "metadata": {},
   "source": [
    "This shows the projection of the data along the axes shown. When taking the PCA the first component is the one with the greatest variance -- in this case **C1**. The second principal component is the one with the greatest remaining variance which is **C2**. **C2** is perpendicular to **C1**. If we had more dimensions then we could define more vectors.\n",
    "\n",
    "To find these vectors, we will use a well known linear algebra technique which factorises matrices - *Single Valued Decomposition* (SVD). \n",
    "\n",
    "Numpy has a function which will return these (although the input must be centered around zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a42429",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "#### [SVD](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf) [^](#index)\n",
    "<a id='SVD'></a>\n",
    "\n",
    "The singular value decomposition of a matrix $A$ is the factorisation of $M$ into the\n",
    "product of three matrices $A = UDV^T$ where the columns of $U$ and $V$ are orthonormal and the matrix $D$ is diagonal with positive real entries.\n",
    "\n",
    "To gain insight into the *SVD*, treat the rows of an n × d matrix A as n points in a\n",
    "d-dimensional space and consider the problem of finding the best k-dimensional subspace with respect to the set of points. Here best means minimize the sum of the squares of the perpendicular distances of the points to the subspace. We begin with a special case of the problem where the subspace is 1-dimensional, a line through the origin. We will see later that the best-fitting k-dimensional subspace can be found by k applications of the best fitting line algorithm. Finding the best fitting line through the origin with respect to a set of points ${\\{x_i|1 ≤ i ≤ n\\}}$ in the plane means minimizing the sum of the squared distances of the points to the line. Here distance is measured perpendicular to the line. The problem is called the best least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X-X.mean(axis=0)\n",
    "\n",
    "U,s, Vh=np.linalg.svd(X_centered) # docs at https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\n",
    "c1=Vh.T[:,0]\n",
    "c2=Vh.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8a12e",
   "metadata": {},
   "source": [
    "## Projecting Down to d - Dimensions [^](#index)\n",
    "<a id='PDD'></a>\n",
    "\n",
    "After identifying the principal components, you can project the data onto a hyperplane of dimension 'd' that's defined by the initial 'd' principal components. This process is designed to minimize the loss of data variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6abe7a",
   "metadata": {},
   "source": [
    "## Looking at MNIST (again) [^](#index)\n",
    "<a id='MNIST'></a>\n",
    "\n",
    "So let's return to our old friend MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist['data']\n",
    "y=mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd7c0",
   "metadata": {},
   "source": [
    "Using PCA is even easier in sklearn than it is in numpy as it does all the zeroing for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92097755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=200) # choose the first 200 components out 784\n",
    "X_reduced=pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the components will then just be stored in pca.components.T i.e. the transpose\n",
    "print(len(pca.components_)) # should be 200 vectors\n",
    "print(len(pca.components_.T[:,0])) # in 784 dimensions\n",
    "print(len(X_reduced)) #should be 60000 from all the original training data\n",
    "print(len(X_reduced[0,:])) # should now only be 200 not 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde4e18",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio [^](#index)\n",
    "<a id='VR'></a>\n",
    "\n",
    "The explained variance ratio indicates the proportion of the dataset's variance that lies along each principal component, so we can see the speed at which the variance drops off. For example, using just the first 10 components in MNIST leads to a variance loss of around 3% (not much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_[0:10])\n",
    "print(\"lost=\",1-pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bdea3",
   "metadata": {},
   "source": [
    "## How many dimensions to choose? [^](#index)\n",
    "<a id='DC'></a>\n",
    "\n",
    "Rather than plucking a number out of a hat, you could decide how much variance you wish to retain and use this to decide the dimensionality that you want to use. You could do this for an increasing number of dimensions and then see which is the first above your threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53964ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad03b03",
   "metadata": {},
   "source": [
    "Sklearn can also do this for you. Rather than n_components being equal to a number of principal components, n_components can instead be the variance ratio that you want to keep (a number between 0 and 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb009e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=0.95)\n",
    "X_reduced=pca.fit_transform(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0e043",
   "metadata": {},
   "source": [
    "You could also plot the explained variance as a function of the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57739405",
   "metadata": {},
   "source": [
    "The value of 154 means that you only need to store $\\approx$ 20% of the data for 95% of the information so you can use this as a form of compression. Look at the example below and note how you can reverse the transformation but that you have lost some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad217872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154) \n",
    "#try varying n_components (starting at 1) and see what difference it makes to the images\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225cba1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise [^](#index)\n",
    "<a id='Exercise'></a>\n",
    "\n",
    "As the comment suggests, try varying the number of dimensions and seeing what difference it makes to the the image. Start with very low numbers and build up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54990225",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Use your favourite classifier (e.g. SVC or BDT) to classify the MNIST data set as well as you can. Then see how the timing and accuracy changes if you use versions with reduced dimensions and plot your reults (say accuracy against number of dimensions). This is what you are really interested in.\n",
    "\n",
    "[SVC: Support Vector Classification; BDT: Boosted Decision Trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05265d97",
   "metadata": {},
   "source": [
    "## Other sorts of PCA [^](#index)\n",
    "<a id='oPCA'></a>\n",
    "\n",
    "We don't have time here but you should be aware that there are also varioants of PCA that can be useful. These include:\n",
    "* **Kernel PCA** where you use a similar kernel trick as with SVMs to introduce nonlinear features (without really doing so).\n",
    "* Randomised PCA that generates good approximations to the PC in a semi-random way and is very much faster for large feature sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31f428",
   "metadata": {},
   "source": [
    "## Other forms of dimensionality reduction\n",
    "\n",
    "There exist many other forms of dimensionality reduction but none anywhere near as popular as PCA. Thes include: Locally Linear Embedding, Random Projections, Linear Discriminant Analysis, ... They all have their place and you should know that there are more out there that exist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
