{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8c0b81",
   "metadata": {},
   "source": [
    "# Dimension Reduction and unsupervised learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54fd2ef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "**Recommend reading** for extra information on dimension analysis: https://web.stanford.edu/class/cs246/slides/06-dim_red.pdf and Hands on Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fb983",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "1. [Dimension Reduction](#DR)\n",
    "1. [Principal Component Analysis](#PCA)\n",
    "1. [SVD](#SVD)\n",
    "1. [Projecting down to d-Dimensions](#PDD)\n",
    "1. [Looking at MNIST](#MNIST)\n",
    "1. [Variance Ratio](#VR)\n",
    "1. [How many dimensions](#DC)\n",
    "1. [Exercise 1](#Exercise)\n",
    "1. [Other sorts of PCA](#oPCA)\n",
    "1. [Unsupervised learning](#UL)\n",
    "1. [K-Means](#KM)\n",
    "1. [Exercise 2](#Exercise_2)\n",
    "1. [Density-Based Spatial Clustering of Applications with Noise](#DBSCAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fedbf7",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction [^](#index)\n",
    "<a id='DR'></a>\n",
    "\n",
    "We have only examined datasets containing a relatively limited number of features, with MNIST being the largest, consisting of 784 features. However, it's common to encounter datasets with thousands or even millions of features. As you have observed, models trained on datasets with numerous features are significantly slower compared to those built on datasets with only a few features - consider the contrast between the iris data and the MNIST data. Consequently, when dealing with datasets containing a large number of features, the computational speed can become considerably slower.\n",
    "\n",
    "There are further reasons to use dimensionality reduction. Even if you transform all your data (so that each feature has a value range between 0 and 1), adding features will add more dimensions to the hyperspace that you need to fill and characterise/model. If you pick two points at random from a unit square (i.e. a two-featured space), their separation is, on average, approximately 0.52; if you go to a 3D cube, then it grows to approximately 0.66. For a million features, i.e. a 1,000,000D hypercube, this has grown to approximately 408.25. This means that your model is now training on very sparse data which may not be representative. The obvious answer is to use more data to train the model; however, you would need more data than there are atoms in the observable universe to have an average separation of 0.1 for just 100 dimensions (not sure how they calculated the actual number of atoms in the observable universe). This is sometimes called [*the curse of dimensionality*](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "\n",
    "So some form of dimensionality reduction can often be very helpful. However, in any such reduction you will loose some information and you want to minimise this. A good way to consider this is to try to retain the maximum variance in your reduced data set. That way you will (generally) loose the smallest amount of discrimination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efee9e",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) [^](#index)\n",
    "<a id='PCA'></a>\n",
    "\n",
    "While there are many different forms of dimensionality reduction, PCA is by far the most common and so it is the only one that we will really cover, in PCA your data are projected along axes that retain the greatest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7cac5",
   "metadata": {},
   "source": [
    "Consider the diagrams below - don't worry about code that generates them, this is taken directly from {homl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first some basics\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for rotation and stretch\n",
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "# Set seed for reproducibility of random data\n",
    "np.random.seed(3)\n",
    "\n",
    "# Generate random 2D data and scale it down\n",
    "X = np.random.randn(m, 2) / 10\n",
    "\n",
    "# Stretch the data in the direction of the x-axis\n",
    "X = X.dot(np.array([[stretch, 0],[0, 1]]))\n",
    "\n",
    "# Rotate the data by the defined angle\n",
    "X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "# Define three unit vectors for projection in different directions\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])\n",
    "u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])\n",
    "\n",
    "# Project the data onto these three vectors\n",
    "X_proj1 = X.dot(u1.reshape(-1, 1))\n",
    "X_proj2 = X.dot(u2.reshape(-1, 1))\n",
    "X_proj3 = X.dot(u3.reshape(-1, 1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# First subplot: original 2D data with the unit vectors\n",
    "plt.subplot2grid((3,2), (0, 0), rowspan=3)\n",
    "# Plot unit vectors and their respective lines\n",
    "plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], \"k-\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], \"k--\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], \"k:\", linewidth=2)\n",
    "\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "\n",
    "# Plot arrows representing unit vectors\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "\n",
    "\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", fontsize=22)\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", fontsize=22)\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "# Second, third, and fourth subplots: projections onto each unit vector\n",
    "for idx, X_proj in enumerate([X_proj1, X_proj2, X_proj3], start=1):\n",
    "    plt.subplot2grid((3,2), (idx-1, 1))\n",
    "    plt.plot([-2, 2], [0, 0], \"k-\" if idx==1 else \"k--\" if idx==2 else \"k:\", linewidth=1)\n",
    "    plt.plot(X_proj[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "    plt.gca().get_yaxis().set_ticks([])\n",
    "    plt.gca().get_xaxis().set_ticklabels([])\n",
    "    plt.axis([-2, 2, -1, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "# Add x-axis label to the last subplot\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb95318",
   "metadata": {},
   "source": [
    "This shows the projection of the data along the axes shown. When taking the PCA the first component is the one with the greatest variance -- in this case **C1**. The second principal component is the one with the greatest remaining variance which is **C2**. **C2** is perpendicular to **C1**. If we had more dimensions then we could define more vectors.\n",
    "\n",
    "To find these vectors, we will use a well known linear algebra technique which factorises matrices - *Single Valued Decomposition* (SVD). \n",
    "\n",
    "Numpy has a function which will return these (although the input must be centered around zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a42429",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "#### [SVD](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/book-chapter-4.pdf) [^](#index)\n",
    "<a id='SVD'></a>\n",
    "\n",
    "The singular value decomposition of a matrix $A$ is the factorisation of $M$ into the\n",
    "product of three matrices $A = UDV^T$ where the columns of $U$ and $V$ are orthonormal and the matrix $D$ is diagonal with positive real entries.\n",
    "\n",
    "To gain insight into the *SVD*, treat the rows of an n × d matrix A as n points in a\n",
    "d-dimensional space and consider the problem of finding the best k-dimensional subspace with respect to the set of points. Here best means minimize the sum of the squares of the perpendicular distances of the points to the subspace. We begin with a special case of the problem where the subspace is 1-dimensional, a line through the origin. We will see later that the best-fitting k-dimensional subspace can be found by k applications of the best fitting line algorithm. Finding the best fitting line through the origin with respect to a set of points ${\\{x_i|1 ≤ i ≤ n\\}}$ in the plane means minimizing the sum of the squared distances of the points to the line. Here distance is measured perpendicular to the line. The problem is called the best least squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X-X.mean(axis=0)\n",
    "\n",
    "U,s, Vh=np.linalg.svd(X_centered) # docs at https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\n",
    "c1=Vh.T[:,0]\n",
    "c2=Vh.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8a12e",
   "metadata": {},
   "source": [
    "## Projecting Down to d - Dimensions [^](#index)\n",
    "<a id='PDD'></a>\n",
    "\n",
    "After identifying the principal components, you can project the data onto a hyperplane of dimension 'd' that's defined by the initial 'd' principal components. This process is designed to minimize the loss of data variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6abe7a",
   "metadata": {},
   "source": [
    "## Looking at MNIST (again) [^](#index)\n",
    "<a id='MNIST'></a>\n",
    "\n",
    "So let's return to our old friend MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist['data']\n",
    "y=mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd7c0",
   "metadata": {},
   "source": [
    "Using PCA is even easier in sklearn than it is in numpy as it does all the zeroing for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92097755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=200) # choose the first 200 components out 784\n",
    "X_reduced=pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the components will then just be stored in pca.components.T i.e. the transpose\n",
    "print(len(pca.components_)) # should be 200 vectors\n",
    "print(len(pca.components_.T[:,0])) # in 784 dimensions\n",
    "print(len(X_reduced)) #should be 60000 from all the original training data\n",
    "print(len(X_reduced[0,:])) # should now only be 200 not 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde4e18",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio [^](#index)\n",
    "<a id='VR'></a>\n",
    "\n",
    "The explained variance ratio indicates the proportion of the dataset's variance that lies along each principal component, so we can see the speed at which the variance drops off. For example, using just the first 10 components in MNIST leads to a variance loss of around 3% (not much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_[0:10])\n",
    "print(\"lost=\",1-pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bdea3",
   "metadata": {},
   "source": [
    "## How many dimensions to choose? [^](#index)\n",
    "<a id='DC'></a>\n",
    "\n",
    "Rather than plucking a number out of a hat, you could decide how much variance you wish to retain and use this to decide the dimensionality that you want to use. You could do this for an increasing number of dimensions and then see which is the first above your threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53964ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad03b03",
   "metadata": {},
   "source": [
    "Sklearn can also do this for you. Rather than n_components being equal to a number of principal components, n_components can instead be the variance ratio that you want to keep (a number between 0 and 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb009e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=0.95)\n",
    "X_reduced=pca.fit_transform(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0e043",
   "metadata": {},
   "source": [
    "You could also plot the explained variance as a function of the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57739405",
   "metadata": {},
   "source": [
    "The value of 154 means that you only need to store $\\approx$ 20% of the data for 95% of the information so you can use this as a form of compression. Look at the example below and note how you can reverse the transformation but that you have lost some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad217872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154) \n",
    "#try varying n_components (starting at 1) and see what difference it makes to the images\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225cba1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise [^](#index)\n",
    "<a id='Exercise'></a>\n",
    "\n",
    "As the comment suggests, try varying the number of dimensions and seeing what difference it makes to the the image. Start with very low numbers and build up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54990225",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Use your favourite classifier (e.g. SVC or BDT) to classify the MNIST data set as well as you can. Then see how the timing and accuracy changes if you use versions with reduced dimensions and plot your reults (say accuracy against number of dimensions). This is what you are really interested in.\n",
    "\n",
    "[SVC: Support Vector Classification; BDT: Boosted Decision Trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05265d97",
   "metadata": {},
   "source": [
    "## Other sorts of PCA [^](#index)\n",
    "<a id='oPCA'></a>\n",
    "\n",
    "We don't have time here but you should be aware that there are also varioants of PCA that can be useful. These include:\n",
    "* **Kernel PCA** where you use a similar kernel trick as with SVMs to introduce nonlinear features (without really doing so).\n",
    "* Randomised PCA that generates good approximations to the PC in a semi-random way and is very much faster for large feature sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31f428",
   "metadata": {},
   "source": [
    "## Other forms of dimensionality reduction\n",
    "\n",
    "There exist many other forms of dimensionality reduction but none anywhere near as popular as PCA. Thes include: Locally Linear Embedding, Random Projections, Linear Discriminant Analysis, ... They all have their place and you should know that there are more out there that exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48541db7",
   "metadata": {},
   "source": [
    "# Unsupervised learning [Ideally want to reformat sessions] [^](#index)\n",
    "<a id='UL'></a>\n",
    "\n",
    "\n",
    "I was rather hoping to spend this entire session on unsupervised learning, but figured that dimensionality reduction was so important that it had to be covered explicitly.\n",
    "\n",
    "As it is I can only give you a taste of this and show you an example of a commonly used approach (and give you an exercise of course).\n",
    "\n",
    "Although you should note that dimensionality reduction is in itself a form of unsupervised learning as you only consider the variance of the data and not the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9b0d",
   "metadata": {},
   "source": [
    "## So what is unsuperived learning and what is it used for?\n",
    "\n",
    "As you might have guessed, unsupervised learning is where you don't have the answer to what you are looking i.e. you don't have the target, so you cannot train your favourite classifier. Somehow you need an algorithm that will train a model to pick things that are \"the same\". This could be trying to distinguish coins on weight and diameter without know which coin is which - this is often used as an example. \n",
    "\n",
    "The most common form is clustering (although fault detection and density estimation are other common forms) and the most common form of clustering is K-Means, so this is the example that we will look at. Again, this is an example taken from {homl}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea75a29",
   "metadata": {},
   "source": [
    "## K-Means [^](#index)\n",
    "<a id='KM'></a>\n",
    "\n",
    "K-means can take an unlabeled data set and group it into clusters. So let's generate some random data made up of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49698f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10882f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be19ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "print(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919013e7",
   "metadata": {},
   "source": [
    "We have five new blobs of data, but how can we make a model identify each blob? If we knew the centroids of each blob or the identity of each entry, this task would be easy. However, in unsupervised learning, we have neither of these.\n",
    "\n",
    "K-Means starts by randomly selecting centroids and classifying each instance based on its nearest centroid. It then updates the centroids using the associated data. This process continues iteratively, with instances being reclassified and centroids recalculated until the centroids no longer move. It's a simple yet effective approach. Let's give it a try here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d25ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887ab5d",
   "metadata": {},
   "source": [
    "Unfortunately, you have to tell it how many clusters to look for, but hey, now each of the data points has been assigned to one of the 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85390d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd04d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_ # to find the centres of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b91604",
   "metadata": {},
   "source": [
    "Let's look at how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aca516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "#print(X)\n",
    "plt.plot(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],\"ro\")\n",
    "#print(kmeans.inertia_)\n",
    "plt.show() # pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92f8f0",
   "metadata": {},
   "source": [
    "You can use it to predict the label for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]]) #try adding some more to see what happens\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ef3dc",
   "metadata": {},
   "source": [
    "You can plot the decision boundaries as a Voronoi plot (code taken straight from {homl}):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65096a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)  # Plotting data points from input X\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]  # Filter centroids based on their weights\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],  # Plot centroids\n",
    "                marker='o', s=30, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],  # Plot centroids\n",
    "                marker='x', s=10, linewidths=10,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1 \n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),  # Generate grid of points in the defined limits\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])  # Perform clustering on the grid points\n",
    "    Z = Z.reshape(xx.shape)  # Reshape results to have same shape as the grid\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),  # Plot the filled contours (decision boundaries)\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),  # Plot the contour lines\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)  # Plot the original data\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)  # Plot the centroids if specified\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)  # Show x-axis label if specified\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)  # Hide x-axis labels\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)  # Show y-axis label if specified\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)  # Hide y-axis labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307f00f",
   "metadata": {},
   "source": [
    "If you happen to have a vague idea of where the centroids are, you can tell it where to start:\n",
    "```python\n",
    "init_guess=np.array([-3,1.0],[-3,2],[-3,3],[-1,2],[0,2])\n",
    "kmeans=KMeans(n-cluster=5,init=init_guess,n_inits=5)\n",
    "```\n",
    "\n",
    "### Inertia\n",
    "\n",
    "When the K-Means algorithm runs, it actually runs several times. The number of times it runs is given by n_inits and the default value is 10. It then uses a performance algorithm to detemine which is the best and keeps that one. The performance metric is called *inertia* and is the mean squared distance between each centroid and the instances associated with it. This is available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fbe12",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 2 [^](#index)\n",
    "<a id='Exercise_2'></a>\n",
    "\n",
    "Plot what happens to the inertia score as you change the number of centroids in your algorithm. Can you use this to determine how many centroids you should have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66daef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try to use K-Means on the iris data to separate them out without using labels. Even though we have only been using 2D to show plots the algorithm will happily run in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp \n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],iris['target'], test_size=0.2) \n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42,n_init=10)\n",
    "y_pred = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp=kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787503de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How accurate was this? Can you write code to classify the accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36090c4e",
   "metadata": {},
   "source": [
    "### Density-based spatial clustering of applications with noise (DBSCAN) [^](#index)\n",
    "<a id='DBSCAN'></a>\n",
    "\n",
    "DBSCAN is a popular clustering algorithm used in machine learning and data mining. Unlike K-Means, which requires specifying the number of clusters in advance, DBSCAN automatically determines the number of clusters based on the density of the data.\n",
    "\n",
    "DBSCAN operates by grouping together data points that are close to each other in a dense region, while separating regions of lower density. It identifies core points, which have a sufficient number of neighboring points within a specified distance (epsilon), and expands clusters by including reachable points within this distance. Any points that are not part of a cluster are considered outliers or noise.\n",
    "\n",
    "Practical usage: In high-energy physics experiments, particle tracks are reconstructed from the signals recorded by particle detectors. DBSCAN can be applied to identify and group together the recorded signals that belong to the same particle track. By clustering these signals based on their spatial proximity, DBSCAN helps in reconstructing the paths of particles accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4053d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Optional Exercise\n",
    "\n",
    "\n",
    "We have a dataset of particles with two features: momentum and charge. The goal is to group these particles based on these two features.\n",
    "\n",
    "Let's first simulate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "np.random.seed(50)\n",
    "\n",
    "# Make up data for 4 different particles\n",
    "data, _ = make_blobs(n_samples=500, centers=4, cluster_std=1)\n",
    "\n",
    "# Let's say the first feature is momentum and the second is charge\n",
    "momentum = data[:, 0]\n",
    "charge = data[:, 1]\n",
    "\n",
    "plt.scatter(momentum, charge)\n",
    "plt.xlabel('Momentum')\n",
    "plt.ylabel('Charge')\n",
    "plt.title('Simulated Particle Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18558225",
   "metadata": {},
   "source": [
    "Here, we are simulating data from four types of particles, each with different distributions of momentum and charge.\n",
    "\n",
    "Next, let's use DBSCAN to identify the particle types:\n",
    "\n",
    "DBSCAN takes two parameters, the **eps** which specifies the maximum distance between two samples for them to be considered as in the same neighborhood, and **min_sample**s which is the number of samples in a neighborhood for a point to be considered as a core point. You can alter these parameters based on the density of your data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62960d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN on data\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=10)\n",
    "clusters = dbscan.fit_predict(data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.scatter(momentum, charge, c=clusters, cmap='viridis')\n",
    "plt.xlabel('Momentum')\n",
    "plt.ylabel('Charge')\n",
    "plt.title('DBSCAN Clustering of Particle Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff16d6",
   "metadata": {},
   "source": [
    "Try to alter the values in DBSCAN to see how the plot changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
