{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8c0b81",
   "metadata": {},
   "source": [
    "# Dimension Reduction and unsupervised learning\n",
    "\n",
    "I was hoping that Jarvist would teach you about dimensionality reduction last week and I am no great expert in unsupervised learning. This means that I have taken much of this from other sources -- mostly {homl} -- because I had to write about one topic in a hurry and for the other topic this is a good a source as any other. As I have said before I recommend {homl} as a very good practical guide. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fedbf7",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "We have only been looking at data sets with a relatively small number of features, with MNIST being the largest with 784 featueres. In many cases you will have data sets with thousands and possibly millions of features. You have already seen that models built on data sets with many features are much slower than those built on data with just a few features - just compare the iris data with the MNIST data. When you get to data sets with a great many features this can be very slow.\n",
    "\n",
    "But there are other reasons as well. Even if you transform all your data so that the range of each feature is a single value between 0 and 1, as you add features you are adding dimensions to the hyperspace that you need to fill and characterise/model. If you pick two points at random from a unit square (i.e. a two featured space) their separation is, on average, $\\approx$0.52, if you go to 3D cube then it grows to $\\approx$0.66 but if you go a million features i.e. a 1000000D hypercube this has grown to $\\approx$408.25. This means that your model is now training on very sparce data which may not be representative. The obvious answer is to use more data to train the model, however (according to {homl})  you would need more data than there are atoms in the observable universe to have an average separation of 0.1 for just 100 dimensions (not sure how the calculated the actual nuber of atoms in the observable universe). This is sometimes called [*the curse of dimensionality*](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\n",
    "\n",
    "So some form of dimensionality reduction can often be very helpful. However, in any such reduction you will loose some information and you want to minimise this. A good way to consider this is to try to retain the maximum variance in your reduced data set. That way you will (generally) loose the smallest amount of discrimination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9efee9e",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "While there are many different forms of dimensionality reduction PCA is by far the most common and so it is the only one that we will really cover. In PCA your data are projected along axes that retain the greatest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe7cac5",
   "metadata": {},
   "source": [
    "Consider the diagrams below -- don't worry about code that generates them, this is taken directly from {homl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first some basics\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(3)\n",
    "X = np.random.randn(m, 2) / 10\n",
    "X = X.dot(np.array([[stretch, 0],[0, 1]])) # stretch\n",
    "X = X.dot([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]]) # rotate\n",
    "\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi/6), np.sin(angle - 2 * np.pi/6)])\n",
    "u3 = np.array([np.cos(angle - np.pi/2), np.sin(angle - np.pi/2)])\n",
    "\n",
    "X_proj1 = X.dot(u1.reshape(-1, 1))\n",
    "X_proj2 = X.dot(u2.reshape(-1, 1))\n",
    "X_proj3 = X.dot(u3.reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot2grid((3,2), (0, 0), rowspan=3)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u1[1]/u1[0], 1.4*u1[1]/u1[0]], \"k-\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u2[1]/u2[0], 1.4*u2[1]/u2[0]], \"k--\", linewidth=1)\n",
    "plt.plot([-1.4, 1.4], [-1.4*u3[1]/u3[0], 1.4*u3[1]/u3[0]], \"k:\", linewidth=2)\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\", alpha=0.5)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=5, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", fontsize=22)\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", fontsize=22)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot2grid((3,2), (0, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=1)\n",
    "plt.plot(X_proj1[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot2grid((3,2), (1, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k--\", linewidth=1)\n",
    "plt.plot(X_proj2[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot2grid((3,2), (2, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\n",
    "plt.plot(X_proj3[:, 0], np.zeros(m), \"bo\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb95318",
   "metadata": {},
   "source": [
    "This shows the projection of the data along the axes shown. When taking the PCA the first component is the one with the greatest variance -- in this case **C1**. The second principal component is the one with the greatest remaining variance which is **C2**. **C2** is perpendicular to **C1**. If we had more dimensions then we could define more vectors.\n",
    "\n",
    "So how do we find these, well there is a well known linear algebra technique which factorises matrices called *Single Valued Decomposition* (SVD). \n",
    "\n",
    "Numpy has a function which will return these (although the input must be centered around zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X-X.mean(axis=0)\n",
    "\n",
    "U,s, Vh=np.linalg.svd(X_centered) # docs at https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\n",
    "c1=Vh.T[:,0]\n",
    "c2=Vh.T[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8a12e",
   "metadata": {},
   "source": [
    "## Projecting Down to d Dimensions\n",
    "\n",
    "Once the principal components have been found you can project down on to a d-dimensional hyperplane defined by the first d principal components. This is means that as little varioance as possible is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6abe7a",
   "metadata": {},
   "source": [
    "## Looking at MNIST (again)\n",
    "\n",
    "So lets return to our old friend MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mnist['data']\n",
    "y=mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173fd7c0",
   "metadata": {},
   "source": [
    "Using PCA is even easier in sklearn than it is in numpy as it does all the zeroing for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92097755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=200) # choose the first 200 components out 784\n",
    "X_reduced=pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the components will then just be stored in pca.components.T i.e. the transpose\n",
    "print(len(pca.components_)) # should be 200 vectors\n",
    "print(len(pca.components_.T[:,0])) # in 784 dimensions\n",
    "print(len(X_reduced)) #should be 60000 from all the original training data\n",
    "print(len(X_reduced[0,:])) # should now only be 200 not 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde4e18",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio\n",
    "\n",
    "The explained variance ratio indicates the proportion of the datasets variance that lies along each principal component. So if we look at (say) the first 10 you can see how quickly/slowly it drops off. You can also see how much variance you have lost (in this case  around 3% so not much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_[0:11])\n",
    "print(\"lost=\",1-pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bdea3",
   "metadata": {},
   "source": [
    "## But how many dimensions to choose?\n",
    "\n",
    "Rather than plucking a number out of the hat for the number of dimensions you wish to project onto you could side how much variance you wish to retain and use this to decide the dimensionality that you want to use. You could do this for all  number of dimensions and see which is the first abouve your threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53964ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad03b03",
   "metadata": {},
   "source": [
    "However, sklearn have done this for you and rather than n_components being equal to a number of principal components you can  give it a number between 0 and 1 which is the variance that you want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb009e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=0.95)\n",
    "X_reduced=pca.fit_transform(X_train)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0e043",
   "metadata": {},
   "source": [
    "You could also plot this number as a function of the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57739405",
   "metadata": {},
   "source": [
    "The value of 154 means that you only need to store $\\approx$20% of the data for 95% of the information so you can use this as a form of compression. Look at the example below and note how you can reverse the transformation but that you have lost some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad217872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154) #try varying this 154 (start qt 1)and see what difference it makes to the images below\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225cba1",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "As the comment suggests, try varying the 154 number of dimensions and see what difference it makes to the the image. Start with very low numbers and build up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54990225",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use your favourite classifier (may be SVC or BDT) to classify the MNIST data set as well as you can. Then see how the timing and accuracy changes if you use versions with reduced dimensions and plot your reults (say accuracy against number of dimensions). This is what you are really interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05265d97",
   "metadata": {},
   "source": [
    "## Other sorts of PCA\n",
    "\n",
    "We don't have time here but you should be aware that there are also varioants of PCA that can be useful. These include:\n",
    "* **Kernel PCA** where you use a similar kernel trick as with SVMs to introduce nonlinear features (without really doing so).\n",
    "* Randomised PCA that generates good approximations to the PC in a semi-random way and is very much faster for large feature sets.\n",
    "\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31f428",
   "metadata": {},
   "source": [
    "## Other forms of dimensionality reduction\n",
    "\n",
    "There exist many other forms of dimensionality reduction but none anywhere near as popular as PCA. Thes include Locally Linear Embedding, Random Projections, Linear Discriminant Analysis, ... They all have their place and you should know that there are more out there that exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48541db7",
   "metadata": {},
   "source": [
    "# Unsupervised learning \n",
    "\n",
    "I was rather hoping to spend this entire session on unsupervised learning, but figured that dimensionality reduction was so important that it had to be covered explicitly.\n",
    "\n",
    "As it is I can only give you a taste of this and show you an example of a commonly used approach (and give you an exercise of course).\n",
    "\n",
    "Although you should note that dimensionality reduction is in itself a form of unsupervised learning as you only consider the variance of the data not  the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9b0d",
   "metadata": {},
   "source": [
    "## So what is unsuperived learning and what is it used for?\n",
    "\n",
    "As you might have guessed, unsupervised learning is where you don't have the answer as to what you are looking i.e. you don't have the target, so you cannot train your favourite classifier. Somehow you need an algorithm that will train a model to pick things that are \"the same\". This could be trying to distinguish coins on weight and diameter without know which coin is which - this is often used as an example. \n",
    "\n",
    "The most common form is clustering (although fault detection and density estimation are other common forms) and the most common form of clustering is K-Means, so this is the example that we will look at. Again this is an example taken from {homl}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea75a29",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "K-means can take an unlabeled data set and groups them into clusters. So lets generate some random data made up of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49698f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10882f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be19ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "#print(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919013e7",
   "metadata": {},
   "source": [
    "OK so we can see by eye that there are 5 new blobs of data here. However, how can you have a model as to which is which. If you you could tell the model the centroids of each then this would be easy or if you could tell it the identity of each entry (the y from the make blobs command) but in unsupervised learning you can do neither.\n",
    "\n",
    "K-Means starts off by picking random centroids and then classifies each instance according to its nearest centroid. It then recalculates the centroids from the data associated with it,  followed by a reclassification of the data according which of the new centroids it is nearest to and so  it continues until the centroids stop moving. Yep, it really is that simple. So lets try it out here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d25ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887ab5d",
   "metadata": {},
   "source": [
    "Unfortuneately you have to tell it how many clusters to look for but hey. Now each of the data points has been assigned to one of the 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85390d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd04d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_ # to find the centres of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b91604",
   "metadata": {},
   "source": [
    "Lets look at how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aca516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "#print(X)\n",
    "plt.plot(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],\"ro\")\n",
    "#print(kmeans.inertia_)\n",
    "plt.show() # pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92f8f0",
   "metadata": {},
   "source": [
    "You can use it to predict the label for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]]) #try adding some more to see what happens\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ef3dc",
   "metadata": {},
   "source": [
    "You can plot the decision boundaries as a Voronoi plot (code taken straight from {homl}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65096a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=30, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=10, linewidths=10,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307f00f",
   "metadata": {},
   "source": [
    "If you happen to have a vague idea of where the centroids are you can tell it where to start:\n",
    "```python\n",
    "init_guess=np.array([-3,1.0],[-3,2],[-3,3],[-1,2],[0,2])\n",
    "kmeans=KMeans(n-cluster=5,init=init_guess,n_inits=5)\n",
    "```\n",
    "\n",
    "### Inertia\n",
    "\n",
    "When the K-Means algorithm runs it actually runs several time. The number of times it runs is given by n_inits and the default value is 10. It then uses a performance algorithm to detemine which is the best and keeps that one. The performance metric is called *inertia* and is the mean squared distance between each centroid and the instances associated with it. This is available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fbe12",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Plot what happens to the inertia score as you change the number of centroids in your algorithm. Can you use this to determine how man centroids you should have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66daef",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try to use K-Means on the iris data to separate them out without using labels. Even though we have only been using 2D to show plots the algorithm will happily run in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp \n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],iris['target'], test_size=0.2) \n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42,n_init=10)\n",
    "y_pred = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp=kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787503de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0->2, 1->0, 2->1?, 3->1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca12dfd",
   "metadata": {},
   "source": [
    "There are many other forms of unsupervised learning. If I had more time the next one that I would introduc you to is *Density-based spatial clustering of applications with noise* (DBSCAN) which I think is more powerful than K-Mean but not as popular (there are a couple of reasons for this). Really you should know that there are a variety of tools out there and you can then search for them if you need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36090c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
