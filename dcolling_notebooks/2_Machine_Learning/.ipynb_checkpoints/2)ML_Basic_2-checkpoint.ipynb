{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification\n",
    "\n",
    "This is when you want ML model to correctly tell you what sort of an object that you have. I will try to talk you through this process and introduce you to some additional features on the way. Again much of this is taken from **{homl}**, especially later when we look at the MNIST dataset. But lets start with some basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at one of the most famous datasets in ML, in fact it far predates ML as we know it as goes back to a paper first published in 1936. You can find out more about it from [wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set). Basically it describes three different sorts of iris flowers. This data set is great for many things and I would use it a lot more except that it is quite small. This is so famous that it is actually part of sklearn. So let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp \n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "print(type(iris))\n",
    "display(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each data entry has 4 values sepal length and width and petal length and width and that the target is an integer from 0 to 2 corresponding to the three different forms of iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets build a model\n",
    "\n",
    "Lets start by building a very simple model. We used the KNN algorithm in regression and we will start off with this now in calssification. Don't forget we need to split into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],iris['target'], test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train) # sanity check that all is well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always important to look at your data first to look for obvious decriminators or strong correlations. As we saw before a pandas scatter matrix is a good way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_iris=pd.DataFrame(X_train,columns=iris.feature_names)\n",
    "\n",
    "pd.plotting.scatter_matrix(df_iris,figsize=(15,15),c=y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so this looks as though there might be some hope of separating them, so lets turn to our old friend the KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neighbors=12\n",
    "weights=\"uniform\"\n",
    "#weights=\"distance\"\n",
    "iris_knn=KNeighborsClassifier(n_neighbors=neighbors,weights=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make some predictions and see how well they match the true values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=iris_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pred)\n",
    "#print(y_test)\n",
    "agree=0\n",
    "disagree=0\n",
    "for i in range(len(pred)): #could easily be a lamda function but just to be explicit\n",
    "    if pred[i] == y_test[i]:\n",
    "        agree = agree +1\n",
    "    else:\n",
    "        disagree = disagree +1\n",
    "\n",
    "print (\"Agreeing \",agree)\n",
    "print (\"Disagreeing \",disagree)\n",
    "print (\"Success fraction\",agree/(agree+disagree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Systematically investigate how well a KNN classifier can work with these data (change the number of neighbours, weighting etc) and present the information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifiers\n",
    "\n",
    "Having given you a taste of a full multiclass classifier I am now going to reduce myself to binary classifiers for a while. Why? Well because for a lot of the history of ML binary classifiers dominated and many of the concepts that I want to get across to you make most sense with binary classifiers. In these classifiers you basically distinguish between some being something of type X or it not being of type X. The argument was always that you could then sub classify the things that weren't of type X. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Something that we have only touched on so far is cross validation. In many ML algorithms it was traditional to keep iterating until the results you got on your test sample were \"good enough\" i.e. better than some threshold. However, this means that in some way your training is also learning your testing sample. \n",
    "To get around this the idea of cross validation came around. So in cross validation you break down the training set into a number of smaller sets called *folds*. You then train your model with all but one of these folds and and evaluate with remaining fold and that way you get an array of evaluations. We will meet this later on in this notebook. This way you keep your testing samples for final vlaidation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST data set\n",
    "\n",
    "The iris data set is very famous. The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset is probably the most famous in ML. It is a relatively large collection of handwritten numerals. We will analyse it here.\n",
    "\n",
    "**I was going to write this section myself but it is so well described in {homl} that I have largely taken parts from there. As I have said before if you only buy one book for practical ML {homl} has to be a good candidate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "28 * 28 # these are 28 x 28 pixel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit = X[7] # just to pick an arbitrary figure. Try a different one\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[7] # check that the target matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8) # just setting as unsignes ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a couple of functions to help display the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    # This is equivalent to n_rows = ceil(len(instances) / images_per_row):\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # Append empty images to fill the end of the grid, if needed:\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n",
    "\n",
    "    # Reshape the array so it's organized as a grid containing 28Ã—28 images:\n",
    "    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n",
    "\n",
    "    # Combine axes 0 and 2 (vertical image grid axis, and vertical image axis),\n",
    "    # and axes 1 and 3 (horizontal axes). We first need to move the axes that we\n",
    "    # want to combine next to each other, using transpose(), and only then we\n",
    "    # can reshape:\n",
    "    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n",
    "                                                         images_per_row * size)\n",
    "    # Now that we have a big image, we just need to show it:\n",
    "    plt.imshow(big_image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Normally we now break the data set up into a training and test set. However the MNIST people have already done this with the last 10,000 entries being a well shuffled training set so life  is made easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need a binary classifier \n",
    "\n",
    "OK so we have said that we are going to examine binary classifiers in this section so we need to decide what that is. The simplest one here would be to try to pick out only digit from the possible 10. Following the example in **{homl}** then lets pick the digit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # true if was 5 otherwise false\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so now we need a binary classifier. There are many that you could use here and you will learn about two very common ones next week. However **{homl}** uses a different one called *Stochastic Gradient Descent*. This is typically good for very large datasets. Lets stick with this one for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sgd_clf.predict([some_digit])) # remember that we set this to X[7] easrlier which was 3\n",
    "print(sgd_clf.predict([X[0]])) # However X[0] was 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how well does it do?\n",
    "\n",
    "So now we need to work how well this works and there are a number of ways of doing this. We could have a look at the test set but lets keep that \"clean\" so that we can use it for final evaluation at the end.\n",
    "\n",
    "First lets try cross validation. Remember here we take N folds from the training set and then use all but one fold as training and the remaining one as testing, cycling around until this has happened N times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\") # so this has 3 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! More than 95%! However, not so fast. Only roughly 1 in 10 are 5 so even if you had a classifier saying that everything was not true it would be $\\approx$ 90% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A much more insightful way of looking at this would be to look at how often a 5 was not classified as a 5  (This is called a false negative - FN) or a non-5 as a 5 (false positive -FP) as well how often a 5 was correctly identified (true positive - TP) or a non-5 was correctly identified (true negative - TN).\n",
    "\n",
    "We are still trying to keep our actual test data \"clean\" so lets lets use cross validation again. This time rather than cross_val_score we cross_val_predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row of this matrix is about the non-5s. It shows that there were 53892 TN and 687 FP (i.e. identified as 5 even though they weren't). The second row is about actual 5. It shows that there were 1891 FN (i.e. not idientified as 5) and 3530 TP.\n",
    "\n",
    "We can fake what a perfect prediction would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5  # pretend we reached perfection\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "The confusion matrix gives the information that we need but sometimes it is better expressed as a fraction where you want to know two things:\n",
    "\n",
    "* What fraction of those identified as 5 really are 5. This is called precision (sometimes, especially in particle physics, purity)\n",
    "\n",
    "precision=$\\dfrac{TP}{TP+FP}$\n",
    "\n",
    "* What fraction of the true 5s did it select. This is called recall (sometimes, especially in particle physics, efficiency)\n",
    "\n",
    "recall= $\\dfrac{TP}{TP+FN}$\n",
    "\n",
    "Sklearn can compute these for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(y_train_5, y_train_pred))\n",
    "print(recall_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision V Recall\n",
    "\n",
    "Often in your ML environment you will have a choice between recall and precision (efficiency v purity).  \n",
    "\n",
    "This is not completely obvious in this case but we can look at it. Inside the SGDClassifier it calculates a score and then depending on whether or not that score is above or below a threhold it makes a decision. Changing that threshold would change the precision and recall for this classifier. Sklearn doesn't let you alter the threshold directly, but it does give you access to the decision score. You can do this by calling decision_function rather than predict.\n",
    "\n",
    "We can then use these values to make our own threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print(y_some_digit_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so how to we decide where we want the threshold? Well the first thing to do is to look at the scores.\n",
    "\n",
    "We start by using the cross validation again but this time with the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "\n",
    "\n",
    "\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\n",
    "                                            # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Another very important way of visualising this is as a direct trade off between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train_pred == (y_scores > 0)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ROC curve\n",
    "\n",
    "Another very common why of viewing performance is the *receiver operating characteristic* curve - ROC curve. The ROC curve plots the TP v the FP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                                    # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           # Not shown\n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   # Not shown\n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")  # Not shown\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")               # Not shown\n",
    "#save_fig(\"roc_curve_plot\")                                    # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to compare the performance of different classifiers is to compare the area under the ROC curve (AUC). The closer this is to 1. the better. Let's see for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can tell you that from my experience that is pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "You have seen one multiclass classifier today and you will see two more next week. So for now just an exercise..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the KNN classifier as a multiclass classifier to identify all the digits in the MNIST to best accuracy that you can on the test data (that we haven't yet used). This will require a lot of parameter setting and computing time. You will need to do a grid scan.\n",
    "\n",
    "As an example for you to follow lets try the iris optimisation (don't forget that MNIST is a lot bigger than iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],iris['target'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7]}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
