{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "\n",
    "1. [Unsupervised learning](#UL)\n",
    "1. [K-Means](#KM)\n",
    "1. [Exercise 2](#Exercise_2)\n",
    "1. [Density-Based Spatial Clustering of Applications with Noise](#DBSCAN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9b0d",
   "metadata": {},
   "source": [
    "## So what is unsuperived learning and what is it used for?  [^](#index)\n",
    "<a id='UL'></a>\n",
    "\n",
    "\n",
    "As you might have guessed, unsupervised learning is where you don't have the answer to what you are looking i.e. you don't have the target, so you cannot train your favourite classifier. Somehow you need an algorithm that will train a model to pick things that are \"the same\". This could be trying to distinguish coins on weight and diameter without know which coin is which - this is often used as an example. \n",
    "\n",
    "The most common form is clustering (although fault detection and density estimation are other common forms) and the most common form of clustering is K-Means, so this is the example that we will look at. Again, this is an example taken from {homl}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea75a29",
   "metadata": {},
   "source": [
    "## K-Means [^](#index)\n",
    "<a id='KM'></a>\n",
    "\n",
    "K-means can take an unlabeled data set and group it into clusters. So let's generate some random data made up of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49698f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10882f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be19ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "print(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919013e7",
   "metadata": {},
   "source": [
    "We have five new blobs of data, but how can we make a model identify each blob? If we knew the centroids of each blob or the identity of each entry, this task would be easy. However, in unsupervised learning, we have neither of these.\n",
    "\n",
    "K-Means starts by randomly selecting centroids and classifying each instance based on its nearest centroid. It then updates the centroids using the associated data. This process continues iteratively, with instances being reclassified and centroids recalculated until the centroids no longer move. It's a simple yet effective approach. Let's give it a try here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d25ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4887ab5d",
   "metadata": {},
   "source": [
    "Unfortunately, you have to tell it how many clusters to look for, but hey, now each of the data points has been assigned to one of the 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85390d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd04d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_ # to find the centres of the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b91604",
   "metadata": {},
   "source": [
    "Let's look at how well it did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aca516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X)\n",
    "#print(X)\n",
    "plt.plot(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],\"ro\")\n",
    "#print(kmeans.inertia_)\n",
    "plt.show() # pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92f8f0",
   "metadata": {},
   "source": [
    "You can use it to predict the label for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]]) #try adding some more to see what happens\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ef3dc",
   "metadata": {},
   "source": [
    "You can plot the decision boundaries as a Voronoi plot (code taken straight from {homl}):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65096a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)  # Plotting data points from input X\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]  # Filter centroids based on their weights\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],  # Plot centroids\n",
    "                marker='o', s=30, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],  # Plot centroids\n",
    "                marker='x', s=10, linewidths=10,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1 \n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),  # Generate grid of points in the defined limits\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])  # Perform clustering on the grid points\n",
    "    Z = Z.reshape(xx.shape)  # Reshape results to have same shape as the grid\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),  # Plot the filled contours (decision boundaries)\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),  # Plot the contour lines\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)  # Plot the original data\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)  # Plot the centroids if specified\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)  # Show x-axis label if specified\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)  # Hide x-axis labels\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)  # Show y-axis label if specified\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)  # Hide y-axis labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307f00f",
   "metadata": {},
   "source": [
    "If you happen to have a vague idea of where the centroids are, you can tell it where to start:\n",
    "```python\n",
    "init_guess=np.array([-3,1.0],[-3,2],[-3,3],[-1,2],[0,2])\n",
    "kmeans=KMeans(n-cluster=5,init=init_guess,n_inits=5)\n",
    "```\n",
    "\n",
    "### Inertia\n",
    "\n",
    "When the K-Means algorithm runs, it actually runs several times. The number of times it runs is given by n_inits and the default value is 10. It then uses a performance algorithm to detemine which is the best and keeps that one. The performance metric is called *inertia* and is the mean squared distance between each centroid and the instances associated with it. This is available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fbe12",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise [^](#index)\n",
    "<a id='Exercise_2'></a>\n",
    "\n",
    "Plot what happens to the inertia score as you change the number of centroids in your algorithm. Can you use this to determine how many centroids you should have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66daef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try to use K-Means on the iris data to separate them out without using labels. Even though we have only been using 2D to show plots the algorithm will happily run in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp \n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7bccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'],iris['target'], test_size=0.2) \n",
    "k = 4\n",
    "kmeans = KMeans(n_clusters=k, random_state=42,n_init=10)\n",
    "y_pred = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp=kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787503de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How accurate was this? Can you write code to classify the accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36090c4e",
   "metadata": {},
   "source": [
    "### Density-based spatial clustering of applications with noise (DBSCAN) [^](#index)\n",
    "<a id='DBSCAN'></a>\n",
    "\n",
    "DBSCAN is a popular clustering algorithm used in machine learning and data mining. Unlike K-Means, which requires specifying the number of clusters in advance, DBSCAN automatically determines the number of clusters based on the density of the data.\n",
    "\n",
    "DBSCAN operates by grouping together data points that are close to each other in a dense region, while separating regions of lower density. It identifies core points, which have a sufficient number of neighboring points within a specified distance (epsilon), and expands clusters by including reachable points within this distance. Any points that are not part of a cluster are considered outliers or noise.\n",
    "\n",
    "Practical usage: In high-energy physics experiments, particle tracks are reconstructed from the signals recorded by particle detectors. DBSCAN can be applied to identify and group together the recorded signals that belong to the same particle track. By clustering these signals based on their spatial proximity, DBSCAN helps in reconstructing the paths of particles accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4053d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Optional Exercise\n",
    "\n",
    "\n",
    "We have a dataset of particles with two features: momentum and charge. The goal is to group these particles based on these two features.\n",
    "\n",
    "Let's first simulate this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "np.random.seed(50)\n",
    "\n",
    "# Make up data for 4 different particles\n",
    "data, _ = make_blobs(n_samples=500, centers=4, cluster_std=1)\n",
    "\n",
    "# Let's say the first feature is momentum and the second is charge\n",
    "momentum = data[:, 0]\n",
    "charge = data[:, 1]\n",
    "\n",
    "plt.scatter(momentum, charge)\n",
    "plt.xlabel('Momentum')\n",
    "plt.ylabel('Charge')\n",
    "plt.title('Simulated Particle Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18558225",
   "metadata": {},
   "source": [
    "Here, we are simulating data from four types of particles, each with different distributions of momentum and charge.\n",
    "\n",
    "Next, let's use DBSCAN to identify the particle types:\n",
    "\n",
    "DBSCAN takes two parameters, the **eps** which specifies the maximum distance between two samples for them to be considered as in the same neighborhood, and **min_sample**s which is the number of samples in a neighborhood for a point to be considered as a core point. You can alter these parameters based on the density of your data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62960d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN on data\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=10)\n",
    "clusters = dbscan.fit_predict(data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.scatter(momentum, charge, c=clusters, cmap='viridis')\n",
    "plt.xlabel('Momentum')\n",
    "plt.ylabel('Charge')\n",
    "plt.title('DBSCAN Clustering of Particle Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff16d6",
   "metadata": {},
   "source": [
    "Try to alter the values in DBSCAN to see how the plot changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
