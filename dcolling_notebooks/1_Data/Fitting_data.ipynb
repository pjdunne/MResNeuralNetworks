{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Data #\n",
    "\n",
    "In data science, you will often be required to fit a curve to a set of data, because you want to interpolate between the data points you have, parameterize it, or extract some physical value from the data you have taken with associated uncertainty.\n",
    "\n",
    "This notebook serves as an introduction to some of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation #\n",
    "\n",
    "If you want to draw a smooth line of best fit between the data taken from an experiment, the simplest method is to use linear interpolation. You can draw a straight line between your data points and then estimate the data between these points. While you can write your own linear interpolation algorithm, a built-in option is available in `scipy`. Another common approach is to use a spline, with a cubic spline being the most commonly used as it provides smoothness and double differentiability. Consider the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example using Linear Interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "import scipy.interpolate as spi\n",
    "pl.rcParams['figure.figsize'] = [10, 15] \n",
    "\n",
    "\n",
    "x=np.arange(0,6.5,0.5) # plus 0.5 to make sure that we get 6\n",
    "y=np.sin(x)\n",
    "\n",
    "# now get the finer binned version for comparison\n",
    "xf=np.arange(0,6,0.001) \n",
    "yf=np.sin(xf)\n",
    "\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(xf,yf,\"g--\",label=\"True Values\")\n",
    "pl.xlabel(\"$x$\")\n",
    "pl.ylabel(\"$y$\")\n",
    "\n",
    "# now the linear 1D interpololation\n",
    "f=spi.interp1d(x, y)\n",
    "\n",
    "# f is a function that will return you a value of y for any x\n",
    "# so now lets compare the difference at a finer scale\n",
    "pl.plot(xf,f(xf), label=\"Linear Interpolation\")\n",
    "\n",
    "pl.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n",
    "#now lets histogram the differences.\n",
    "ydiff=yf-f(xf)\n",
    "pl.subplot(2,1,2)\n",
    "pl.hist(ydiff,bins=100)\n",
    "pl.xlabel(\"Difference between interpolated and true values\")\n",
    "pl.ylabel(\"Number of enteries\")\n",
    "#pl.xlim([-0.04,0.04])\n",
    "\n",
    "\n",
    "\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown from the plot above, not only can the differences be quite large, but there is also structure in the differences, which can cause a systematic effect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cubic Spline\n",
    "\n",
    "An alternative is to use a spline of some form, with the most common choice being the previously mentioned cubic spline. \n",
    "\n",
    "The example below is identical to the previous one, except that instead of linear interpolation, a cubic spline has been employed.\n",
    "\n",
    "To achieve this, we use the same `spi.interp1d` function, but specify the keyword argument `kind='cubic'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pylab as pl\n",
    "import scipy.interpolate as spi\n",
    "pl.rcParams['figure.figsize'] = [10, 15] \n",
    "\n",
    "\n",
    "x=np.arange(0,6.5,0.5) # plus 0.5 to make sure that we get 6\n",
    "y=np.sin(x)\n",
    "\n",
    "# now get the finer binned version for comparison\n",
    "xf=np.arange(0,6,0.001) \n",
    "yf=np.sin(xf)\n",
    "\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(xf,yf,\"g--\",label=\"True Values\")\n",
    "pl.xlabel(\"$x$\")\n",
    "pl.ylabel(\"$y$\")\n",
    "\n",
    "# now the cubic spline interpolation. Notice that we must specify the keyword argument kind='cubic'\n",
    "f=spi.interp1d(x, y, kind='cubic')\n",
    "\n",
    "# f is a function that will return you a value of y for any x\n",
    "# so now lets compare the difference at a finer scale\n",
    "\n",
    "pl.plot(xf,f(xf), label=\"Cubic Spline\")\n",
    "\n",
    "pl.legend(loc=\"upper right\")\n",
    "\n",
    "\n",
    "\n",
    "# visualise the differences on a histogram\n",
    "\n",
    "\n",
    "ydiff=yf-f(xf)\n",
    "pl.subplot(2,1,2)\n",
    "pl.hist(ydiff,bins=100)\n",
    "pl.xlabel(\"Difference between interpolated and true values\")\n",
    "pl.ylabel(\"Number of enteries\")\n",
    "#pl.xlim([-0.04,0.04])\n",
    "\n",
    "\n",
    "\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a cubic spline as an approximation provides a much better fit, with significantly smaller differences between the interpolated and true values *(notice the distinct scaling of the x-axis)*. However, even with this improved accuracy, there is still a structure present, resulting in systematic biases (although these biases are small when the differences are small).\n",
    "\n",
    "Scipy also implements two-dimensional forms of these interpolation algorithms.\n",
    "\n",
    "It's important to note that while using a cubic spline is generally effective, it is not a foolproof solution. In cases where there aren't enough data points, the cubic spline can occasionally overestimate or underestimate the true curve (as shown in the example below). However, it is generally pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pl.rcParams['figure.figsize'] = [10, 10] \n",
    "x = np.linspace(0, 10, num=11, endpoint=True)\n",
    "y = np.cos(-x**2/9.0)\n",
    "f = spi.interp1d(x, y)\n",
    "f2 = spi.interp1d(x, y, kind='cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnew = np.linspace(0, 10, num=101, endpoint=True)\n",
    "import matplotlib.pyplot as pl\n",
    "yt=np.cos(-xnew**2/9.0)\n",
    "pl.plot(x, y, 'o', xnew, f(xnew), '-', xnew, f2(xnew), '--',xnew,yt,'r:')\n",
    "pl.legend(['data', 'linear', 'cubic','true'], loc='best')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 ##\n",
    "\n",
    "The purpose of this exercise is to understand how interpolations work and their limitations.\n",
    "\n",
    "Select a function that has different scales of structure (such as $y=\\cos(x^2)$ with $0 \\le x \\lt 10$ for example) and investigate how well a cubic spline is able to interpolate with different sampling separations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Data #\n",
    "\n",
    "One of the most important elements in data analysis is often fitting the data distributions. In performing a fit you minimise how far your fitted solution deviates from the data, whilst making some assumptions about the data. You may be assuming that they are distributed according to a well defined probability density function (pdf), or that the data points should follow some functional form. In the measurement of all physical quantities, the **uncertainty** on that measurement is paramount to your true understanding of the measurement. \n",
    "\n",
    "Scipy does have a series of functions for fitting data in scipy.optimize and these are pretty good. However, we tend to use the more advanced [minuit](https://root.cern.ch/download/minuit.pdf) developed by CERN. This is written in C++ and wrapped in python as [iminuit](https://iminuit.readthedocs.io/en/stable/). \n",
    "\n",
    "`iminuit` has some fantastic [tutorials](https://iminuit.readthedocs.io/en/stable/tutorials.html) on their website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics ###\n",
    "\n",
    "You will learn basic usage of iminuit and how to approach standard fitting problems.\n",
    "\n",
    "`iminuit` is a Python frontend to the Minuit library in C++, an integrated software that combines a local minimizer (called Migrad) and two error calculators (called Hesse and the Minos). You provide it an analytical function, which accepts one or several parameters, and an initial guess of the parameter values. It will then find a local minimum of this function starting from the initial guess. In that regard, iminuit minimizer is like other local minimizers, like those in scipy.optimize.\n",
    "\n",
    "In addition, iminuit has the ability to compute uncertainty estimates for model parameters. iminuit was designed to solve statistics problems, where uncertainty estimates are an essential part of the result. The two ways of computing uncertainty estimates, Hesse and the Minos, have different advantages and disadvantages.\n",
    "\n",
    "iminuit is the successor of pyminuit. If you used pyminuit before, you will find iminuit very familiar. An important feature of iminuit (and pyminuit) is that it uses introspection to detect the parameter names of your function. This is very convenient, especially when you work interactively in a Jupyter notebook. It also provides special output routines for Jupyter notebooks to pretty print the fit results, as you will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic setup of the notebook\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# everything in iminuit is done through the Minuit object, so we import it\n",
    "from iminuit import Minuit\n",
    "\n",
    "# we also need a cost function to fit and import the LeastSquares function\n",
    "from iminuit.cost import LeastSquares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick start ###\n",
    "\n",
    "In this first section, we look at a simple case where a straight line should be fitted to a scattered (x,y) dataset. A line has two parameters, which we call (α,β). We go through the full fit, showing all basic steps to get you started quickly. In the following sections we will revisit the steps in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our line model, unicode parameter names are supported :)\n",
    "def line(x, α, β):\n",
    "    return  α + β * x\n",
    "\n",
    "\n",
    "# generate random toy data with random offsets in y\n",
    "np.random.seed(1)\n",
    "data_x = np.linspace(0, 1, 10)\n",
    "data_yerr = 0.1  # could also be an array\n",
    "data_y = line(data_x, 1, 2) + data_yerr * np.random.randn(len(data_x))\n",
    "\n",
    "# draw toy data\n",
    "plt.errorbar(data_x, data_y, data_yerr, fmt=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the parameters α and β of the line model from this data, we need to a minimize a suitable cost function. The cost function must be twice differentiable and have a minimum at the optimal parameters. We use the method of least-squares here, whose cost function computes the sum of squared residuals between the model and the data. The task of iminuit is to find the minimum of that function. Let’s do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10] \n",
    "\n",
    "# iminuit contains a LeastSquares class to conveniently generate a least-squares cost function.\n",
    "# We will revisit how to write this by hand in a later section.\n",
    "least_squares = LeastSquares(data_x, data_y, data_yerr, line)\n",
    "\n",
    "m = Minuit(least_squares, α=0, β=0)  # starting values for α and β\n",
    "\n",
    "m.migrad()  # finds minimum of least_squares function\n",
    "m.hesse()   # accurately computes uncertainties\n",
    "\n",
    "\n",
    "\n",
    "#print the values and errors and then note how to access individual ones.\n",
    "\n",
    "print(m)\n",
    "\n",
    "print(m.values)\n",
    "print(m.errors)\n",
    "\n",
    "\n",
    "print(m.values[0])\n",
    "print(m.errors[0])\n",
    "\n",
    "# draw data and fitted line\n",
    "plt.errorbar(data_x, data_y, data_yerr, fmt=\"o\", label=\"data\")\n",
    "plt.plot(data_x, line(data_x, *m.values), label=\"fit\")\n",
    "\n",
    "# display legend with some fit info\n",
    "fit_info = [\n",
    "    f\"$\\\\chi^2$ / $n_\\\\mathrm{{dof}}$ = {m.fval:.1f} / {len(data_x) - m.nfit}\",\n",
    "]\n",
    "\n",
    "# try printing out the parameters, values and errors to see the format\n",
    "\n",
    "for p, v, e in zip(m.parameters, m.values, m.errors):\n",
    "    fit_info.append(f\"{p} = ${v:.3f} \\\\pm {e:.3f}$\")\n",
    "    \n",
    "\n",
    "plt.legend(title=\"\\n\".join(fit_info))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is already it for a basic fit. Easy, right?\n",
    "\n",
    "In the following, we dive into the details step by step: how the Minuit object is initialized, how to run the algorithms, and how to get the results.\n",
    "\n",
    "iminuit was designed to make it easy to fit functions like least_squares(...), where the parameters are individual arguments of the function. There is an alternative function signature that Minuit supports, which is more convenient when you work a lot with numpy. Here, the parameters are passed as a numpy array. The two kinds of function definitions have each pros and cons. \n",
    "\n",
    "We will first look at how to work with functions of the first kind and come back to the second kind later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Minuit object\n",
    "To minimize a function, one has to create an instance of the Minuit class, pass the function, and a starting value for each parameter. This does not start the minimization yet, this will come later.\n",
    "\n",
    "The Minuit object uses introspection to get the number and names of the function parameters automatically, so that they can be initialized with keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(least_squares, α=0, β=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we forget a parameter or mistype, Minuit will raise an error - see the 2 cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Minuit(least_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Minuit(least_squares,a=0,b=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameter values\n",
    "Minuit’s main algorithm, **Migrad**, is a local minimizer. It searches for a local minimum by a doing a mix of Newton steps and gradient-descents from a starting point. If your function has several minima, the minimum found will depend on the starting point. Even if it has only one minimum, if you start in the proximity of the minimum, iminuit will converge to it faster.\n",
    "\n",
    "You can set the starting point using the parameter names as keywords, name = value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(least_squares, α=5, β=5)  # pass starting values for α and β"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the starting values can also be passed as positional arguments, like below:\n",
    "```\n",
    "Minuit(least_squares, 5, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using iminuit with Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use iminuit with functions that accept numpy arrays. This has pros and cons.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Easy to change number of fitted parameters\n",
    "* Sometimes simpler function body that’s easier to read\n",
    "* Technically this is more efficient, but this is hardly going to be noticable\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* iminuit cannot figure out names for each parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we will use a version of the line model which accepts the parameters as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_np(x, par):\n",
    "    return np.polyval(par, x)  # for len(par) == 2, this is a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling line_np with more or less arguments is easy. For $n$ arguments, a polynomial of order $n$ is used to predict the behavior of the data.\n",
    "\n",
    "The built-in cost functions support such a model. For it to be detected properly, you need to pass the starting values in the form of a single sequence of numbers.\n",
    "\n",
    "Remember: by starting values, we mean the point at which the minimiser starts from when searching for a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares_np = LeastSquares(data_x, data_y, data_yerr, line_np)\n",
    "\n",
    "Minuit(least_squares_np, (5, 5))  # pass starting values as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a tuple here for initialisation, but any sequence will work, we could instead have passed a list or a numpy array here. iminuit uses the length of the sequence to detect how many parameters the model has. By default, the parameters are named automatically $x_0$ to $x_N$. \n",
    "\n",
    "One can override this with the keyword argument `name`, passing a sequence of parameter names. Of course, this sequence must be of the same length as the sequence of starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(least_squares_np, (5, 5), name=(\"a\", \"b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since least_squares_np works for parameter arrays of any length, one can easily change the number of fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a forth order polynomial\n",
    "Minuit(least_squares_np, (5, 5, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to try different orders of a polynomial model. If the order is too small, the polynomial will not follow the data. If it is too large, it will overfit the data and pick up random fluctuations and not the underlying trend. \n",
    "\n",
    "We can figure out the right order by experimenting or using an algorithm like cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Fit the following data set with different orders of polynomials. As you increase the order of the polynomial how does the goodness of fit change? When does the fit show signs of overfitting? \n",
    "    \n",
    "You will need to put commas between the data points, and errors cannot be negative so you need to take the absolute value of these (I generated them according to an algorithm which does allow negative values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "y_measured = [343.39452514, 251.31914561, 186.7806368, 121.30027965, 45.23336652, 23.49470302, 18.46581766, 6.58329486, 1.98522328, -\n",
    "              6.74799454, -7.55489379, -29.55544088, -31.32898172, -36.27525348, -25.23860169, -1.09156819, 52.32898397, 225.38126087, 460.67437131]\n",
    "\n",
    "y_err = [141.312, 86.205, 49.632, 26.877, 13.872, 7.197, 4.08, 2.397, 0.672,\n",
    "         -1.923, -5.568, 9.795, 13.488, -14.883, -11.568, 0.483, 22.08, 60.477, 119.712]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting current parameters\n",
    "You can check the current parameter values and settings with the method `Minuit.params` at any time. It returns a special list of Param objects which pretty-prints in Jupyter and in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a nice table with numbers rounded according to the rules of the Particle Data Group. The table will be updated once you run the actual minimization. To look at the initial conditions later, use Minuit.init_params. We will come back to the meaning of Hesse Error and Minos Error later.\n",
    "\n",
    "Minuit.params returns a tuple-like container of Param objects, which are data objects with attributes that one can query. Use `repr()` to get a detailed representation of the data object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in m.params:\n",
    "    print(repr(p), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters with limits\n",
    "iminuit allows you to set parameter limits. Often a parameter is limited mathematically or physically to a certain range. For example, if your function contains sqrt(x), then x must be non-negative, x ≥ 0. You can set upper-, lower-, or two-sided limits for each parameter individually with the limits property.\n",
    "\n",
    "* lower limit: use Minuit.limits = (<value>, None) or (<value>, float(\"infinity\"))\n",
    "* upper limit: use Minuit.limits = (None, <value>) or (-float(\"infinity\"), <value>)\n",
    "* two-sided limit: use Minuit.limits = (<min_value>, <max_value>)\n",
    "* remove limits: use Minuit.limits = None or (-float(\"infinity\"), float(\"infinity\")\n",
    "\n",
    "    \n",
    " You can also set limits for several parameters at once with a sequence. To impose the limits α≥0 and 0≤β≤10 in our example, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.limits = [(0, None), (0, 10)]\n",
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing and releasing parameters\n",
    "In some cases, you may have a parameter that needs to be temporarily set to a fixed value. This can be useful when you have a guess for its value and want to observe how the other parameters adapt when this specific parameter is held constant at that value.\n",
    "\n",
    "Alternatively, if you have a complex function with numerous parameters that have varying impacts on the function, you can assist the minimizer in finding the minimum faster by initially fixing the less influential parameters to their initial guesses and only fitting the significant parameters. \n",
    "\n",
    "Once the minimum is obtained under these conditions, you can release the fixed parameters and optimize all the parameters together. Minuit retains the previous minimization state and resumes from there. The time required for minimization approximately scales with the square of the number of parameters - by performing iterated minimization over parameter subspaces, this time can be reduced.\n",
    "\n",
    "To fix an individual parameter, you can utilise the keyword Minuit.fixed[name] = True. In our example, we fix parameter \"α\" as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fixed[\"α\"] = True\n",
    "m.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# migrad will not vary α, only β\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we release α and fix β and minimize again, can also use parameter index\n",
    "m.fixed[0] = False\n",
    "m.fixed[1] = True\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Starting Points for Minimization ###\n",
    "\n",
    "It is sometimes useful to manually change the values of some fixed parameters and fit the others, or to restart the fit from another starting point. \n",
    "\n",
    "For example, if the cost function has several minima, changing the starting value can be used to find the other minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show an example below for a function which has 2 minima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_with_two_minima(x):\n",
    "    return x ** 4 - x ** 2 + 1\n",
    "\n",
    "# we come back to the meaning of errordef in the next section\n",
    "cost_function_with_two_minima.errordef = Minuit.LEAST_SQUARES\n",
    "\n",
    "x = np.linspace(-1.5, 1.5)\n",
    "plt.plot(x, cost_function_with_two_minima(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting at -0.1 gives the left minimum\n",
    "m = Minuit(cost_function_with_two_minima, x=-0.1)\n",
    "m.migrad()\n",
    "print(\"starting value -0.1, minimum at\", m.values[\"x\"])\n",
    "\n",
    "# changing the starting value to 0.1 gives the right minimum\n",
    "m.values[\"x\"] = 0.1  # m.values[0] = 0.1 also works\n",
    "m.migrad()\n",
    "print(\"starting value +0.1, minimum at\", m.values[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "### Advanced: Simplex and Scan minimizers\n",
    "iminuit also offers two other minimizers which are less powerful than Migrad, but may be useful in special cases.\n",
    "\n",
    "Simplex\n",
    "The Nelder-Mead method (aka SIMPLEX) is well described on Wikipedia. It is a gradient-free minimization method that usually converges more slowly, but may be more robust. For some problems it can help to start the minimization with SIMPLEX and then finish with MIGRAD. Since the default stopping criterion for SIMPLEX is much more lax than MIGRAD, either running MIGRAD after SIMPLEX or reducing the tolerance with Minuit.tol is strongly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(cost_function_with_two_minima, x=10).simplex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run MIGRAD to finish the minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minuit(cost_function_with_two_minima, x=10).simplex().migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination uses slightly fewer function evaluations and produces a more accurate result than just running MIGRAD alone in this case (for another problem this may not be true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scan\n",
    "\n",
    "Scan is a last resort. It creates a N-dimensional grid scan over the parameter space. The number of function evaluations scale like $n^k$, where $k$ is the number of parameters and n the number of steps along one dimension. Using 'scan' for high-dimensional problems is unfeasible, but it can be useful in low-dimensional problems and when all but a few parameters are fixed. The scan needs bounds, which are best set with Minuit.limits. The number of scan points is set with the ncall keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(cost_function_with_two_minima, x=10)\n",
    "m.limits = (-10, 10)\n",
    "m.scan(ncall=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scan brought us in close proximity of the minimum.\n",
    "\n",
    "In this case, the minimum is considered valid, because the **EDM** (**estimated distance to minimum**) value is smaller than the EDM goal, but the scan may also end up in an invalid minimum, which is also ok. The scan minimizes the cost function using a finite number of steps, regardless of the EDM value (which is only computed after the scan for the minimum).\n",
    "\n",
    "One should always run MIGRAD or SIMPLEX after a SCAN. This is often a good way to find the minimum that you are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "### Advanced: Errordef\n",
    "\n",
    "If you do not use one of the cost functions from the iminuit.cost module, you need to pass an additional parameter to Minuit. Let’s make a custom least-squares function and try to run Migrad on it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple least-squares cost function looks like this...\n",
    "def custom_least_squares(a, b):\n",
    "    ym = line(data_x, a, b)\n",
    "    z = (data_y - ym) / data_yerr\n",
    "    return np.sum(z ** 2)\n",
    "\n",
    "\n",
    "Minuit(custom_least_squares, a=5, b=5).migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minuit now warns about using its default value for the errordef parameter, which may not be appropriate. Setting this is not needed for the cost functions in iminuit.cost, but it is needed for custom cost functions.\n",
    "\n",
    "The `errordef` parameter is used to compute the correct uncertainties. If you don’t care about uncertainty estimates, you can ignore the warning. In statistical problems, there are two kinds of cost functions to minimize, the negative log-likelihood and the least-squares function. \n",
    "\n",
    "Each has a corresponding value for errordef:\n",
    "- -0.5 or the constant: `Minuit.LIKELIHOOD` for negative log-likelihood functions\n",
    "- -1 or the constant: `Minuit.LEAST_SQUARES` for least-squares functions\n",
    "\n",
    "The origin of these numbers is not too complicated, but cannot be explained briefly. If you are curious, have a look into  [“Error computation with HESSE and MINOS”](https://iminuit.readthedocs.io/en/stable/notebooks/hesse_and_minos.html), which explains in depth how uncertainties are computed and where this value comes from.\n",
    "\n",
    "For our custom cost function, we need to set m.errordef=1 or equivalent and more readable m.errordef=Minuit.LEAST_SQUARES, because it is of the least-squares type. If we do that, the warning disappears.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(custom_least_squares, a=5, b=5)\n",
    "m.errordef = Minuit.LEAST_SQUARES\n",
    "m.migrad()  # no warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the fit status\n",
    "Calling `Minuit.migrad()` runs the actual minimization with the Migrad algorithm. Migrad essentially tries a Newton-step and if that does not produce a smaller function value, it tries a line search along the direction of the gradient. So far so ordinary. The clever bits in Migrad are how various pathological cases are handled.\n",
    "\n",
    "Let’s look again at the output of Minuit.migrad().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Minuit(least_squares, α=5, β=5)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Minuit.migrad method returns the Minuit instance so that one can chain method calls, the instance also pretty prints the latest state of the minimization.\n",
    "\n",
    "The first block in this output is showing information about the function minimum. This is good for a quick check:\n",
    "\n",
    "All blocks should be green.\n",
    "* Purple means something bad.\n",
    "* Yellow may be bad or not. Be careful.\n",
    "* Let’s see how it looks when the function is bad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_bad = Minuit(lambda x: 0, x=1)  # a constant function has no minimum\n",
    "m_bad.errordef = 1  # avoid the errordef warning\n",
    "m_bad.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to our previous good example, the info about the function minimum can be directly accessed with Minuit.fmin:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(repr(...)) to see a detailed representation of the data object\n",
    "print(repr(m.fmin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important one here is `is_valid`. If this is false, the fit does not converge and the result is useless. Since this is so often queried, a shortcut is provided with `Minuit.valid`.\n",
    "\n",
    "If the fit fails, there is usually a numerical or logical issue.\n",
    "\n",
    "Either:\n",
    "\n",
    "- The fit function is not analytical everywhere in the parameter space\n",
    "\n",
    "or\n",
    "\n",
    "- The fit function does not have a local minimum (the minimum may be at infinity, the extremum may be a saddle point or maximum).\n",
    "\n",
    "Indicators for this are `is_above_max_edm=True`, `hesse_failed=True`, `has_posdef_covar=False`, or `has_made_posdef_covar=True`. A non-analytical function is one with a discrete step, for example.\n",
    "\n",
    "Possible problems are:\n",
    "\n",
    "\n",
    "* Migrad reached the call limit before the convergence so that `has_reached_call_limit=True`. The used number of function calls is nfcn, and the call limit can be changed with the keyword argument ncall in the method Minuit.migrad. Note that nfcn can be slightly larger than ncall, because Migrad internally only checks this condition after a full iteration, in which several function calls can happen.\n",
    "* Migrad detects convergence by a small edm value, the estimated distance to minimum. This is the difference between the current minimum value of the minimized function and the prediction based on the current local quadratic approximation of the function (something that Migrad computes as part of its algorithm). If the fit does not converge, is_above_max_edm is true.\n",
    "\n",
    "If you are interested in parameter uncertainties, you should make sure that:\n",
    "\n",
    "* has_covariance, has_accurate_covar, and has_posdef_covar are true.\n",
    "* has_made_posdef_covar and hesse_failed are false.\n",
    "The second object of interest after the fit is the parameter list, which can be directly accessed with Minuit.params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m.params is a tuple-like container of Param data objects which contain information about the fitted parameters. Important fields are: \n",
    "- number: parameter index. \n",
    "- name: parameter name. \n",
    "- value: value of the parameter at the minimum. \n",
    "- error: uncertainty estimate for the parameter value.\n",
    "\n",
    "The accuracy of the uncertainty estimate depends on two factors: the correct mathematical modeling of the fitting problem and the appropriate usage of the errordef value in Minuit. But what exactly do we mean by \"correct mathematical modeling\"? To understand this, let's examine the function simple_least_squares(a, b). \n",
    "\n",
    "Notice that each squared residual is divided by the expected variance of the residual. This division is crucial for obtaining accurate uncertainty estimates for the parameters.\n",
    "\n",
    "In some cases, the expected variance of the residual may not be well-known. When the function to minimize is a least-squares function, there is a simple test to assess the adequacy of the residual variances. One can evaluate the function value at the minimum, denoted by fmin.fval, and divide it by the difference between the number of residuals and the number of fitted parameters. This difference can be conveniently obtained using the nfit attribute. This metric is referred to as the reduced $\\chi^2$. You should be familiar with this from statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fval / (len(data_y) - m.nfit)  # reduced chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value should be around 1. The more data points one has, the closer. If the value is much larger than 1, then the data variance is underestimated or the model does not describe the data. If the value is much smaller than 1, then the data variance is overestimated (perhaps because of positive correlations between the fluctuations of the data values).\n",
    "\n",
    "The last block shows the covariance matrix, this is useful to check for large correlations which are usually a sign of trouble.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Uncertainties, Covariances, and Confidence Intervals\n",
    "You saw how to get the uncertainty of each individual parameter and how to access the full covariance matrix of all parameters together, which includes the correlations. Correlations are essential additional information if you want to work with parameter uncertainties seriously.\n",
    "\n",
    "Minuit offers two ways to compute the parameter uncertainties, Hesse and Minos. Both have pros and cons.\n",
    "\n",
    "#### Hesse for Covariance and Correlation Matrices\n",
    "The Hesse algorithm numerically computes the matrix of second derivatives at the function minimum (called the Hesse matrix) and inverts it. The Hesse matrix is symmetric by construction. In the limit of infinite data samples to fit, the result of this computation converges to the true covariance matrix of the parameters. It often is already a good approximation even for finite statistic. These errors obtained from this method are sometimes called parabolic errors, because the Hesse matrix method is exact if the function is a hyperparabola (third and higher-order derivatives are all zero).\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* (Comparably) fast computation.\n",
    "* Provides covariance matrix for error propagation.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* May not have good coverage probability when sample size is small\n",
    "\n",
    "The Migrad algorithm computes an approximation of the Hesse matrix automatically during minimization. When the default strategy is used, Minuit does a check whether this approximation is sufficiently accurate and if not, it computes the Hesse matrix automatically.\n",
    "\n",
    "All this happens inside C++ Minuit and is a bit intransparent, so to be on the safe side, we recommend to call Minuit.hesse explicitly after the minimization, if exact errors are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The Hesse matrix (or Hessian) is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field.\n",
    "The notation for the Hesse Matrix of a function $f$ is $H_f$. The value in the $i$th row and $j$th column is given by $(H_f)_{i,j} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's mess up the current errors a bit so that hesse has something to do\n",
    "m.errors = (0.16, 0.2)\n",
    "m.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.hesse().params # note the change in \"Hesse Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance and correlation Matrix\n",
    "To see the covariance matrix of the parameters, you do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(m.covariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance.correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonzero correlation is not necessarily a bad thing, but if you have freedom in redefining the parameters of the fit function, it is good to choose parameters which are not strongly correlated.\n",
    "\n",
    "Minuit cannot accurately minimise the function if two parameters are (almost) perfectly (anti-)correlated. It also means that one of two parameters is superfluous - it doesn’t add new information. You should rethink the fit function in this case and try to remove one of the parameters from the fit.\n",
    "\n",
    "Both matrices are subclasses of numpy.ndarray, so you can use them everywhere you would use a numpy array. In addition, these matrices support value access via parameter names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.covariance[\"α\", \"β\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minos for non-parabolic minima\n",
    "Minuit has another algorithm to compute uncertainties: Minos. It implements the so-called profile likelihood method, where the neighborhood around the function minimum is scanned until the contour is found where the function increase by the value of errordef. The contour defines a confidence region that covers the true parameter point with a certain probability. The probability is exactly known in the limit of infinitely large data samples, but approximate for the finite case. Please consult a textbook about statistics about the mathematical details or look at the tutorial “Error computation with HESSE and MINOS”.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Produces pretty confidence regions in 2D (or higher) for scientific plots\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Computationally expensive\n",
    "* Asymmetric errors are difficult to error-propagate\n",
    "\n",
    "Minos is not automatically called during minimization, it needs to be called explicitly afterwards, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.minos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you have likely become accustomed to seeing green colors, which indicate that Minos has run successfully. However, it is crucial to exercise caution if these colors turn red instead, as it signifies a failure of Minos. The fields in the new Minos table carry the following meanings:\n",
    "\n",
    "* Valid: Indicates whether Minos considers the scan result valid. \n",
    "* At Limit: Becomes true if Minos encounters a parameter limit before completing the contour, which is undesirable.\n",
    "* Max FCN: Becomes true if Minos reaches the maximum number of allowed calls before completing the contour, also an undesirable outcome.\n",
    "* New Min: Becomes true if Minos discovers a deeper local minimum in the vicinity of the current one. While not necessarily problematic, it should ideally be avoided.\n",
    "\n",
    "The errors computed by Minos are now also shown in the parameter list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage Probability of Intervals Constructed with Hesse and Minos Algorithms\n",
    "In applications, it is important to construct confidence regions with a well-known coverage probability. As previously mentioned, the coverage probability of the intervals constructed from the uncertainties reported by Hesse and Minos are not necessarily the standard 68 %.\n",
    "\n",
    "Whether Hesse or Minos produce an interval with a coverage probability closer to the desired 68 % in finite samples depends on the case. There are theoretical results which suggest that Hesse may be slightly better, but we also found special cases where Minos intervals performed better.\n",
    "\n",
    "Some sources claim that Minos gives better coverage when the cost function is not parabolic around the minimum; that is not generally true, in fact Hesse intervals may have better coverage.\n",
    "\n",
    "As a rule-of-thumb, use Hesse as the default and try both algorithms if accurate coverage probability matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick access to fit results\n",
    "You get the main fit results with properties and methods from the Minuit object. We used several of them already. Here is a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.values)  # array-like view of the parameter values\n",
    "\n",
    "# access values by name or index\n",
    "print(\"by name \", m.values[\"α\"])\n",
    "print(\"by index\", m.values[0])\n",
    "\n",
    "# iterate over values\n",
    "for key, value in zip(m.parameters, m.values):\n",
    "    print(f\"{key} = {value}\")\n",
    "    \n",
    "# slicing works\n",
    "print(m.values[:1])\n",
    "\n",
    "print(m.errors)  # array-like view of symmetric uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minuit.errors supports the same access as Minuit.values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.params) # parameter info (using str(m.params))\n",
    "\n",
    "print(repr(m.params)) # parameter info (using repr(m.params))\n",
    "\n",
    "# asymmetric uncertainties (using str(m.merrors))\n",
    "print(m.merrors)\n",
    "\n",
    "print(m.covariance)  # covariance matrix computed by Hesse (using str(m.covariance))\n",
    "print(repr(m.covariance))  # covariance matrix computed by Hesse (using repr(m.covariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, you can play around with iminuit by assigning new values to m.values and m.errors and then run m.migrad() again. The values will be used as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "iminuit comes with buildin methods to draw the confidence regions around the minimum, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the minimum again after messing around with the parameters\n",
    "m.migrad()\n",
    "\n",
    "# draw three contours with 68%, 90%, 99% confidence level\n",
    "m.draw_mncontour(\"α\", \"β\", cl=(0.68, 0.9, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asymptotically (in large samples), the cl is equal to the probability that the region contains the true value. In finite samples, this is usually only approximately equivilent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get individual contours to plot them yourself\n",
    "ctr_xy = m.mncontour(\"α\", \"β\", cl=0.68, size=10)\n",
    "print(ctr_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to inspect the cost function around the minimum because MINUIT warns you about some issues, you can quickly scan it with a call to Minuit.draw_profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.draw_profile(\"α\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use this to plot the result of the scan yourself\n",
    "a, fa = m.profile(\"α\")\n",
    "plt.plot(a, fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use mnprofile to do a full profile likelihood scan. This mimics what MINOS does to compute confidence intervals. If you have trouble with MINOS, running this may help to inspect the issue.\n",
    "\n",
    "This is computationally expensive, since the scan runs MIGRAD for each point on the profile.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.draw_mnprofile(\"α\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also look at the 2D contours of the cost function around the minimum. Note that these are just contours of the fit function, not confidence regions. The latter you can only get from mncontour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = m.contour(\"α\", \"β\", subtract_min=True)\n",
    "#print(len(x))\n",
    "cs = plt.contour(x, y, z, (1, 2, 3, 4))  # these are not sigmas, just the contour values\n",
    "plt.clabel(cs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use this function for a quick look\n",
    "m.draw_contour(\"α\", \"β\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functions\n",
    "We give an in-depth guide on how to use the builtin cost functions.\n",
    "\n",
    "The iminuit package comes with a couple of common cost functions that you can import from iminuit.cost for convenience. Of course, you can write your own cost functions to use with iminuit, but most of the cost function is always the same. What really varies is the statistical model which predicts the probability density as a function of the parameter values. This you still have to provide yourself and the iminuit package will not include machinery to build statistical models (that is out of scope).\n",
    "\n",
    "Using the builtin cost functions is not only convenient, they also have some extra features.\n",
    "\n",
    "* Support of fitted weighted histograms.\n",
    "* Technical tricks improve numerical stability.\n",
    "* Optional numba acceleration (if numba is installed).\n",
    "* Cost functions can be added to fit data sets with shared parameters.\n",
    "* Temporarily mask data.\n",
    "\n",
    "\n",
    "We demonstrate each cost function on an standard example from high-energy physics, the fit of a peak over some smooth background (here taken to be constant).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import cost, Minuit\n",
    "from scipy.stats import norm, uniform\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate our data. We sample from a Gaussian peak around 0 with width 0.1 and from uniform background from -1 to 1. We then bin the original data. One can fit the original or the binned data. The latter is often much faster and if the binning is fine enough, there is no loss in precision as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = -1, 1\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "xdata = rng.normal(0, 0.1, size=400)\n",
    "xdata = np.append(xdata, rng.uniform(*xrange, size=1000))\n",
    "\n",
    "n, xe = np.histogram(xdata, bins=50, range=xrange)  \n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "\n",
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.plot(xdata, np.zeros_like(xdata), \"|\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum-likelihood fits\n",
    "\n",
    "You will learn about Maximum-liklehoods in a few weeks in the\n",
    "\n",
    "Maximum-likelihood fits are the state-of-the-art when it comes to fitting models to data. The can be applied to unbinned and binned data (histograms).\n",
    "\n",
    "* Unbinned fits are the easiest to use, because they can be apply directly to the raw sample. They become slow when the sample size is large.\n",
    "\n",
    "* Binned fits require you to appropriately bin the data. The binning has to be fine enough to retain all essential information. Binned fits are much faster when the sample size is large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbinned fit\n",
    "Unbinned fits are ideal when the data samples are not too large or very high dimensional. There is no need to worry about the appropriate binning of the data. Unbinned fits are inefficient when the samples are very large and can become numerically unstable, too. Binned fits are a better choice then.\n",
    "\n",
    "The cost function for an unbinned maximum-likelihood fit is really simple, it is the sum of the logarithm of the pdf evaluated at each sample point (times -1 to turn maximimization into minimization). You can easily write this yourself, but a naive implementation will suffer from instabilities when the pdf becomes locally zero. Our implementation mitigates the instabilities to some extend.\n",
    "\n",
    "To perform the unbinned fit you need to provide the pdf of the model, which must be vectorized (a numpy ufunc). The pdf must be normalized, which means that the integral over the sample value range must be a constant for any combination of model parameters.\n",
    "\n",
    "The model pdf in this case is the weighted sum of the normal and the uniform pdfs. The parameters are  (the weight),  and  of the normal distribution. The uniform distribution is parameter-free. The cost function detects the parameter names.\n",
    "\n",
    "It is important to put appropriate limits on the parameters, so that the problem does not become mathematically undefined. $0 \\lt z \\lt 1$ , $-1 \\lt \\mu \\lt 1$  , $\\sigma \\gt 0$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pdf(x, z, mu, sigma):\n",
    "    return (z * norm.pdf(x, mu, sigma) +\n",
    "            (1 - z) * uniform.pdf(x, xrange[0], xrange[1] - xrange[0])) #note how useful the ,pdf are\n",
    "\n",
    "c = cost.UnbinnedNLL(xdata, model_pdf)\n",
    "\n",
    "m = Minuit(c, z=0.4, mu=0, sigma=0.2)\n",
    "m.limits[\"z\"] = (0, 1)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.limits[\"sigma\"] = (0, None)\n",
    "\n",
    "m.migrad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "xm = np.linspace(xe[0], xe[-1])\n",
    "plt.plot(xm, model_pdf(xm, *[p.value for p in m.init_params]) * len(xdata) * dx[0],\n",
    "         ls=\":\", label=\"init\")\n",
    "plt.plot(xm, model_pdf(xm, *m.values) * len(xdata) * dx[0], label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended unbinned fit\n",
    "\n",
    "This is often used in the world of particle physics where we are trying to \n",
    "\n",
    "An important variant of the unbinned ML fit is described by Roger Barlow, *Nucl.Instrum.Meth.A 297 (1990) 496-506*. Use this if both the shape and the integral of the density are of interest. In practice, this is often the case, for example, if you want to estimate a cross-section or yield.\n",
    "\n",
    "The model in this case has to return the integral of the density and the density itself (which must be vectorized). The parameters in this case are $n_{sig}$  (integral of the signal density), $n_{bkg}$ (integral of the uniform density), $\\mu$  and $\\sigma$ of the normal distribution. Again, the parameters need limits so that the problem is mathematically defined.\n",
    "\n",
    "* $n_{sig} \\gt 0$\n",
    "* $n_{bkg} \\gt 0$\n",
    "* $-1 \\lt \\mu \\lt 1$\n",
    "* $\\sigma \\gt 0$\n",
    "\n",
    "Compared to the previous case, we have one more parameter to fit. This is common to extended fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density(x, nsig, nbkg, mu, sigma):\n",
    "    return nsig + nbkg, (nsig * norm.pdf(x, mu, sigma) +\n",
    "        nbkg * uniform.pdf(x, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedUnbinnedNLL(xdata, model_density)\n",
    "\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted values and the uncertainty estimates for $\\mu$ and $\\sigma$  are identical to the ordinary ML fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "xm = np.linspace(xe[0], xe[-1])\n",
    "plt.plot(xm, model_density(xm, *[p.value for p in m.init_params])[1] * dx[0],\n",
    "         ls=\":\", label=\"init\")\n",
    "plt.plot(xm, model_density(xm, *m.values)[1] * dx[0], label=\"fit\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binned Fit\n",
    "Binned fits are computationally more efficient and numerically more stable when samples are large. The caveat is that one has to choose an appropriate binning. The binning should be fine enough so that the essential information in the original is retained.\n",
    "\n",
    "The sample investigate here is large enough and 50 bins are fine enough to retain all information. The maximum-likelihood method applied to binned data gives correct results even if bins no entries, so chosing a binning that is very fine is not an issue. It just increases the computational cost.\n",
    "\n",
    "The cost functions for binned fits implemented here assume that the bin contents are independently Poisson distributed around an unknown expected value per bin. This is exactly correct for ordinary histograms.\n",
    "\n",
    "For a binned maximum-likelihood fit, one sums the logarithm of Poisson probabilities for the observed counts as a function of the predicted counts in this case (times -1 to turn maximization into minimization). Instead of a pdf, you need to provide a cdf in this case (which must be vectorized). Note that you can approximate the cdf as “bin-width times pdf evaluated at center” if it is difficult to calculate, but this is an approxmiation. Using the cdf is exact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cdf(xe, z, mu, sigma):\n",
    "    return (z * norm.cdf(xe, mu, sigma) +\n",
    "            (1-z) * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.BinnedNLL(n, xe, model_cdf)\n",
    "\n",
    "m = Minuit(c, z=0.4, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"z\"] = (0, 1)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.limits[\"sigma\"] = (0, None)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted values and the uncertainty estimates for μ and σ are not identical to the unbinned fit, but very close. For practical purposes, the results are equivalent. This shows that the binning is fine enough to retain the essential information in the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_cdf(xe, *[p.value for p in m.init_params])) * len(xdata), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_cdf(xe, *m.values)) * len(xdata), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended binned maximum-likelihood fit\n",
    "The binned extended maximum-likelihood fit is strictly the binned equivalent of the corresponding unbinned fit. One sums the logarithm of Poisson probabilities for the observed counts as a function of the predicted counts in this case (times -1 to turn maximization into minimization).\n",
    "\n",
    "Instead of a density, you need to provide a cdf of the density in this case (which must be vectorized). There is no need to separately return the total integral like the unbinned case. The parameters are the same as in the unbinned extended fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density_cdf(xe, nsig, nbkg, mu, sigma):\n",
    "    return (nsig * norm.cdf(xe, mu, sigma) +\n",
    "            nbkg * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedBinnedNLL(n, xe, model_density_cdf)\n",
    "\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary masking\n",
    "\n",
    "In complicated binned fits with peak and background, it is sometimes useful to fit in several stages. One typically starts by masking the signal region, to fit only the background region.\n",
    "\n",
    "This can be used to set up sensible values for a fit or to perform a blind analaysis.\n",
    "\n",
    "\n",
    "The cost functions have a mask attribute to that end. We demonstrate the use of the mask with an extended binned fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density_cdf(xe, nsig, nbkg, mu, sigma):\n",
    "    return (nsig * norm.cdf(xe, mu, sigma) +\n",
    "            nbkg * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedBinnedNLL(n, xe, model_density_cdf)\n",
    "\n",
    "# we set the signal amplitude to zero and fix all signal parameters\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.fixed[\"nsig\", \"mu\", \"sigma\"] = True\n",
    "\n",
    "# we temporarily mask out the signal\n",
    "c.mask = (cx < -0.5) | (0.5 < cx)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fix the background and fit only the signal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mask = None # remove mask\n",
    "m.fixed = False # release all parameters\n",
    "m.fixed[\"nbkg\"] = True # fix background amplitude\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we release all parameters and fit again to get the correct uncertainty estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fixed = None\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result, of course. Since this was an easy problem, we did not need these extra steps, but doing this is usually helpful to fit lots of histograms without adjusting each fit manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted histograms\n",
    "The cost functions for binned data also support weighted histograms. Just pass an array with the shape (n, 2) instead of (n,) as the first argument, where the first number of each pair is the sum of weights and the second is the sum of weights squared (an estimate of the variance of that bin value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-squares fits\n",
    "\n",
    "A cost function for a general weighted least-squares fit (aka chi-square fit) is also included. In statistics this is called non-linear regression.\n",
    "\n",
    "In this case you need to provide a model that predicts the y-values as a function of the x-values and the parameters. The fit needs estimates of the y-errors. If those are wrong, the fit may be biased. If your data has errors on the x-values as well, checkout the tutorial about automatic differentiation, which includes an application of that to such fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a + b * x ** 2\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "yt = model(x, 1, 2)\n",
    "ye = 0.4 * x**5 + 0.1\n",
    "y = rng.normal(yt, ye)\n",
    "\n",
    "plt.plot(x, yt, ls=\"--\", label=\"truth\")\n",
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.LeastSquares(x, y, ye, model)\n",
    "\n",
    "m = Minuit(c, a=0, b=0)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust least-squares\n",
    "The builtin least-squares function also supports robust fitting with alternative loss functions. See the documentation of iminuit.cost.LeastSquares for details. Users can pass their own loss functions. Builtin loss functions are:\n",
    "\n",
    "* linear (default): gives ordinary weighted least-squares\n",
    "\n",
    "* soft_l1: quadratic ordinary loss for small deviations $(\\ll 1\\sigma)$, linear loss for large deviations $(\\gg 1\\sigma)$, and smooth interpolation in between\n",
    "\n",
    "Let’s create one outlier and see what happens with ordinary loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a + b * x ** 2\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "yt = model(x, 1, 2)\n",
    "ye = 0.4 * x**5 + 0.1\n",
    "y = rng.normal(yt, ye)\n",
    "y[3] = 3 # generate an outlier\n",
    "\n",
    "c = cost.LeastSquares(x, y, ye, model)\n",
    "\n",
    "m = Minuit(c, a=0, b=0)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is distorted by the outlier. We can repair this with the soft_l1 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.loss = \"soft_l1\"\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is now practically identical as in the previous case without an outlier.\n",
    "\n",
    "Robust fitting is a very powerful if the data are contaminated with small amounts of outliers. It comes with a price, however, the uncertainties are in general larger. Compare the estimated uncertainty of the parameter b, which was 0.15 and now is 0.23.\n",
    "\n",
    "We can actually do better by manually removing the point (using the mask attribute) and switching back to ordinary loss. **You should always be careful removing any data without a good reason.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mask = c.x != c.x[3]\n",
    "c.loss = \"linear\"\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the uncertainty on $b$ is back to 0.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise ##\n",
    "\n",
    "This is a slightly longer exercise that is very close to real data analaysis. The data in data1 contains a signal on a falling background. Find where this signal is, characterise it (finds its location,width and how many events it contains). Finally display it clearly - for example you may want to subtract the background and you may want to think about how you display the uncertainties in your characterisation. These data are in pickle format and so you will have to learn how to use pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "# The iminuit doumentation also has excellent tutorials on:\n",
    "#### - Fits with shared parameters\n",
    "#### - Fit PDF with conditional variables\n",
    "\n",
    "#### That we recommend that you do when you have time.\n",
    " \n",
    "#### In fact, all of the [tutorials](https://iminuit.readthedocs.io/en/stable/tutorials.html) are excellent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
