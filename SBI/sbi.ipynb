{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation-based inference with Machine Learning\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index:  <a id='index'></a>\n",
    "1. [Introduction](#intro)\n",
    "1. [Learning the likelihood-density ratio with classifiers](#sbi_with_classifiers)\n",
    "1. [Simple hypothesis test](#simple_hypothesis_test)\n",
    "1. [Hypothesis test in higher-dimensions (particle spin example)](#complex_hypothesis_test)\n",
    "1. [Parameter estimation with parametric classifiers](#parameter_estimation)\n",
    "1. [Further reading](#further_reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# 1. Introduction <a id='intro'></a>\n",
    "\n",
    "In this notebook you will learn how to use Machine Learning (ML) for simulation-based inference (SBI). Let's begin by introducing what this means...\n",
    "\n",
    "Statistical inference is a key component of the scientific method. It explains how we extract any understanding from the data we collect. As you have learnt from your statistics lectures, the **likelihood function** holds the key to inference. \n",
    "\n",
    "Let's say we perform an experiment $N_{obs}$ times and observe the data set $\\mathcal{D} = \\{x_i\\}^{N_{obs}}_{i=1}$, where $x \\in \\mathbb{R}^d$ is the vector of features describing the data of dimension $d$. We have a theoretical model, defined by parameters $\\theta$, which describes the data. In order to infer the values of $\\theta$ from the data, we need the (conditional) probability density: $p(x|\\theta)$. The **likelihood** is then constructed as the product of the densities over the observed data:\n",
    "\n",
    "$$ \n",
    "p(\\mathcal{D}|\\theta) = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} p(x_i|\\theta) \n",
    "$$\n",
    "\n",
    "This tells us the probability of observing dataset $\\mathcal{D}$ given $\\theta$. There are then two paradigms for inference:\n",
    "\n",
    "* <u>**Frequentist inference**</u>: the interpretation here is over hypothetical repetitions of the experiment i.e. probability is the fraction of experiments under which the true value of $\\theta$ would be included in the estimator’s confidence interval. For this, we construct a test-statistic $t_\\theta$, and compare the observed value of the test-statistic to distributions under different values of $\\theta$. The Neyman-Pearson lemma tells us that (for a simple hypothesis test) the most powerful test-statistic is the log-likelihood ratio:\n",
    "\n",
    "    $$\n",
    "    t_\\theta = -\\ln{\\frac{p(\\mathcal{D}|\\theta)}{p(\\mathcal{D}|\\theta_0)}}\n",
    "    $$\n",
    "\n",
    "    where $\\theta_0$ is a reference value of the parameter vector $\\theta$.\n",
    "\n",
    "* <u>**Bayesian inference**</u>: the interpretation here is over degrees of belief about parameter values. We use Bayes Theorem to estimate the posterior density:\n",
    "\n",
    "    $$\n",
    "    p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{p(\\theta)}\n",
    "    $$\n",
    "\n",
    "    from which we build credible intervals for parameters as a function of the data.\n",
    "\n",
    "Both paradigms rely on the likelihood: $p(\\mathcal{D}|\\theta)$. This notebook has a frequentist flavour (put together by a particle physicist), but there is nothing stopping you applying these ideas in a Bayesian formalism.\n",
    "\n",
    "## Intractable likelihoods\n",
    "\n",
    "For many scientific domains, we are no longer in the regime of simple models with analytic (tractable) likelihoods. The systems that we are trying to model become increasingly complex with many unobserved (latent) variables, $z$. Take disease modelling over a population:\n",
    "\n",
    "![disease_modelling](disease_modelling.png)\n",
    "\n",
    "We need to model the effect of social mixing, mask wearing, level of caution etc (latent variables) in order to infer something meaningful about the data. \n",
    "\n",
    "This crucially cannot be done analytically. The likelihood becomes a complicated integral over all possible trajectories through latent space:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\theta) = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} \\int dz \\,p(x_i|\\theta,z) \n",
    "$$\n",
    "\n",
    "How can we approximate this intractable likelihood? Answer: **Simulation**.\n",
    "\n",
    "We can use (stochastic) simulators to generate synthetic data under different model parameters $\\theta$. These simulators are typically very complex in order to encapsulate all the underlying physics. **Simulation-based inference** (SBI) refers to the comparison of the synthetic data to the observed data, in order to infer something about $\\theta$.\n",
    "\n",
    "## Simulation-based inference\n",
    "\n",
    "There are two traditional approaches to SBI:\n",
    "\n",
    "* Approximate Bayesian Computation (ABC): which you have already covered.\n",
    "\n",
    "* Probability density estimation using the simulated (synthetic) data via histogramming or kernel-density estimation.\n",
    "\n",
    "Over recent years, scientists across many disciplines have turned to Machine Learning (ML) for SBI. The recent boom has lead to the development of a plethora of ML techniques, most of which cannot be covered in this tutorial. Instead, we will lay the foundations by showing how a simple ML classifer can be used for hypothesis testing, and later parameter estimation. This (in theory) will help you bridge the gap between your statistics and ML lectures. If you are interested and want to delve deeper into this fast-growing topic, we have added a [Further reading](#further_reading) section at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# 2. Learning the likelihood ratio with classifiers <a id='sbi_with_classifiers'></a>\n",
    "\n",
    "As mentioned in the previous section, we will show how to perform SBI with ML classifiers.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Binary classifiers are tasked with discriminating between two hypotheses: $\\mathcal{H}_0$ and $\\mathcal{H}_1$. They are trained by minimising the binary cross-entropy (BCE) loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}[f] = -\\frac{1}{N} \\sum^{N}_{i=1} y_i \\ln{f(x_i)} + (1-y_i)\\ln{(1-f(x_i))}\n",
    "$$\n",
    "\n",
    "where the sum is over $N$ samples $x_i$ drawn from the (conditional) probability densities $p(x|\\mathcal{H}_0)$ or $p(x|\\mathcal{H}_1)$ with assigned labels $y_i=0$ or $y_i=1$, respectively. Note, for SBI these samples are produced with the simulator.\n",
    "\n",
    "The quantity $f(x_i)$ is the classifier decision function (i.e. the output of the classifier). The optimal decision function (in the infinite sample limit i.e. as $N \\rightarrow \\infty$) which minimises the (BCE) loss function is:\n",
    "\n",
    "$$\n",
    "f(x_i) = \\frac{p(x_i|\\mathcal{H}_1)}{p(x_i|\\mathcal{H}_0)+p(x_i|\\mathcal{H}_1)}\n",
    "$$\n",
    "\n",
    "assuming that the two hypotheses are equally represented in the training data. \n",
    "\n",
    "It is important to note that our trained classifier $\\hat{f}(x_i)$ will be an estimator of the optimal decision function: $\\hat{f}(x_i) \\approx f(x_i)$. For a classification task, this will just lead to a mis-labelling of the data. However, for inference, it can lead to a bias in our measurement. Hence, we take great care in validating our SBI ML algorithms when applied in real-world scenarios (see [Further reading](#further_reading)).\n",
    "\n",
    "With a simple re-arranging known as the likelihood-ratio trick [1], we arrive at an approximation of the (conditional) probability density ratio between the two hypotheses for a single sample $x_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)} \\approx \\frac{p(x_i|\\mathcal{H_1})}{p(x_i|\\mathcal{H_0})}.\n",
    "$$\n",
    "\n",
    "The likelihood ratio (which is what we want for Frequentist inference) is then constructed by taking the product of the probability density ratios for all samples in the observed data set. \n",
    "\n",
    "$$\n",
    "\\frac{p(\\mathcal{D}|\\mathcal{H}_0)}{p(\\mathcal{D}|\\mathcal{H}_1)} = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} \\frac{p(x_i|\\mathcal{H_1})}{p(x_i|\\mathcal{H_0})} \\approx \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} \\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\n",
    "$$\n",
    "\n",
    "Under the Neyman-Pearson lemma, the (twice negative) log-likelihood ratio provides the most powerful test-statistic for a simple hypothesis test:\n",
    "\n",
    "$$\n",
    "t = 2\\Delta\\mathrm{NLL} = -2 \\ln{\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_0)}} \\approx -2 \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\\bigg)}\n",
    "$$\n",
    "\n",
    "We can use this \"learned\" test-statistic to perform a hypothesis test!\n",
    "\n",
    "</div>\n",
    "\n",
    "In summary, we first showed how the output of a classifier can be used to approximate the probability density ratio between two hypotheses for a single sample, $x_i$. This can then be used to define a test-statistic over a full dataset. We will now show how to use the test-statistic for inference by performing a hypothesis test. This is something that you have already seen in your statistics lectures. We will begin with a simple example where the analytic likelihood is known and compare to the performance of the \"learned\" test-statistic.\n",
    "\n",
    "[1] - K. Cranmer, J. Pavez and G. Louppe, *Approximating Likelihood Ratios with Calibrated Discriminative Classifiers* (2016). [arXiv:1506.02169](https://arxiv.org/abs/1506.02169)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# 3. Simple hypothesis test  <a id='simple_hypothesis_test'></a>\n",
    "\n",
    "Let's show how to use a binary classifier to perform a two-class hypothesis test for a simple problem. You will use your knowledge of hypothesis testing from the statistics lectures. The problem is as follows:\n",
    "\n",
    "* There are two possible hypotheses (classes) to explain the data. Data produced from the classes have one observable $x$ which is distributed according to a 1D Gaussian with unit width. Class 0 has a mean of zero, and class 1 has a mean of one. We will define class 0 as the null hypothesis ($\\mathcal{H}_0$), and class 1 as the alternative hypothesis ($\\mathcal{H}_1$).\n",
    "* An experiment is performed in which $x$ is measured for $N_{obs}=10$ independent samples. The observed data are provided in the file `data_simple.csv`.\n",
    "* We are going to set up a hypothesis test to infer which class the data belong to. We will first demonstrate how to do this using the analytic likelihood, where we will use the log-likelihood ratio as a test-statistic. Following this we will show how to learn the log-likelihood ratio from simulated samples (synthetic data) using machine learning. \n",
    "* We will then compare the performance of the two approaches.\n",
    "\n",
    "Obviously, SBI is overkill for this simple problem where the analytic likelihood is known. We will show in [Section 4](#complex_hypothesis_test) how one can extend this to a more complex problem (with an unknown likelihood) to perform inference on the data.\n",
    "\n",
    "Let's begin by importing the relevant libraries and defining the function used to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Function to generate 1D gaussian synthetic data (x)\n",
    "def generate_synthetic_data(num_samples, loc, scale, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.normal(loc=loc, scale=scale, size=num_samples)\n",
    "    return pd.DataFrame(data, columns=['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate synthetic data from each of the two hypotheses. These are subsequently combined into a single dataframe which will later use to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_per_class = 100000\n",
    "# Generate synthetic training data for two classes\n",
    "train_H0 = generate_synthetic_data(num_train_per_class, loc=0, scale=1)\n",
    "train_H0['label'] = 0\n",
    "train_H1 = generate_synthetic_data(num_train_per_class, loc=1, scale=1)\n",
    "train_H1['label'] = 1\n",
    "train_data = pd.concat([train_H0, train_H1]).reset_index(drop=True)\n",
    "\n",
    "# Plot histograms of the synthetic training data\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(train_H0['x'], bins=100, range=(-4,4), alpha=0.5, label='H0', density=True)\n",
    "ax.hist(train_H1['x'], bins=100, range=(-4,4), alpha=0.5, label='H1', density=True)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed data is stored in `data_simple.csv`. Let's plot the observed data on top of the expected distributions for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from csv file\n",
    "data_obs = pd.read_csv('data_simple.csv')\n",
    "N_obs = len(data_obs)\n",
    "\n",
    "# Plot histogram of the observed data\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(data_obs['x'], bins=100, range=(-4,4), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax.hist(train_H0['x'], bins=100, range=(-4,4), alpha=0.5, label='H0', density=True)\n",
    "ax.hist(train_H1['x'], bins=100, range=(-4,4), alpha=0.5, label='H1', density=True)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell by eye which class the observed data belong to? In any case, let's perform a proper hypothesis test. We will start with the analytic solution.\n",
    "\n",
    "## Analytic likelihood-ratio\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The probability of observation $x_i$ is given by the following formula, assuming $x$ follows a Gaussian distribution with mean $\\mu$ and width $\\sigma$ is:\n",
    "\n",
    "$$\n",
    "p(x_i|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Therefore, the likelihood over dataset $\\mathcal{D} = \\{x_i\\}$ of $N_{obs}$ samples is:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{D}|\\mu,\\sigma) = \\prod^{N_{obs}}_{x_i \\in \\mathcal{D}} p(x_i|\\mu,\\sigma)\n",
    "$$\n",
    "\n",
    "As discussed above, the Neyman-Pearson lemma tells us that the log-likelihood ratio is the most powerful test-statistic for simple hypothesis testing ($\\mathcal{H}_0$ vs $\\mathcal{H}_1$). We will use $2\\Delta\\mathrm{NLL}$ as convention for the choice of test-statistic: \n",
    "\n",
    "$$\n",
    "t = 2\\Delta\\mathrm{NLL} = -2 \\ln {\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_0)}}\n",
    "$$\n",
    "</div>\n",
    "Do you know why we use the \"twice-negative\" as convention? The answer lies in **Wilk's theorem**: the twice-negative log-likelihood ratio asymptotically approaches the $\\chi^2$ distribution under the null hypothesis. This means we can use the properties of the $\\chi^2$ function to determine the probability distribution of the test-statistic, and subsequently the coverage/confidence intervals. Anyway we will come to this in the parameter estimation section.\n",
    "\n",
    "Let's code this up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate log-likelihood ratio between two hypotheses with (mu0, sigma0) vs (mu1, sigma1)\n",
    "def log_likelihood_ratio(x, mu0=0, mu1=1, sigma0=1, sigma1=1):\n",
    "    ll_H0 = -0.5 * ((x - mu0) / sigma0)**2 - np.log(sigma0) - 0.5 * np.log(2 * np.pi)\n",
    "    ll_H1 = -0.5 * ((x - mu1) / sigma1)**2 - np.log(sigma1) - 0.5 * np.log(2 * np.pi)\n",
    "    return ll_H1 - ll_H0\n",
    "\n",
    "# Function to calculate test statistic (2NLL) over all events \n",
    "def test_statistic(x, mu0=0, mu1=1, sigma0=1, sigma1=1):\n",
    "    llr = log_likelihood_ratio(x, mu0, mu1, sigma0, sigma1)\n",
    "    return -2 * np.sum(llr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a hypothesis test we need to first generate synthetic datasets (toys) which are representative of the observed data ($N_{obs}=10$), under each hypothesis. For each \"toy\" we will calculate the value of the test statistic, building up a distributions of the test statistic under the two hypotheses.\n",
    "\n",
    "We then calculate the test-statistic value for the observed data, and compare this to the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_toys = 10000\n",
    "test_statistic_H0 = []\n",
    "test_statistic_H1 = []\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = generate_synthetic_data(N_obs, loc=0, scale=1, seed=None)\n",
    "    t_H0 = test_statistic(samples_H0['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H0.append(t_H0)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = generate_synthetic_data(N_obs, loc=1, scale=1, seed=None)\n",
    "    t_H1 = test_statistic(samples_H1['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H1.append(t_H1)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs = test_statistic(data_obs['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "\n",
    "# Plot the distributions of the test statistic under both hypotheses, as well as the observed case\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(test_statistic_H0, bins=50, alpha=0.5, range=(-50,50), label='H0')\n",
    "ax.hist(test_statistic_H1, bins=50, alpha=0.5, range=(-50,50), label='H1')\n",
    "ax.axvline(t_obs, color='black', linestyle='solid', linewidth=2, label='Observed data')\n",
    "ax.set_xlabel('$t = 2\\\\Delta$NLL')\n",
    "ax.set_xlim(-50, 65)\n",
    "ax.set_ylabel('Number of pseudo-experiments')\n",
    "ax.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the test-statistic distributions we can calculate the $p$-value with respect to the null hypothesis ($\\mathcal{H}_0$) by integrating the left-hand tail of the $\\mathcal{H}_0$ distribution up to the observed value of the test-statistic. This $p$-value tells us:\n",
    "* Assuming $\\mathcal{H}_0$ is true, what is the probability of obtaining data at least as extreme as what you observed, over a long run of repeated identical experiments.\n",
    "\n",
    "If the $p$-value is below some pre-defined critical threshold ($\\alpha=0.05$), then we can reject the null-hypothesis at the $1-\\alpha$ confidence level (CL). \n",
    "\n",
    "We can calculate the Type-1 and Type-2 error rate for fixed $\\alpha$ and subsequently the power of the test. The error rates are defined as follows:\n",
    "* Type-1: assuming $\\mathcal{H}_0$ is true, it is the probability of incorrectly rejecting $\\mathcal{H}_0$ ($=\\alpha$) i.e. false positive rate.\n",
    "* Type-2: assuming $\\mathcal{H}_1$ is true, it is the probability of incorrectly failing to reject $\\mathcal{H}_0$ ($\\beta$) i.e. it quantifies the fraction of the $\\mathcal{H}_1$ distribution which lies beyond the critical value. In this way ($1-\\beta$) is equal to the true positive rate (which is also referred to as the statistical power). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-value for H0\n",
    "p_value_H0 = np.sum(np.array(test_statistic_H0) <= t_obs) / N_toys\n",
    "\n",
    "# Calculate the type-1 and type-2 errors and the statistical power for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value = np.percentile(test_statistic_H0, alpha * 100)\n",
    "type_2_error = np.sum(np.array(test_statistic_H1) > critical_value) / N_toys\n",
    "power = 1 - type_2_error    \n",
    "\n",
    "# Plot results again with additional annotations\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.hist(test_statistic_H0, bins=50, alpha=0.5, range=(-50,50), label='H0')\n",
    "ax.hist(test_statistic_H1, bins=50, alpha=0.5, range=(-50,50), label='H1')\n",
    "ax.axvline(t_obs, color='black', linestyle='solid', linewidth=2, label='Observed data ($p$={:.3f})'.format(p_value_H0))\n",
    "ax.axvline(critical_value, color='red', linestyle='dashed', linewidth=2, label='Critical value (α={})'.format(alpha))\n",
    "ax.set_xlabel('$t = 2\\\\Delta$NLL')\n",
    "ax.set_xlim(-50, 65)\n",
    "ax.set_ylabel('Number of pseudo-experiments')\n",
    "ax.legend(loc='best', fontsize=6)\n",
    "# Add text to plot with type-1, type-2 errors and power\n",
    "textstr = '\\n'.join((\n",
    "    'Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    'Type-2 error ($\\\\beta$): {:.3f}'.format(type_2_error),\n",
    "    'Power (1-$\\\\beta$): {:.3f}'.format(power)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=6,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $p$-value is less than the critical value ($\\alpha=0.05$). We can reject the null hypothesis $\\mathcal{H}_0$ for this data at the 95% CL.\n",
    "\n",
    "We can also calculate a compatibility $p$-value with respect to the alternative hypothesis $\\mathcal{H}_1$. What we want to know is \"how extreme is the observed data compared to what I would expect under $\\mathcal{H}_1$? Since large values of the test-statistic look more like $\\mathcal{H}_0$, then here it is flipped i.e. is it the upper tail of the $\\mathcal{H}_1$ distribution which indicates the incompatibility with $\\mathcal{H}_1$. \n",
    "\n",
    "Let's calculate this quantity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p-value for alternative hypothesis\n",
    "p_value_H1 = np.sum(np.array(test_statistic_H1) >= t_obs) / N_toys\n",
    "\n",
    "# Print p-values for both hypotheses\n",
    "print('P-value for H0: {:.5f}'.format(p_value_H0))\n",
    "print('P-value for H1: {:.5f}'.format(p_value_H1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is much more compatible with the alternative hypothesis! Note this might not always be the case i.e. you may have two hypotheses, neither of which fit the data well. Under this scenario one should look towards forming a different hypothesis.\n",
    "\n",
    "A ROC curve tells us the trade-off between the Type-I error rate (false positives) and the 1 - Type-II error rate (true positives). We move the threshold ($\\alpha$) and calculate $1-\\beta$, as shown in the code block below. The area-under-curve (AUC) metric is a measure of how well the test statistic separates the $\\mathcal{H}_0$ and $\\mathcal{H}_1$ distributions i.e. the power of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vals = np.linspace(0.0, 1.0, 50)\n",
    "beta_vals = []\n",
    "for alpha in alpha_vals:\n",
    "    critical_value = np.percentile(test_statistic_H0, alpha * 100)\n",
    "    type_2_error = np.sum(np.array(test_statistic_H1) > critical_value) / N_toys\n",
    "    beta_vals.append(type_2_error)\n",
    "\n",
    "fpr = alpha_vals\n",
    "tpr = 1 - np.array(beta_vals)\n",
    "\n",
    "# Calculate AUC using trapezoidal rule\n",
    "auc = np.trapezoid(tpr, fpr)\n",
    "\n",
    "# Plot the ROC curve (beta vs alpha)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(fpr, tpr, label='ROC Curve (AUC = {:.3f})'.format(auc))\n",
    "ax.plot([0, 1], [0, 1], color='black', linestyle='dashed', label='Random Guess (AUC = 0.5)')\n",
    "ax.set_xlabel('False Positive Rate ($\\\\alpha$)')\n",
    "ax.set_ylabel('True Positive Rate (1-$\\\\beta$)')\n",
    "ax.set_xlim(0,1)\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the likelihood-ratio\n",
    "\n",
    "As discussed in [Section 2](#sbi_with_classifiers) we can use a binary classifier to approximate the likelihood ratio. \n",
    "\n",
    "In this simple 1D example we will use <u>**logistic regression**</u>. Logistic regression is a simple linear classifer: it passes a linear (weighted) sum of the obvservable(s) through a sigmoid function, to map any number to a probability that the sample belongs to $\\mathcal{H}_0$ or $\\mathcal{H}_1$. By minimising the loss in training, we find the optimal weights for each observable. More information can be found in [1]. \n",
    "\n",
    "Note, the SBI approach extends to more complicated classifier architectures (e.g. BDTs, neural networks), which become useful when dealing with more complex inference problems i.e. higher dimensions, more complicated features.\n",
    "\n",
    "[1] - Hastie, T. et al, \"The Elements of Statistical Learning\", Section 4.4 [Link](https://www.sas.upenn.edu/~fdiebold/NoHesitations/BookAdvanced.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression classifier to distinguish between H0 and H1\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "X_train = train_data[['x']].values\n",
    "y_train = train_data['label'].values.astype(int)\n",
    "# Fit classifier with BCE loss\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the classifier $\\hat{f}(x)$ has been trained, we can examine the output. Let's first generate independent simulation samples under the two hypotheses. We can then compare the classifier output score distribution from the simulation, to that of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classifier scores on independent test data from the two classes\n",
    "num_test_per_class = 100000\n",
    "test_H0 = generate_synthetic_data(num_test_per_class, loc=0, scale=1)\n",
    "test_H1 = generate_synthetic_data(num_test_per_class, loc=1, scale=1)\n",
    "scores_H0 = clf.predict_proba(test_H0[['x']].values)[:, 1]\n",
    "scores_H1 = clf.predict_proba(test_H1[['x']].values)[:, 1]\n",
    "\n",
    "# Evaluate classifier scores on observed data\n",
    "scores_obs = clf.predict_proba(data_obs[['x']].values)[:, 1]\n",
    "\n",
    "# Plot histograms of the classifier scores\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(scores_H0, bins=100, range=(0,1), alpha=0.5, label='H0', density=True)\n",
    "ax.hist(scores_H1, bins=100, range=(0,1), alpha=0.5, label='H1', density=True)\n",
    "ax.hist(scores_obs, bins=100, range=(0,1), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax.set_xlabel('Classifier score, $\\\\hat{f}(x)$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the observed classifier score distribution help identify the correct hypothesis? Let's do the full test...\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Using the likelihood-ratio trick, the classifier output can be converted to the density ratio:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)} \\approx \\frac{p(x_i|\\mathcal{H}_1)}{p(x_i|\\mathcal{H}_0)}\n",
    "$$\n",
    "\n",
    "With this, we can approximate the test-statistic over the full ($N_{obs}=10$) dataset ($\\mathcal{D}$):\n",
    "\n",
    "$$\n",
    "t \\approx -2  \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\\bigg)}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "In the same way as the analytic approach, we will write python functions to calculate the log-likelihood ratio and the test-statistic. These functions evaluate the classifier on input data $\\{x_i\\} \\in \\mathcal{D}$, convert the classifier output to the log-likelihood-ratio per sample, and then calculate the test-statistic over the dataset by summing the values and multiplying by -2. Note, we clip the classifier output scores to avoid log(0) in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_clf(x, clf, clip=1e-10):\n",
    "    scores = clf.predict_proba(x.reshape(-1, 1))[:, 1]\n",
    "    # Avoid log(0) by clipping scores\n",
    "    scores = np.clip(scores, clip, 1-clip)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    return llr\n",
    "\n",
    "def test_statistic_clf(x, clf):\n",
    "    llr = log_likelihood_ratio_clf(x, clf)\n",
    "    return -2 * np.sum(llr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can perform the same hypothesis-testing procedure but now using the learned test-statistic. Let's generate toy datasets under each hypothesis with $N_{obs}=10$. For each toy we will calculate the analytic and learned test-statistic values, building up distributions of each under the two hypotheses. We can then calculate the analytic and learned test-statistic values for the observed dataset, and calculate the $p$-values with respect to the null hypothesis.\n",
    "\n",
    "We will also compare the statistical power of each hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_toys = 10000\n",
    "test_statistic_H0_clf = []\n",
    "test_statistic_H1_clf = []\n",
    "test_statistic_H0_analytic = []\n",
    "test_statistic_H1_analytic = []\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = generate_synthetic_data(N_obs, loc=0, scale=1, seed=None)\n",
    "    t_H0_clf = test_statistic_clf(samples_H0['x'].values, clf)\n",
    "    t_H0_analytic = test_statistic(samples_H0['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H0_clf.append(t_H0_clf)\n",
    "    test_statistic_H0_analytic.append(t_H0_analytic)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = generate_synthetic_data(N_obs, loc=1, scale=1, seed=None)\n",
    "    t_H1_clf = test_statistic_clf(samples_H1['x'].values, clf)\n",
    "    t_H1_analytic = test_statistic(samples_H1['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)\n",
    "    test_statistic_H1_clf.append(t_H1_clf)\n",
    "    test_statistic_H1_analytic.append(t_H1_analytic)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs_clf = test_statistic_clf(data_obs['x'].values, clf)\n",
    "t_obs_analytic = test_statistic(data_obs['x'], mu0=0, mu1=1, sigma0=1, sigma1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-values for H0\n",
    "p_value_H0_clf = np.sum(np.array(test_statistic_H0_clf) <= t_obs_clf) / N_toys\n",
    "p_value_H0_analytic = np.sum(np.array(test_statistic_H0_analytic) <= t_obs_analytic) / N_toys\n",
    "\n",
    "# Calculate the type-1 and type-2 errors for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "critical_value_analytic = np.percentile(test_statistic_H0_analytic, alpha * 100)\n",
    "type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "type_2_error_analytic = np.sum(np.array(test_statistic_H1_analytic) > critical_value_analytic) / N_toys\n",
    "power_clf = 1 - type_2_error_clf\n",
    "power_analytic = 1 - type_2_error_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results: compare analytic vs classifier-based test statistics\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "ax[0].hist(test_statistic_H0_analytic, bins=50, alpha=0.5, range=(-50,50), label='H0 analytic', color='blue')\n",
    "ax[0].hist(test_statistic_H0_clf, bins=50, range=(-50,50), label='H0 classifier', color='blue', histtype='step', linewidth=2)\n",
    "ax[0].hist(test_statistic_H1_analytic, bins=50, alpha=0.5, range=(-50,50), label='H1 analytic', color='orange')\n",
    "ax[0].hist(test_statistic_H1_clf, bins=50, range=(-50,50), label='H1 classifier', color='orange', histtype='step', linewidth=2)\n",
    "ax[0].axvline(t_obs_analytic, color='black', linestyle='solid', linewidth=2, label='Observed data analytic ($p$={:.3f})'.format(p_value_H0_analytic))\n",
    "ax[0].axvline(t_obs_clf, color='grey', linestyle='dashed', linewidth=2, label='Observed data classifier ($p$={:.3f})'.format(p_value_H0_clf))\n",
    "\n",
    "ax[0].set_xlabel('$t = 2\\\\Delta$NLL')\n",
    "ax[0].set_xlim(-75, 75)\n",
    "ax[0].set_ylabel('Number of pseudo-experiments')\n",
    "ax[0].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "# Add text to plot with type-1, type-2 errors and power for both methods\n",
    "textstr = '\\n'.join((\n",
    "    'Analytic:',\n",
    "    ' Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    ' Type-2 error ($\\\\beta$): {:.3f}'.format(type_2_error_analytic),\n",
    "    ' Power (1-$\\\\beta$): {:.3f}'.format(power_analytic),\n",
    "    '',\n",
    "    'Classifier:',\n",
    "    ' Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    ' Type-2 error ($\\\\beta$): {:.3f}'.format(type_2_error_clf),\n",
    "    ' Power (1-$\\\\beta$): {:.3f}'.format(power_clf)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "ax[0].text(0.05, 0.95, textstr, transform=ax[0].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Plot ROC curves for both methods\n",
    "alpha_vals = np.linspace(0.0, 1.0, 50)\n",
    "beta_vals_analytic = []\n",
    "beta_vals_clf = []\n",
    "for alpha in alpha_vals:\n",
    "    critical_value_analytic = np.percentile(test_statistic_H0_analytic, alpha * 100)\n",
    "    type_2_error_analytic = np.sum(np.array(test_statistic_H1_analytic) > critical_value_analytic) / N_toys\n",
    "    beta_vals_analytic.append(type_2_error_analytic)\n",
    "\n",
    "    critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "    type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "    beta_vals_clf.append(type_2_error_clf)\n",
    "\n",
    "fpr = alpha_vals\n",
    "tpr_analytic = 1 - np.array(beta_vals_analytic)\n",
    "tpr_clf = 1 - np.array(beta_vals_clf)\n",
    "\n",
    "# Calculate AUC using trapezoidal rule\n",
    "auc_analytic = np.trapezoid(tpr_analytic, fpr)\n",
    "auc_clf = np.trapezoid(tpr_clf, fpr)\n",
    "\n",
    "# Plot the ROC curve (beta vs alpha) for both methods\n",
    "ax[1].plot(fpr, tpr_analytic, label='ROC Curve Analytic (AUC = {:.3f})'.format(auc_analytic), color='blue')\n",
    "ax[1].plot(fpr, tpr_clf, label='ROC Curve Classifier (AUC = {:.3f})'.format(auc_clf), color='orange', linestyle='dashed')\n",
    "ax[1].plot([0, 1], [0, 1], color='black', linestyle='dashed', label='Random Guess (AUC = 0.5)')\n",
    "ax[1].set_xlabel('False Positive Rate ($\\\\alpha$)')\n",
    "ax[1].set_ylabel('True Positive Rate (1-$\\\\beta$)')\n",
    "ax[1].set_xlim(0,1)\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have \"learned\" a test-statistic which is essentially identical to the analytic solution. Instead, this was derived using a simple logistic regression binary classifier. We have shown how to use the output of this classifier to perform a full hypothesis tests. The observed data is inconsistent with the null hypothesis ($\\mathcal{H}_0$), with a $p$-value of less than 0.05. Therefore, under the two-class scenario, we conclude that the observed data support the $\\mathcal{H}_1$ hypothesis. \n",
    "\n",
    "In this simple 1D Gaussian example, we were able to use the analytic solution as a point of comparison. For most real-world scenarios, we do not have the analytic likelihood. In the next section, we will explore an example in which the analytic likelihood is not known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# 4. Hypothesis test in higher-dimensions (particle spin example) <a id='complex_hypothesis_test'></a>\n",
    "\n",
    "Let's use what we have learned in the previous section to perform a hypothesis test when the analytic likelihood is not known. \n",
    "\n",
    "This example concerns the measurement of particle's spin; a purely quantum mechanical property of an elementary particle. It is an intrinsic form of angular momentum, but unlike everyday spinning objects, it does not correspond to any literal rotation in space. Instead, it influences how particles behave, decay, and interact with forces, making it possible to distinguish different types of particles by their spin values.\n",
    "\n",
    "We are going to study the spin of a hypothetical particle $X$, by performing an experiment to look at how it decays.\n",
    "\n",
    "### Aim: \n",
    "\n",
    "Infer the spin-configuration of a particle $X$. \n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Three observables ($x_1$, $x_2$, $x_3$) related to the decay products of $X \\rightarrow aa$. The observed dataset contains $N_{obs}=10$ particle $X$ decays, and is stored in the file `data_spin.csv`.\n",
    "\n",
    "### Hypotheses: \n",
    "* $\\mathcal{H}_0$ = Spin-0 particle\n",
    "* $\\mathcal{H}_1$ = Spin-1 particle\n",
    "\n",
    "### Simulation: \n",
    "\n",
    "We have a faithful particle decay simulator which can reproduce the observable distributions of $X \\rightarrow aa$ under each hypothesis. The simulator can generate $N$ decays using the following code:\n",
    "```python\n",
    "sim_H0 = run_simulation(N, hypothesis='H0')\n",
    "sim_H1 = run_simulation(N, hypothesis='H1')\n",
    "```\n",
    "\n",
    "### Tasks: \n",
    "1. Data exploration to understand the problem\n",
    "1. Train a binary classifier to distinguish decays generated under $\\mathcal{H}_0$ from those under $\\mathcal{H}_1$\n",
    "1. Use the classifier output to perform a hypothesis test on the observed data and infer the spin-configuration of particle $X$.\n",
    "\n",
    "\n",
    "## Data exploration\n",
    "\n",
    "We will begin by generating synthetic data (simulation) and looking at the expected observable distributions under the two hypotheses. Now the simulator is a \"black-box\"... all we know is that it produces faithful synthetic data under the two hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi_utils import run_simulation\n",
    "N_train = 100000\n",
    "sim_H0 = run_simulation(N_train, hypothesis='H0')\n",
    "sim_H1 = run_simulation(N_train, hypothesis='H1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot observable distributions of synthetic data for each hypothesis\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,4)) \n",
    "axs[0].hist(sim_H0['x1'], bins=50, range=(-4,4), alpha=0.5, label='H0', density=True)\n",
    "axs[0].hist(sim_H1['x1'], bins=50, range=(-4,4), alpha=0.5, label='H1', density=True)\n",
    "axs[0].set_xlabel('x1')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "axs[1].hist(sim_H0['x2'], bins=50, range=(-4,4), alpha=0.5, label='H0', density=True)\n",
    "axs[1].hist(sim_H1['x2'], bins=50, range=(-4,4), alpha=0.5, label='H1', density=True)\n",
    "axs[1].set_xlabel('x2')\n",
    "axs[1].set_ylabel('Density')\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "axs[2].hist(sim_H0['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='H0', density=True)\n",
    "axs[2].hist(sim_H1['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='H1', density=True)\n",
    "axs[2].set_xlabel('x3')\n",
    "axs[2].set_ylabel('Density')\n",
    "axs[2].legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $(x1,x2)$ appear to be Gaussian distributed, where $x_1$ differs in mean between $\\mathcal{H}_0$ and $\\mathcal{H}_1$, while $x_2$ differs in the width. The third observable $x_3$ follows a sinusoidal-like distribution for $\\mathcal{H}_1$, and is uniform for $\\mathcal{H}_0$.\n",
    "\n",
    "We can also look at 2D pairs of observables..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15,10))\n",
    "axs[0][0].hist2d(sim_H0['x1'], sim_H0['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Blues')\n",
    "axs[0][0].set_xlabel('x1')\n",
    "axs[0][0].set_ylabel('x2')\n",
    "axs[0][0].set_title('H0')\n",
    "axs[1][0].hist2d(sim_H1['x1'], sim_H1['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Oranges')\n",
    "axs[1][0].set_xlabel('x1')\n",
    "axs[1][0].set_ylabel('x2')\n",
    "axs[1][0].set_title('H1')\n",
    "axs[0][1].hist2d(sim_H0['x1'], sim_H0['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Blues')\n",
    "axs[0][1].set_xlabel('x1')\n",
    "axs[0][1].set_ylabel('x3')\n",
    "axs[0][0].set_title('H0')\n",
    "axs[1][1].hist2d(sim_H1['x1'], sim_H1['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Oranges')\n",
    "axs[1][1].set_xlabel('x1')\n",
    "axs[1][1].set_ylabel('x3')\n",
    "axs[1][0].set_title('H1')\n",
    "axs[0][2].hist2d(sim_H0['x2'], sim_H0['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Blues')\n",
    "axs[0][2].set_xlabel('x2')\n",
    "axs[0][2].set_ylabel('x3')\n",
    "axs[0][0].set_title('H0')\n",
    "axs[1][2].hist2d(sim_H1['x2'], sim_H1['x3'], bins=50, range=[[-4,4],[-np.pi,np.pi]], density=True, cmap='Oranges')\n",
    "axs[1][2].set_xlabel('x2')\n",
    "axs[1][2].set_ylabel('x3')\n",
    "axs[1][0].set_title('H1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two observables $x_1$ and $x_1$ have a different correlation structure for the two hypotheses:\n",
    "* $\\mathcal{H}_0$ (spin 0) = positive correlation\n",
    "* $\\mathcal{H}_1$ (spin 1) = uncorrelated\n",
    "\n",
    "The classifier will be able to leverage this higher-dimensional information (correlation between features) to better discriminate between $\\mathcal{H}_0$ and $\\mathcal{H}_1$. Better discrimination leads to an improvement in the statistical power of the test.\n",
    "\n",
    "Let's look at how the observed dataset is distributed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_obs = pd.read_csv('data_spin.csv')\n",
    "N_obs = len(data_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15,4))\n",
    "ax[0].hist(data_obs['x1'], bins=50, range=(-4,4), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax[0].hist(sim_H0['x1'], bins=50, range=(-4,4), alpha=0.5, label='H0', density=True)\n",
    "ax[0].hist(sim_H1['x1'], bins=50, range=(-4,4), alpha=0.5, label='H1', density=True)\n",
    "ax[0].set_xlabel('x1')\n",
    "ax[0].set_ylabel('Density')\n",
    "ax[0].legend(loc='upper left')\n",
    "\n",
    "ax[1].hist(data_obs['x2'], bins=50, range=(-4,4), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax[1].hist(sim_H0['x2'], bins=50, range=(-4,4), alpha=0.5, label='H0', density=True)\n",
    "ax[1].hist(sim_H1['x2'], bins=50, range=(-4,4), alpha=0.5, label='H1', density=True)\n",
    "ax[1].set_xlabel('x2')\n",
    "ax[1].set_ylabel('Density')\n",
    "ax[1].legend(loc='upper left')\n",
    "\n",
    "ax[2].hist(data_obs['x3'], bins=50, range=(-np.pi,np.pi), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax[2].hist(sim_H0['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='H0', density=True)\n",
    "ax[2].hist(sim_H1['x3'], bins=50, range=(-np.pi,np.pi), alpha=0.5, label='H1', density=True)\n",
    "ax[2].set_xlabel('x3')\n",
    "ax[2].set_ylabel('Density')\n",
    "ax[2].legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D histogram in (x1,x2): observed data overlaid as scatter points on top of density plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "axs[0].hist2d(sim_H0['x1'], sim_H0['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Blues')\n",
    "axs[0].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[0].set_xlabel('x1')\n",
    "axs[0].set_ylabel('x2')\n",
    "axs[0].set_title('H0')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist2d(sim_H1['x1'], sim_H1['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Oranges')\n",
    "axs[1].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[1].set_xlabel('x1')\n",
    "axs[1].set_ylabel('x2')\n",
    "axs[1].set_title('H1')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell by eye which class the observed data belong to? Now it is not so simple!\n",
    "\n",
    "Let's train a classifier and set up a proper hypothesis test as in [Section 3](#simple_hypothesis_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier\n",
    "In the 1D Gaussian example, we used a simple logistic regression for the classification task. This may not be sufficient when we extend to more complicated inference tasks. In this 3D example, we will now use a neural network to classify $\\mathcal{H}_0$ vs $\\mathcal{H}_1$. This will be a simple multi-layer perceptron (MLP), which will be trained using the simulated datasets. We will use the `torch` library to build the MLP.\n",
    "\n",
    "It is important to note that this approach to SBI does not depend on the architecture of the classifier. You can use whichever ML model is most appropriate to the problem (e.g. BDT, CNN, GNN etc). All the method requires is training with a BCE loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to convert pandas dataframe to a torch tensor\n",
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training dataset \n",
    "sim_H0['label'] = 0\n",
    "sim_H1['label'] = 1\n",
    "sim = pd.concat([sim_H0, sim_H1], ignore_index=True)\n",
    "\n",
    "# Split into test/train sets\n",
    "test_size = 0.2\n",
    "sim_train, sim_test = train_test_split(sim, test_size=test_size, shuffle=True)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = df_to_tensor(sim_train[['x1', 'x2', 'x3']])\n",
    "y_train = df_to_tensor(sim_train[['label']])\n",
    "X_test = df_to_tensor(sim_test[['x1', 'x2', 'x3']])\n",
    "y_test = df_to_tensor(sim_test[['label']])\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 2048\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple feedforward MLP with two hidden layers: input -> hidden -> hidden -> output (1)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "input_size = 3\n",
    "hidden_size = 16\n",
    "model = SimpleMLP(input_size, hidden_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: track loss per epoch for training and esting datasets\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * inputs.size(0)\n",
    "    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the training history to see if we have reached the plateau..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks reasonable!\n",
    "\n",
    "Now let's take a look at the classifier output. We will plot the distribution of $f(x)$ for $\\mathcal{H}_0$ and $\\mathcal{H}_1$ from the test dataset, and compare this to the classifier output distribution for the observed data. \n",
    "\n",
    "Does this help indicate which hypothesis the data belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test simulation samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sim_test_outputs = model(X_test).numpy().flatten()\n",
    "sim_test_labels = y_test.numpy().flatten()\n",
    "\n",
    "# Also evaluate for the observed data\n",
    "X_obs = df_to_tensor(data_obs[['x1', 'x2', 'x3']])\n",
    "with torch.no_grad():\n",
    "    obs_outputs = model(X_obs).numpy().flatten()\n",
    "\n",
    "# Plot histograms of the classifier scores\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(sim_test_outputs[sim_test_labels==0], bins=100, range=(0,1), alpha=0.5, label='H0', density=True)\n",
    "ax.hist(sim_test_outputs[sim_test_labels==1], bins=100, range=(0,1), alpha=0.5, label='H1', density=True)\n",
    "ax.hist(obs_outputs, bins=100, range=(0,1), color='black', histtype='step', label='Observed Data', density=True)\n",
    "ax.set_xlabel('Classifier score, $\\\\hat{f}(\\\\mathbf{x})$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing\n",
    "Following the same procedure as in [Section 3](#simple_hypothesis_test), we will now use the classifier output to perform a hypothesis test. \n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "In exactly the same way, we can convert the classifier output to the density ratio using the likelihood-ratio trick:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)} \\approx \\frac{p(x_i|\\mathcal{H}_1)}{p(x_i|\\mathcal{H}_0)}\n",
    "$$\n",
    "\n",
    "And with this, we can approximate the test-statistic over the full ($N_{obs}=10$) dataset:\n",
    "\n",
    "$$\n",
    "t = 2\\Delta\\mathrm{NLL} = -2 \\ln{\\frac{p(\\mathcal{D}|\\mathcal{H}_1)}{p(\\mathcal{D}|\\mathcal{H}_0)}} \\approx -2  \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i)}{1-\\hat{f}(x_i)}\\bigg)}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "Let's rewrite the python functions to be compatible with the torch-based model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_torch(X, model, clip=1e-10):\n",
    "    with torch.no_grad():\n",
    "        scores = model(torch.tensor(X, dtype=torch.float32)).numpy().flatten()\n",
    "    # Avoid log(0) by clipping scores\n",
    "    scores = np.clip(scores, clip, 1-clip)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    return llr\n",
    "\n",
    "def test_statistic_torch(X, model):\n",
    "    llr = log_likelihood_ratio_torch(X, model)\n",
    "    return -2 * np.sum(llr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the hypothesis test we first need to generate toy datasets under each hypothesis with $N_{obs}=10$. We will build up distributions of the learned test-statistic for each hypothesis. We then calculate the test-statistic value for the observed dataset, and determine the $p$-value with respect to the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_toys = 10000\n",
    "test_statistic_H0_clf = []\n",
    "test_statistic_H1_clf = []\n",
    "for _ in range(N_toys):\n",
    "    # Samples under H0\n",
    "    samples_H0 = run_simulation(N_obs, hypothesis='H0')\n",
    "    t_H0_clf = test_statistic_torch(samples_H0[['x1', 'x2', 'x3']].values, model)\n",
    "    test_statistic_H0_clf.append(t_H0_clf)\n",
    "\n",
    "    # Samples under H1\n",
    "    samples_H1 = run_simulation(N_obs, hypothesis='H1')\n",
    "    t_H1_clf = test_statistic_torch(samples_H1[['x1', 'x2', 'x3']].values, model)\n",
    "    test_statistic_H1_clf.append(t_H1_clf)\n",
    "\n",
    "# Calculate observed test statistic\n",
    "t_obs_clf = test_statistic_torch(data_obs[['x1', 'x2', 'x3']].values, model)\n",
    "\n",
    "# Calculate the p-value for H0\n",
    "p_value_H0_clf = np.sum(np.array(test_statistic_H0_clf) <= t_obs_clf) / N_toys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we will calculate the Type-I and Type-II errors, as well as the power of the statistical test for fixed $\\alpha=0.05$. We will also compute the errors for different $\\alpha$ values to construct the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the type-1 and type-2 errors for fixed alpha=0.05\n",
    "alpha = 0.05\n",
    "type_1_error = alpha\n",
    "critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "power_clf = 1 - type_2_error_clf\n",
    "\n",
    "# Plot the distributions of the test statistic under both hypotheses, as well as the observed case\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,6))\n",
    "axs[0].hist(test_statistic_H0_clf, bins=50, alpha=0.5, range=(-50,50), label='H0')\n",
    "axs[0].hist(test_statistic_H1_clf, bins=50, alpha=0.5, range=(-50,50), label='H1')\n",
    "axs[0].axvline(t_obs_clf, color='black', linestyle='solid', linewidth=2, label='Observed data ($p$={:.3f})'.format(p_value_H0_clf))\n",
    "axs[0].axvline(critical_value_clf, color='red', linestyle='dashed', linewidth=2, label='Critical value (α={})'.format(alpha))\n",
    "axs[0].set_xlabel('$t = 2\\\\Delta$NLL')\n",
    "axs[0].set_xlim(-50, 50)\n",
    "axs[0].set_ylabel('Number of pseudo-experiments')\n",
    "axs[0].legend(loc='upper right', fontsize=8)\n",
    "# Add text to plot with type-1, type-2 errors and power\n",
    "textstr = '\\n'.join((\n",
    "    'Type-1 error ($\\\\alpha$): {:.3f}'.format(type_1_error),\n",
    "    'Type-2 error ($\\\\beta$): {:.3f}'.format(type_2_error_clf),\n",
    "    'Power (1-$\\\\beta$): {:.3f}'.format(power_clf)))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "axs[0].text(0.05, 0.95, textstr, transform=axs[0].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "# Plot ROC curve\n",
    "alpha_vals = np.linspace(0.0, 1.0, 1000)\n",
    "beta_vals_clf = []\n",
    "for alpha in alpha_vals:\n",
    "    critical_value_clf = np.percentile(test_statistic_H0_clf, alpha * 100)\n",
    "    type_2_error_clf = np.sum(np.array(test_statistic_H1_clf) > critical_value_clf) / N_toys\n",
    "    beta_vals_clf.append(type_2_error_clf)\n",
    "fpr = alpha_vals\n",
    "tpr_clf = 1 - np.array(beta_vals_clf)\n",
    "# Calculate AUC using trapezoidal rule\n",
    "auc_clf = np.trapezoid(tpr_clf, fpr)\n",
    "axs[1].plot(fpr, tpr_clf, label='ROC Curve Classifier (AUC = {:.3f})'.format(auc_clf), color='orange')\n",
    "axs[1].plot([0, 1], [0, 1], color='black', linestyle='dashed', label='Random Guess (AUC = 0.5)')\n",
    "axs[1].set_xlabel('False Positive Rate ($\\\\alpha$)')\n",
    "axs[1].set_ylabel('True Positive Rate (1-$\\\\beta$)')\n",
    "axs[1].legend(loc='best')\n",
    "axs[1].set_xlim(0,1)\n",
    "axs[1].set_ylim(0,1)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We were presented with a research problem with an unknown (intractable) likelihood i.e. we do not know the probability of observing data sample $(x_1,x_2,x_3)$ under hypothesis $\\mathcal{H}_i$: $p(x_1,x_2,x_3|\\mathcal{H}_i)$.\n",
    "\n",
    "Nevertheless, we had access to an accurate simulation of the data. We used this simulation to train a classifier to distinguish $\\mathcal{H}_0$ (spin-0 particle) from $\\mathcal{H}_1$ (spin-1 particle).\n",
    "\n",
    "The output of the classifier was used to approximate the probability density ratio. This was then used to calculate the log-likelihood-ratio test-statistic over the full dataset. Using a frequentist approach, we compared the observed value of the \"learned\" test-statistic to the distributions generated from toy experiments under each hypothesis. \n",
    "\n",
    "The data is inconsistent with the null hypothesis ($\\mathcal{H}_0$) with a $p$-value of less than 0.05. Therefore we can reject $\\mathcal{H}_0$ and infer that particle $X$ is spin-one!\n",
    "\n",
    "This is an end-to-end example of using ML to perform SBI. In the next section we will go beyond hypothesis testing, and show how to use ML classifiers for parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### 4.1 Amortized inference\n",
    "One great advantage of this approach is that we can use the trained classifier to perform inference on different observations, assuming that the experiment is performed under identical conditions. This means we do not need to re-run an expensive inference procedure each time. In other words, we have **amortized** the inference for future experiments. This becomes extremely useful when dealing with problems with extremely complex likelihoods.\n",
    "\n",
    "We re-run the experiment, where $X \\rightarrow aa$ decays are recorded over a longer period such that we obtain a larger dataset. This dataset can be found in `data_spin_extension.csv`. Your task is to repeat the hypothesis-test procedure using the larger dataset with the trained classifier. How do your conclusions change? Is this in line with what you expect from taking more data?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### 4.2 Power of the statistical test\n",
    "Above we evaluated the statistical power of the test statistic. Using `data_spin.csv` investigate how the power diminishes when you remove the $x_1$ observable. Compare the ROC curves obtained with the 3D and 2D inputs? What does this tell us about the impact of adding more information on the ability to resolve the two hypotheses?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_reduced = sim[['x2', 'x3']]  # Use only x1 and x2 features\n",
    "input_size = 2\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# 5. Parameter estimation with parametric classifiers <a id='parameter_estimation'></a>\n",
    "\n",
    "This section will describe how to perform parameter estimation (with the extraction of confidence intervals) using SBI. Note, this is essentially an extension of the hypothesis testing procedure where the alternative hypothesis $\\mathcal{H}_1$ is a \"composite\" hypothesis, containing the ensemble of all possible values for the parameter of interest. As discussed in the introduction, for parameter estimation we need to learn the conditional probability density: $p(x|\\theta)$.\n",
    "This quantity tells us the probabilty of observing data $x$ given parameter value $\\theta$. In fact, what we want to learn for inference with classifiers is the conditional density ratio:\n",
    "\n",
    "$$\n",
    "\\frac{p(x|\\theta)}{p(x|\\theta_0)}\n",
    "$$\n",
    "\n",
    "where $\\theta_0$ is some reference value of the parameter. We can then define a log-likelihood-ratio test-statistic to infer $\\theta$ from the data, and its corresponding confidence intervals. All we need is a faithful simulator that can generate $x$ for any value of $\\theta$.\n",
    "\n",
    "The parameter estimation with SBI discussed in this notebook is founded on the concept of parametric classifiers [1]. These models are trained such that the classifier output is \"parametric\" in $\\theta$: $\\hat{f}(x|\\theta)$ i.e. the decision function depends on the value of $\\theta$. \n",
    "\n",
    "In practice, we learn this by simply adding $\\theta$ as an additional parameter to the model training:\n",
    "\n",
    "![parametric neural network](pnn.png)\n",
    "\n",
    "Crucially, one must initially make the distribution of the conditional feature ($\\theta$) the same between the two classes. \n",
    "\n",
    "Let's go through the training steps in detail...\n",
    "\n",
    "1) Class 0: generate $N$ samples using the simulator for the reference hypothesis $\\theta = \\theta_0$ i.e. draw samples from $p(x|\\theta_0)$ for fixed $\\theta_0$. It is up to you what to choose for $\\theta_0$ i.e. we must pick a choice for the reference hypothesis. In theory, the final parameter estimation will be independent of this choice, but it helps to pick something sensible which is not too far from the observed parameter values.\n",
    "\n",
    "2) Class 1: generate $N$ samples using the simulator with different values of $\\theta$ i.e. draw samples from $p(x|\\theta)$ for various $\\theta$. If possible, one should generate the samples to be continuous in the values of $\\theta$ i.e. randomly sampled between some upper and lower ranges, where the ranges are chosen to be sufficiently wide for the confidence interval level that you want to probe. In practice it is often much easier to generate subsamples with discrete steps in $\\theta$. The method still works as long as the discrete steps are sufficiently fine-grained (compared to the sensitivity). We will use the latter approach in this notebook.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "3) Consider training a classifier $\\hat{f}(x,\\theta)$ to distinguish between Class 0 and Class 1 with $\\{x,\\theta\\}$ as input features. After the likelihood-ratio trick, we arrive at an approximation of the joint-likelihood ratio between the two classes:\n",
    "    $$\n",
    "    \\frac{\\hat{f}(x_i,\\theta)}{1-\\hat{f}(x_i,\\theta)} \\approx \\frac{p(x_i,\\theta|\\mathrm{Class\\,1})}{p(x_i,\\theta|\\mathrm{Class\\,0})}\n",
    "    $$\n",
    "    If we expand the joint distributions we obtain:\n",
    "    $$\n",
    "    \\frac{p(x_i,\\theta|\\mathrm{Class\\,1})}{p(x_i,\\theta|\\mathrm{Class\\,0})} = \\frac{p(x_i|\\theta)p(\\theta|\\mathrm{Class\\,1})}{p(x_i|\\theta_0)p(\\theta|\\mathrm{Class\\,0})}\n",
    "    $$\n",
    "    What we want is only the conditional density ratio:\n",
    "    $$\n",
    "    \\frac{p(x_i|\\theta)}{p(x_i|\\theta_0)}\n",
    "    $$\n",
    "    Hence, we need to make sure the conditional parameter distributions are the same between the two classes:\n",
    "    $$\n",
    "    p(\\theta|\\mathrm{Class\\,1}) = p(\\theta|\\mathrm{Class\\,0})\n",
    "    $$\n",
    "    so that the terms cancel in the joint density ratio, and we are left with just the conditional density ratio. \n",
    "    \n",
    "    In other words, we need the $\\theta$ parameter distributions in Class 0 and Class 1 to have identical densities. In practice, this means using the same sampling distribution for $\\theta$ in both classes. If they differ, the classifier will pick up class-specific information about how often certain $\\theta$ values appear in each class, rather than purely how likely the data $x$ are given $\\theta$. This would break the interpretation of the classifier output as a conditional density ratio.\n",
    "\n",
    "    So before training we need to artificially generate a $\\theta$ distribution for Class 0 by sampling from the Class 1 distribution. \n",
    "\n",
    "</div> \n",
    "\n",
    "4) After the $\\theta$ distributions between the two classes are made to be the same in training, the classifier output can be used to approximate the conditional density ratio. We will show later on how this can be used for parameter estimation.\n",
    "\n",
    "[1] - P. Baldi et al., *Parameterized neural networks for high-energy physics*, Eur. Phys. J. C 76 (2016) 235. [arXiv:1601.07913](https://www.arxiv.org/abs/1601.07913)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Gaussian example\n",
    "\n",
    "In this notebook we will follow a simple example, where the analytic likelihood is known. The example is described below:\n",
    "\n",
    "### Dataset:\n",
    "\n",
    "Two observables ($x_1, x_2$) which follow a 2D Gaussian distribution. The mean and width related to $x_1$, $\\mu_1$ and $\\sigma_1$, are known. Only the width related to $x_2$, $\\sigma_2$, is known. The two unknowns are $\\mu_2$ and the correlation between the observables $\\rho_{12}$. We are provided with a dataset of $N_{obs}=20$ samples of ($x_1, x_2$) values stored in `data_parameter_estimation.csv`. \n",
    "\n",
    "### Aim:\n",
    "\n",
    "Infer the values of $\\mu_2$ and $\\rho_{12}$ from the data, along with the corresponding confidence intervals. Comparing to the notation above $x = \\{x_1,x_2\\}$ and $\\theta = \\{\\mu_2,\\rho_{12}\\}$.\n",
    "\n",
    "### Simulation \n",
    "\n",
    "We have a simulator that can generate samples from a 2D Gaussian with arbitrary $\\mu_1$, $\\sigma_1$, $\\mu_2$, $\\sigma_2$, $\\rho_{12}$. The simulation can generate $N$ samples using the following code:\n",
    "```python\n",
    "sim = run_simulation(N, mu1, sigma1, mu2, sigma2, rho12)\n",
    "```\n",
    "\n",
    "### Analytic solution\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "The analytic (conditional) probability density for a 2D Gaussian is:\n",
    "\n",
    "$$\n",
    "p(x_1,x_2|\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,\\rho_{12})\n",
    "=\n",
    "\\frac{1}{2\\pi\\,\\sigma_1\\sigma_2\\sqrt{1-\\rho_{12}^2}}\n",
    "\\exp\\!\\left(\n",
    "-\\frac{1}{2(1-\\rho_{12}^2)}\n",
    "\\left[\n",
    "\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}\n",
    "+\n",
    "\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}\n",
    "-\n",
    "\\frac{2\\rho_{12}(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}\n",
    "\\right]\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "We will use this to compare to the \"learned\" conditional density ratio.\n",
    "\n",
    "</div>\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Run the simulation and compare to the observed data\n",
    "1. Extract analytic result\n",
    "1. Train a parametric classifier\n",
    "1. Use parametric classifier output to estimate the values and confidence intervals of $\\mu_2$ and $\\rho_{12}$.\n",
    "\n",
    "## Run simulation and data exploration\n",
    "We will begin by writing a python function to run our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(N, mu1=0, sigma1=1, mu2=0, sigma2=1, rho12=0):\n",
    "    cov_matrix = [[sigma1**2, rho12 * sigma1 * sigma2],\n",
    "                  [rho12 * sigma1 * sigma2, sigma2**2]]\n",
    "    data = np.random.multivariate_normal(mean=[mu1, mu2], cov=cov_matrix, size=N)\n",
    "    df = pd.DataFrame(data, columns=['x1', 'x2'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pick a sensible reference hypothesis ($\\theta=\\theta_0$). For this example, we will pick a 2D unit Gaussian with zero correlation.\n",
    "\n",
    "Let's run the simulation for our reference sample ($\\mathcal{H}_0$) and for a potential alternative hypothesis (from the ensemble of alternative hypotheses). In this example we know $\\mu_1=0, \\sigma_1=1$ and $\\sigma_2=1$, so we only vary $\\mu_2$ and $\\rho_{12}$ in the alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "num_train_per_class = 100000\n",
    "\n",
    "# H0: reference sample\n",
    "sim_H0 = run_simulation(num_train_per_class, mu1=0, sigma1=1, mu2=0, sigma2=1, rho12=0)\n",
    "\n",
    "# H1: example alternative hypothesis\n",
    "sim_H1 = run_simulation(num_train_per_class, mu1=0, sigma1=1, mu2=0.5, sigma2=1, rho12=0.5)\n",
    "\n",
    "# Load the data from csv file\n",
    "data_obs = pd.read_csv('data_parameter_estimation.csv')\n",
    "N_obs = len(data_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2D heatmaps for (x1,x2) and overlay observed data\n",
    "# Show for both reference sample and example alternative hypothesis\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
    "axs[0].hist2d(sim_H0['x1'], sim_H0['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Blues')\n",
    "axs[0].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[0].set_xlabel('x1')\n",
    "axs[0].set_ylabel('x2')\n",
    "axs[0].set_title('H0: reference')\n",
    "axs[0].legend()\n",
    "# Add text box with parameter values for reference sample\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "textstr = '\\n'.join((\n",
    "    r'$\\mu_1=0$ (known)',\n",
    "    r'$\\sigma_1=1$ (known)',\n",
    "    r'$\\mu_2=0$',\n",
    "    r'$\\sigma_2=1$ (known)',\n",
    "    r'$\\rho_{12}=0$'))\n",
    "axs[0].text(0.05, 0.95, textstr, transform=axs[0].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "axs[1].hist2d(sim_H1['x1'], sim_H1['x2'], bins=50, range=[[-4,4],[-4,4]], density=True, cmap='Oranges')\n",
    "axs[1].scatter(data_obs['x1'], data_obs['x2'], color='black', label='Observed Data')\n",
    "axs[1].set_xlabel('x1')\n",
    "axs[1].set_ylabel('x2')\n",
    "axs[1].set_title('H1: example alternative hypothesis')\n",
    "axs[1].legend()\n",
    "# Add text box with parameter values for example alternative hypothesis\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "textstr = '\\n'.join((\n",
    "    r'$\\mu_1=0$ (known)',\n",
    "    r'$\\sigma_1=1$ (known)',\n",
    "    r'$\\mu_2=0.5$',\n",
    "    r'$\\sigma_2=1$ (known)',\n",
    "    r'$\\rho_{12}=0.5$'))\n",
    "axs[1].text(0.05, 0.95, textstr, transform=axs[1].transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the data look more like the reference hypothesis or the example alternative hypothesis? It is hard to tell. Let's perform a proper parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic calculation of test statistic\n",
    "We will use the analytic form of the density to calculate the log-likelihood ratio. With this formula, we can calculate the test-statistic ($2\\Delta\\mathrm{NLL}$) as a function of $(\\mu_2,\\rho_{12})$ for the observed data. From this we can determine the best-fit value (minimum) and the corresponding confidence intervals.\n",
    "\n",
    "Make sure you understand the form of the function below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_analytic(x, params, params_ref):\n",
    "    mu1, sigma1, mu2, sigma2, rho12 = params\n",
    "    mu1_ref, sigma1_ref, mu2_ref, sigma2_ref, rho12_ref = params_ref\n",
    "\n",
    "    cov_matrix = np.array([[sigma1**2, rho12 * sigma1 * sigma2],\n",
    "                           [rho12 * sigma1 * sigma2, sigma2**2]])\n",
    "    cov_matrix_ref = np.array([[sigma1_ref**2, rho12_ref * sigma1_ref * sigma2_ref],\n",
    "                               [rho12_ref * sigma1_ref * sigma2_ref, sigma2_ref**2]])\n",
    "    \n",
    "    inv_cov = np.linalg.inv(cov_matrix)\n",
    "    inv_cov_ref = np.linalg.inv(cov_matrix_ref)\n",
    "    \n",
    "    diff = x - np.array([mu1, mu2])\n",
    "    diff_ref = x - np.array([mu1_ref, mu2_ref])\n",
    "\n",
    "    ll = -0.5 * np.einsum('ij,jk,ik->i', diff, inv_cov, diff) - 0.5 * np.log(np.linalg.det(cov_matrix)) - np.log(2 * np.pi)\n",
    "    ll_ref = -0.5 * np.einsum('ij,jk,ik->i', diff_ref, inv_cov_ref, diff_ref) - 0.5 * np.log(np.linalg.det(cov_matrix_ref)) - np.log(2 * np.pi)\n",
    "    return ll-ll_ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan over mu2 and rho12 values and calculate llr for observed data at each point\n",
    "mu2_points = np.linspace(-1, 1, 100)\n",
    "rho12_points = np.linspace(-0.99, 0.99, 100)\n",
    "llr_vals = np.zeros((len(mu2_points), len(rho12_points)))\n",
    "params_ref = (0, 1, 0, 1, 0)  # Reference parameters (H0)\n",
    "for i, mu2 in enumerate(mu2_points):\n",
    "    for j, rho12 in enumerate(rho12_points):\n",
    "        params = (0, 1, mu2, 1, rho12)  # Current parameters\n",
    "        llr = log_likelihood_ratio_analytic(data_obs[['x1', 'x2']].values, params, params_ref)\n",
    "        llr_vals[i, j] = np.sum(llr)\n",
    "\n",
    "# Convert llr to test statistic\n",
    "test_statistic_analytic = -2 * (llr_vals-np.max(llr_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the test-statistic surface $t(\\mu_2,\\rho_{12})$ in the $(\\mu_2,\\rho_{12})$ plane. The best-fit parameter value corresponds to the minimum of the test-statistic curve. \n",
    "\n",
    "Using Wilk's theorem [1], this $2\\Delta\\mathrm{NLL}$ test-statistic distribution asymptotically approaches the $\\chi^2$-distribution with $n=2$ degrees of freedom. We will use this \"asymptotic approximation\" to estimate the 68% and 95% confidence intervals. These are the union of points in the plane for which $2\\Delta\\mathrm{NLL}$ is less than 2.3 and 5.99, respectively. In principle, one should check the validity of Wilk's theorem with the Neyman construction of confidence intervals (using toys), but we will not go into that in this notebook.\n",
    "\n",
    "Let's plot the test-statistic surface, and highlight the best-fit, 68% and 95% CL intervals for the $(\\mu_2,\\rho_{12})$ parameters. \n",
    "\n",
    "[1] - Wilk's Theorem. [Wikipedia link](https://en.wikipedia.org/wiki/Wilks%27_theorem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "# Set limit on color scale as t=20\n",
    "c = ax.imshow(test_statistic_analytic.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "              origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=20)\n",
    "ax.set_xlabel(r'$\\mu_2$')\n",
    "ax.set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c, ax=ax, label=r'Test Statistic $t = 2\\Delta NLL$')\n",
    "# Overlay contour lines\n",
    "contours = ax.contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles='dashed')\n",
    "ax.clabel(contours, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "# Add minimum point\n",
    "min_idx = np.unravel_index(np.argmin(test_statistic_analytic), test_statistic_analytic.shape)\n",
    "ax.plot(mu2_points[min_idx[0]], rho12_points[min_idx[1]], marker='x', color='red', markersize=5, label='Best Fit', linestyle='None')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data favours a positive correlation ($\\rho_{12}$), with a mean $\\mu_2$ around zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a parametric classifier\n",
    "Now we will perform the parameter estimation using SBI.\n",
    "\n",
    "As described above we need to generate samples for Class 1 i.e. where we draw samples from the simulator for various values of $\\theta = \\{\\mu_2,\\rho_{12}\\}$. \n",
    "\n",
    "We will create subsamples with small discrete points in a grid of $\\mu_2$ and $\\rho_{12}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_H1_ensemble = []\n",
    "mu2_train_vals = np.linspace(-1,1,50)\n",
    "rho12_train_vals = np.linspace(-0.9999,0.9999,50) \n",
    "# Calculate the number of training samples per parameter point to keep total = num_train_per_class\n",
    "num_train_per_subsample = num_train_per_class // (len(mu2_train_vals) * len(rho12_train_vals))\n",
    "for mu2 in mu2_train_vals:\n",
    "    for rho12 in rho12_train_vals:\n",
    "        sim_subsample = run_simulation(num_train_per_subsample, mu1=0, sigma1=1, mu2=mu2, sigma2=1, rho12=rho12)\n",
    "        sim_subsample['mu2'] = mu2\n",
    "        sim_subsample['rho12'] = rho12\n",
    "        sim_H1_ensemble.append(sim_subsample)\n",
    "\n",
    "# Concatenate all subsamples into a single dataframe and add label\n",
    "sim_H1 = pd.concat(sim_H1_ensemble, ignore_index=True)\n",
    "sim_H1['label'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step is crucial to ensure that we learn the conditional density ratio. We need to match the distribution of the conditional parameters $\\mu_2,\\rho_{12}$ in Class 0 (reference) to Class 1. This ensures that the classifier will not pick up class-specific information about how often certain $\\theta$ values appear in each class, and instead will learn how likely the data $x$ are given $\\theta$.\n",
    "\n",
    "To do this, for each sample in Class 0 we randomly choose $\\mu_2,\\rho_{12}$ values from the possible discrete values used to generate Class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_H0['mu2'] = np.random.choice(mu2_train_vals, size=len(sim_H0))\n",
    "sim_H0['rho12'] = np.random.choice(rho12_train_vals, size=len(sim_H0))\n",
    "\n",
    "# Add label for H0\n",
    "sim_H0['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explicitly check that the conditional parameter distributions are identical (give or take statistical fluctuations)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,4))\n",
    "ax[0].hist(sim_H0['mu2'], bins=50, range=(-1,1), alpha=0.5, label='Class 0')\n",
    "ax[0].hist(sim_H1['mu2'], bins=50, range=(-1,1), alpha=0.5, label='Class 1')\n",
    "ax[0].set_xlabel(r'$\\mu_2$')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[0].set_ylim(0, ax[0].get_ylim()[1]*1.2)\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].hist(sim_H0['rho12'], bins=50, range=(-1,1), alpha=0.5, label='Class 0')\n",
    "ax[1].hist(sim_H1['rho12'], bins=50, range=(-1,1), alpha=0.5, label='Class 1')\n",
    "ax[1].set_xlabel(r'$\\rho_{12}$')\n",
    "ax[1].set_ylabel('Counts')\n",
    "ax[1].set_ylim(0, ax[1].get_ylim()[1]*1.2)\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now that the conditional parameter distributions are the same, we can correctly train a parametric classifier.\n",
    "\n",
    "We will again use a feed-forward MLP for this (relatively) simple example.\n",
    "\n",
    "Let's first prepare the datasets for training and then build the model. Note, the model takes as input both $(x_1,x_2)$ and $(\\mu_2,\\rho_{12})$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets and split into train/test sets\n",
    "sim = pd.concat([sim_H0, sim_H1], ignore_index=True)\n",
    "test_size = 0.2\n",
    "sim_train, sim_test = train_test_split(sim, test_size=test_size, shuffle=True)\n",
    "\n",
    "# Function to convert pandas dataframe to a torch tensor\n",
    "def df_to_tensor(df, feature_cols):\n",
    "    return torch.tensor(df[feature_cols].values, dtype=torch.float32)\n",
    "\n",
    "# Convert to torch tensors\n",
    "feature_cols = ['x1', 'x2', 'mu2', 'rho12']\n",
    "X_train = df_to_tensor(sim_train, feature_cols)\n",
    "y_train = df_to_tensor(sim_train, ['label'])\n",
    "X_test = df_to_tensor(sim_test, feature_cols)\n",
    "y_test = df_to_tensor(sim_test, ['label'])\n",
    "# Create DataLoader for batching\n",
    "batch_size = 2048\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model with optional number of hidden layers and units\n",
    "class ParameterizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers=2):\n",
    "        super(ParameterizedMLP, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "# Initialize model, loss function and optimizer\n",
    "input_size = 4  # x1, x2, mu2, rho12\n",
    "hidden_size = 128\n",
    "num_hidden_layers = 3\n",
    "model = ParameterizedMLP(input_size, hidden_size, num_hidden_layers)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item() * inputs.size(0)\n",
    "    epoch_test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing loss curves\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the parametric classifier for inference\n",
    "Now that the model has been trained, we can perform the parameter estimation.\n",
    "\n",
    "This is done by evaluating the classifier output for the observed data sample $x_i$ at fixed values of the parameters of interest: $\\hat{f}(x_i|\\mu_2,\\rho_{12})$.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Using the likelihood-ratio-trick, we obtain an approximation of the conditional density ratio:\n",
    "\n",
    "$$\n",
    "\\frac{\\hat{f}(x_i|\\mu_2,\\rho_{12})}{1-\\hat{f}(x_i|\\mu_2,\\rho_{12})} \\approx \\frac{p(x_i|\\mu_2,\\rho_{12})}{p(x_i|\\mu_2=0,\\rho_{12}=0)}\n",
    "$$\n",
    "\n",
    "Just like in the hypothesis-testing procedure, we can use this to approximate the log-likelihood-ratio test statistic over the whole data set:\n",
    "\n",
    "$$\n",
    "2\\Delta\\mathrm{NLL}(\\mu_2,\\rho_{12}) \\approx -2  \\sum^{N_{obs}}_{x_i \\in \\mathcal{D}} \\ln{\\bigg(\\frac{\\hat{f}(x_i|\\mu_2,\\rho_{12})}{1-\\hat{f}(x_i|\\mu_2,\\rho_{12})}\\bigg)}\n",
    "$$\n",
    "\n",
    "where crucially the test statistic is now a function of the parameters of interest. \n",
    "\n",
    "</div>\n",
    "\n",
    "We evaluate the \"learned\" test statistic values in a grid of $(\\mu_2,\\rho_{12})$ to build up the test statistic surface. We can then use this surface to infer the best-fit parameter values (minimum) and their respective confidence intervals.\n",
    "\n",
    "The final test-statistic is shifted so that the minimum is at zero:\n",
    "\n",
    "$$\n",
    "2\\mathrm{NLL}(\\mu_2,\\rho_{12}) - 2\\mathrm{NLL}_{\\mathrm{min}}\n",
    "$$\n",
    "\n",
    "Let's again write python functions to calculate the test-statistic using the classifier output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_parameterized(X, mu2_eval, rho12_eval, model, clip=1e-10):\n",
    "    N = X.shape[0]\n",
    "    mu2_array = np.full(N, mu2_eval)\n",
    "    rho12_array = np.full(N, rho12_eval)\n",
    "    input_array = np.column_stack((X, mu2_array, rho12_array))\n",
    "    with torch.no_grad():\n",
    "        scores = model(torch.tensor(input_array, dtype=torch.float32)).numpy().flatten()\n",
    "    # Avoid log(0) by clipping scores\n",
    "    scores = np.clip(scores, clip, 1-clip)\n",
    "    llr = np.log(scores / (1 - scores))\n",
    "    return llr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we scan over the $(\\mu_2,\\rho_{12})$ plane but now calculate the learned test statistic for each point. We will then compare this to the result from the analytic solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu2_points = np.linspace(-1, 1, 100)\n",
    "rho12_points = np.linspace(-0.99, 0.99, 100)\n",
    "llr_vals_clf = np.zeros((len(mu2_points), len(rho12_points)))\n",
    "for i, mu2 in enumerate(mu2_points):\n",
    "    for j, rho12 in enumerate(rho12_points):\n",
    "        llr = log_likelihood_ratio_parameterized(data_obs[['x1', 'x2']].values, mu2, rho12, model)\n",
    "        llr_vals_clf[i, j] = np.sum(llr)\n",
    "\n",
    "# Convert llr to test statistic\n",
    "test_statistic_clf = -2 * (llr_vals_clf-np.max(llr_vals_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test-statistic heatmap from parameterized classifier next to analytic solution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "c1 = ax[0].imshow(test_statistic_analytic.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "              origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=20)\n",
    "ax[0].set_xlabel(r'$\\mu_2$')\n",
    "ax[0].set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c1, ax=ax[0], label=r'Test Statistic $t = 2\\Delta NLL$ (Analytic)')\n",
    "contours1 = ax[0].contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles='dashed')\n",
    "ax[0].clabel(contours1, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "min_idx_analytic = np.unravel_index(np.argmin(test_statistic_analytic), test_statistic_analytic.shape)\n",
    "ax[0].plot(mu2_points[min_idx_analytic[0]], rho12_points[min_idx_analytic[1]], marker='x', color='red', markersize=5, label='Best Fit', linestyle='None')\n",
    "ax[0].legend()\n",
    "\n",
    "c2 = ax[1].imshow(test_statistic_clf.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "                origin='lower', aspect='auto', cmap='viridis', vmin=0, vmax=20)\n",
    "ax[1].set_xlabel(r'$\\mu_2$')\n",
    "ax[1].set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c2, ax=ax[1], label=r'Test Statistic $t = 2\\Delta NLL$ (Classifier)')\n",
    "contours2 = ax[1].contour(mu2_points, rho12_points, test_statistic_clf.T, levels=[2.30, 5.99], colors='blue', linestyles='dashed')\n",
    "ax[1].clabel(contours2, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "min_idx_clf = np.unravel_index(np.argmin(test_statistic_clf), test_statistic_clf.shape)\n",
    "ax[1].plot(mu2_points[min_idx_clf[0]], rho12_points[min_idx_clf[1]], marker='x', color='blue', markersize=5, label='Best Fit', linestyle='None')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise how well we have done in learning the test statistic by taking the difference of the test statistic surfaces, or by comparing the contours on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot difference of test-statistic heatmaps next to plot with all contours on same axis\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,5))\n",
    "diff_test_statistic = test_statistic_clf - test_statistic_analytic\n",
    "c = ax[0].imshow(diff_test_statistic.T, extent=[mu2_points[0], mu2_points[-1], rho12_points[0], rho12_points[-1]],\n",
    "              origin='lower', aspect='auto', cmap='BrBG', vmin=-5, vmax=5)\n",
    "ax[0].set_xlabel(r'$\\mu_2$')\n",
    "ax[0].set_ylabel(r'$\\rho_{12}$')\n",
    "fig.colorbar(c, ax=ax[0], label=r'Difference in Test Statistic $t_{clf} - t_{analytic}$')\n",
    "# Add contours for reference\n",
    "contours_analytic = ax[0].contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles='dashed')\n",
    "contours_clf = ax[0].contour(mu2_points, rho12_points, test_statistic_clf.T, levels=[2.30, 5.99], colors='blue', linestyles='dotted')\n",
    "\n",
    "# Plot all contours on same axis\n",
    "ax[1].set_xlabel(r'$\\mu_2$')\n",
    "ax[1].set_ylabel(r'$\\rho_{12}$')\n",
    "contours_analytic = ax[1].contour(mu2_points, rho12_points, test_statistic_analytic.T, levels=[2.30, 5.99], colors='red', linestyles='dashed')\n",
    "ax[1].clabel(contours_analytic, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "contours_clf = ax[1].contour(mu2_points, rho12_points, test_statistic_clf.T, levels=[2.30, 5.99], colors='blue', linestyles='dotted')\n",
    "ax[1].clabel(contours_clf, inline=True, fontsize=8, fmt={2.30: '68% CL', 5.99: '95% CL'})\n",
    "ax[1].plot(mu2_points[min_idx_analytic[0]], rho12_points[min_idx_analytic[1]], marker='x', color='red', markersize=5, label='Analytic', linestyle='None')\n",
    "ax[1].plot(mu2_points[min_idx_clf[0]], rho12_points[min_idx_clf[1]], marker='x', color='blue', markersize=5, label='Classifier', linestyle='None')\n",
    "ax[1].legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have done a reasonable job at approximating the test statistic surface. This allows us to infer $\\mu_2$ and $\\rho_{12}$ from the observed data without a-priori knowing the likelihood! For reference, the true values used to generate the dataset were $\\mu^{\\mathrm{true}}_2 = -0.1$ and $\\rho^{\\mathrm{true}}_{12} = 0.3$. This is consistent with our SBI measurement within the 68% confidence level interval. \n",
    "\n",
    "That said, our SBI model is not perfect and there is plenty of room for improvement. Differences in the learned likelihood-ratio compared to the true likelihood-ratio will lead to a biased estimator, and therefore can lead to biased measurements. In a worst-case scenario, this may lead to incorrect conclusions! In Exercise 1 below, we will explore how we can improve the training procedure to provide a more accurate test-statistic prediction. Ultimately, when using SBI in real research settings, significant time is spent validating the SBI models to ensure that we do not bias our measurements.\n",
    "\n",
    "Although this example (2D Gaussian with two unknown parameters) is overkill, we have demonstrated all the steps necessary to perform parameter estimation with a ML classifier. You can imagine how useful this becomes for more complex problems when the true likelihood is not known. We can leverage the benefits of machine learning to perform inference in high-dimensions in a complex feature space, thereby squeezing every drop of information out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### 5.1 Improving the accuracy of the parameter estimation\n",
    "In the example above the \"learned\" test statistic shows some deviations from the analytic solution. This could lead to biases (or incorrect conclusions) when using the ML SBI approach. Can you improve the accuracy by altering the model training? One could consider:\n",
    "* Increasing the amount of training data\n",
    "* Increasing the granularity of the $(\\mu_2,\\rho_{12})$ grid used for the training\n",
    "* Extending to a more complex model arhcitecture\n",
    "\n",
    "Reproduce the plots at the end of this section to see how your changes affect the accuracy.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### 5.2 Amortized inference\n",
    "As before, we can use the trained parametric classifier to perform inference on new observations. Try generating data for a particular $(\\mu_2,\\rho_{12})$ set, starting with $N=20$ samples, and perform inference on this new dataset. Then generate $N=50$ samples and repeat the inference. How do the confidence intervals change? Is the \"true\" value used to generate the samples consistent with the results? How does this change for $N=100$, $N=500$ etc? Do you eventually observe a sizeable bias in the results?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "# 6. Appendix: further reading <a id='further_reading'></a>\n",
    "\n",
    "SBI is a fast-evolving field with applications across many domains in science. A nice overview of the topic can be found in the following reference:\n",
    "\n",
    "[1] K. Cranmer, J. Brehmer, G. Louppe, *The frontier of simulation-based inference*. PNAS (2020). [arXiv:1911.01429](https://arxiv.org/pdf/1911.01429)\n",
    "\n",
    "The following sections provide a bit of a deeper dive into some interesting topics. This is by no means an exhaustive list, but is a good place to get started on some cutting-edge research topics.\n",
    "\n",
    "## Validating and calibrating the classifier\n",
    "As we have seen in [Section 5](#parameter_estimation), a sub-optimal training for the classifier may lead to a biased estimator for the likelihood ratio. This could lead to the wrong conclusions for an experiment. Therefore it is crucial to perform diagnostic checks on the SBI model, to validate that the output is what we expect. We can do this using independent simulation samples, different from those used for training. Section 4 of the following paper by the ATLAS collaboration discusses some of these diagnostic checks in detail, and how we can calibrate the classifier to give good performance.\n",
    "\n",
    "[2] - ATLAS Collaboration, *An implementation of neural simulation-based inference for parameter estimation in ATLAS*, Rept. Prog. Phys 88 (2025) 6, 067801. [arXiv:2412.01600](https://arxiv.org/abs/2412.01600)\n",
    "\n",
    "## Inclusion of systematic uncertainties\n",
    "All above sections assume that our simulators are \"faithful\" i.e. they provide an accurate description of the data under some hypothesis/parameter value. In practice, there are always differences between the observed data and the synthetic data which leads to systematic uncertainties in the measurements. In the standard inference procedure, we introduce so-called nuisance parameters to account for these systematic effects. These can be incorporated into an SBI treatment by conditioning the classifiers on the nuisance parameters, in addition to the parameters of interest.\n",
    "\n",
    "This concept is discussed in Section 5 of [1]. Further background information can be found in the following papers:\n",
    "\n",
    "[3] - K. Cranmer, J. Pavez and G. Louppe, *Approximating Likelihood Ratios with Calibrated Discriminative Classifiers* (2016). [arXiv:1506.02169](https://arxiv.org/abs/1506.02169)\n",
    "\n",
    "[4] - A. Ghost, B. Nachman and D. Whiteson, *Uncertainty-aware machine learning for high energy physics*, Phys, Rev. D 104 (2021) 056026. [arXiv:21.05.08742](https://arxiv.org/abs/2105.08742)\n",
    "\n",
    "## Learning the density directly\n",
    "Using classifiers for SBI is just one approach, where we learn the likelihood-density-ratio. Modern ML techniques, including generative models, can be used to learn the probability density directly (i.e. not the ratio). One architecture, known as conditional normalising flows, map the observable space to some simple latent space (of the same dimension), conditional on the parameter values $\\theta$. The trained model can then be used simultaneously for both generating new synthetic data (generative AI) and for inference. Some references are provided below on this topic.\n",
    "\n",
    "[5] - G. Papamakarios et al, *Normalizing flows for probabilistic modeling and inference*, JMLR 22 (2021) 1-64 [Link](https://jmlr.org/papers/volume22/19-1028/19-1028.pdf)\n",
    "\n",
    "## Bayesian approach: neural posterior estimation\n",
    "This notebook presented SBI in a frequentist paradigm. Of course the idea extends to Bayesian inference! Many applications look at learning the posterior directly. The references below concern the use of SBI for Bayesian inference in cosmology and gravitational waves, respectively.\n",
    "\n",
    "[6] - N. Jeffey & B. Wandelt, *Solving high-dimensional parameter inference: marginal posterior densities & moment networks*, NeurIPS (2020) [arXiv:2011.05991](https://arxiv.org/pdf/2011.05991)\n",
    "\n",
    "[7] - M. Dax et al, *Real-time gravitational-wave science with neural posterior estimation*, PRL (2021) [arXiv:2106.12594](https://arxiv.org/abs/2106.12594)\n",
    "\n",
    "</dev>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
