{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b770c523",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b08e5",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index <a id='index'></a>\n",
    "1. [Introduction](#intro)\n",
    "1. [Tokenization and transformer data structures](#tokenization)\n",
    "1. [seq2seq problems and the introduction of attention](#attention)\n",
    "1. [What are transformers?](#transformers)\n",
    "1. [Transformers in PyTorch](#pytorch-transformers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44eb3e9",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Introduction [^](#index) <a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e4b12",
   "metadata": {},
   "source": [
    "So far, we have described different types of neural networks in terms of how they define locality:\n",
    "\n",
    "* Fully connected neural networks are global, as each neuron connects to all neurons in the next layer\n",
    "\n",
    "* Convolutional neural networks are very local, as pixels are convolved with adjacent pixels\n",
    "\n",
    "* Graph neural networks use the graph structure to define locality\n",
    "\n",
    "The figures below illustrate the different levels of locality between these types of neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1aaee",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 320px; margin: 0;\">\n",
    "<img src='nn_locality.png' width=320 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 320px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>(a)</strong>: in a standard fully-connected neural network, all nodes in one layer are connected to every node in the next layer; in other words, the model only has global connections.\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 320px; margin: 0;\">\n",
    "<img src='cnn_locality.png' width=320 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 320px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>(b)</strong>: for a convolutional neural network, each convolutional layer aggregates adjacent pixels based on the size of the kernel. The model is very local.\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 320px; margin: 0;\">\n",
    "<img src='gnn_locality.png' width=320 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 320px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>(c)</strong>: for a graph neural network, each graph layer aggregates the set of neighbouring nodes. Locality is defined when we define the graph structure, i.e. we can define it ourselves.\n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee991f9",
   "metadata": {},
   "source": [
    "Now, we will consider something that was briefly mentioned in the GNN exercises: **attention mechanisms**. To describe how the locality of these mechanisms work, we can interpret this as the model *learning its own definition of locality*. We will discuss this in more detail shortly.\n",
    "<br></br>\n",
    "\n",
    "The main parts for this notebook will be as follows:\n",
    "\n",
    "* A brief discussion of data structures relevant for transformers and the process of **tokenization**\n",
    "\n",
    "* The historical introduction of attention mechanisms for so-called **sequence to sequence** (**seq2seq**) problems and some basic principles as to how they work\n",
    "\n",
    "* One of the most influential developments in machine learning architectures, which relies on attention mechanisms: the **transformer**. This is the development that underpins many of the modern AI models, including large language models like ChatGPT, image generation models like DALL-E, and AlphaFold, a model that predicts the structure of proteins. \n",
    "\n",
    "* An overview of how we can use transformers practically, including building the architecture ourselves using PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47829678",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Tokenization and transformer data structures [^](#intro) <a id='tokenization'></a>\n",
    "\n",
    "You will likely have heard of modern ML models referring to tokens - this is how we break up a sequence and convert it to a numerical vector so we can learn how to solve sequence problems. There are often many ways that we can tokenize a given datatype. For example, for sentences, we could have the following tokenization schemes:\n",
    "\n",
    "<!--could add schematics here if there is time-->\n",
    "\n",
    "* Word-level tokenization: each individual word is a token, and the numerical vector we construct is the index of each word in our dictionary\n",
    "\n",
    "* Character-level tokenization: each individual *character* is a token, and each character is assigned a numerical value\n",
    "\n",
    "* Subword-level tokenization: each token is not necessarily a whole word or an individual character, but instead may be a small part of a word. Each subword may have specific meanings, e.g. we could see prefixes such as \"pre\" or \"post\" as tokens separate from words they may otherwise be a part of.\n",
    "\n",
    "There are of course other options for tokenization in language processing problems, but these are a few examples. See the schematic below for an illustration of these tokenization schemes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6297d",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='tokenization-schematic.png' width=800/>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:780px;display:inline-block;vertical-align:top;margin-top:10px;line-height:1.2;'>\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "*Three different tokenization schemes for the sentence \"The cat isn't black\". **(a)**: the original sentence. **(b)**: word-level tokenization. **(c)**: character-level tokenization. **(d)**: subword-level tokenization.*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48707f78",
   "metadata": {},
   "source": [
    "\n",
    "Often after tokenization, a sequence must be projected to the right number of dimensions to match the model. This is often done using linear layers, and is referred to as finding embeddings of the sequence, similar to the node, edge, and graph embeddings we discussed for GNNs.\n",
    "\n",
    "## Data structures relevant for transformers\n",
    "\n",
    "For the vast majority of our discussion of transformers we will focus on text sequence data sets, as these were the initial class of problems the architecture was proposed to solve. However, there have been successes in applying the transformer architecture to many different data types, including but not limited to:\n",
    "\n",
    "* Image datasets, a class of models called [**vision transformers**](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "* Graph datasets, often referred to as [**graph transformers**](https://arxiv.org/abs/2407.09777)\n",
    "\n",
    "* Audio sequence datasets, such as the [convolution-augmented transformer for speech recognition](https://arxiv.org/abs/2005.08100)\n",
    "\n",
    "The principles behind the transformer architecture have proved very successful generally, resulting in improved performance on many standard benchmark tasks compared to traditional architectures, and have spurred on significant developments in other architectures to incorporate similar ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa89b0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# seq2seq problems and the introduction of attention [^](#index) <a id='attention'></a>\n",
    "\n",
    "One of the first notably successful applications of attention mechanisms was for natural\n",
    "language processing (NLP) - in particular, an approach called **sequence-to-sequence** (**seq2seq**) where\n",
    "NLP problems are understood as a process from one sequence into another sequence, via some\n",
    "intermediate representation that contains all the info necessary to reconstruct the output\n",
    "sequence. We call this intermediate representation the **context vector**.\n",
    "\n",
    "In fact, we can consider this process as using an encoder to find an embedding of the input\n",
    "sequence and using a decoder to go from the embedding to the target sequence, where in this\n",
    "case the embedding is the context vector. We can consider this for an English-to-French\n",
    "translation problem: \n",
    "\n",
    "* The input sequence is a sentence in English, which is represented by a numerical vector\n",
    "  which may e.g. just be indices of words in a dictionary\n",
    "\n",
    "* The encoder converts the input sequence into the context vector\n",
    "\n",
    "* The context vector is then passed to the decoder, which decodes the context vector to a\n",
    "  different numerical vector where each value corresponds to a word in the target language,\n",
    "  in this case French\n",
    "\n",
    "This is illustrated in the schematic below.\n",
    "\n",
    "<center>\n",
    "<img src='seq2seq-schematic.png' width=1000></img>\n",
    "</center>  \n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:950px;display:inline-block;vertical-align:top;margin-top:10px;line-height:1.2;'>\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "*A schematic illustrating a seq2seq problem, for English-to-French translation. The input English phrase \"the cat is black\" is first represented as a numerical vector and then transformed into a context vector by the encoder. The context vector is passed to the decoder, which transforms it to some new numerical vector corresponding to the French translation of the original sentence, \"le chat est noir\".*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18b86e",
   "metadata": {},
   "source": [
    "<!-- How much detail on RNNs and specifically RNN encoder-decoder is needed? e.g. can talk about how hidden states in encoder just depend on input and previous hidden state, while decoder both updates a hidden state and an output *separately*,  -->\n",
    "\n",
    "Historically, both the encoder and decoder were so-called **recurrent neural networks** (**RNNs**), which are designed to handle sequential data where the previous point in the sequence is relevant for the next point in the sequence. This can include:\n",
    "\n",
    "*  NLP: sentences are sequences, as earlier words in the sentence are important for understanding the context of later words in the sentence (and indeed,\n",
    "   vice-versa); we can also see this as we read sentences as a sequence, one word after another.\n",
    "   \n",
    "* Time series data: values at previous times influence values in the future\n",
    "\n",
    "In general, an individual RNN take both the current input and the previous output as inputs to the model, e.g. to find the output at a time $t = 1$ we give the model both the input for time $t = 1$ and the output for $t = 0$. \n",
    "\n",
    "You can read more about the general RNN encoder-decoder structure in [this paper](https://arxiv.org/abs/1406.1078)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2bf19",
   "metadata": {},
   "source": [
    "However, there is a key problem with this approach, related to how much information can be conveyed by the context vector:\n",
    "\n",
    "* For any size of context vector, it must be able to capture *all* the information contained in the input sequence\n",
    "\n",
    "* If we want to handle longer input sequences, the amount of information that has to be captured by the context vector increases\n",
    "\n",
    "* The context vector length is constant regardless of the length of input sequence, so it is difficult to be able to summarise enough information for long sequences without wasting resources for short sequences\n",
    "\n",
    "* For longer sequences, RNNs have difficulty having equal weight from words earlier in the sequence than those later in the sequence, so we lose information from the start of the sentence if we have a long sentence\n",
    "\n",
    "This is referred to as the **bottleneck** problem, as the decoder only sees the context vector. \n",
    "<br></br>\n",
    "\n",
    "To put this in context, assume the average person has a vocabulary of around 20,000 words (as suggested in [this paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC4965448/#sec15)). For a sentence with 10 words, if we assume any word could be in any place in the sentence (i.e disregarding grammar) but not allowing any repeated words, we can find the number of possible sequences using the binomial coefficient:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Number of sequences} = {{20,000}\\choose 10} = 2.82 \\times 10^{36}\n",
    "\\end{equation*}\n",
    "\n",
    "Of course, the actual number of possible sentences will be less due to the restrictions from grammar, but also could have repeated wordsm which increases the number of possible sentences. We can see whatever our context vector is, it has to be able to convey any possible sentence of the given length, and capture that relationship to the output language of choice. This is a massive amount of information to convey through a single vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d9fea",
   "metadata": {},
   "source": [
    "\n",
    "In order to overcome this, we need some way to pass more information from the encoder to the decoder. What information can we get? The general RNN encoder operation goes as follows:\n",
    "\n",
    "* For the 1st step of the input sequence, find some hidden state by passing through the encoder model, like an embedding in GNNs\n",
    "\n",
    "* For the 2nd step of the input sequence, input both the 2nd step input and the 1st step hidden state to find the 2nd step hidden state\n",
    "\n",
    "* Repeat this process until the end of the sequence is reached; the final hidden state is the context vector\n",
    "\n",
    "In fact, what we can do instead of just using the context vector as inputs to the decoder (as well as previous decoder hidden states) is use the information from each step of the encoding, i.e. use all the hidden states rather than just the last one. To do this, we use **attention mechanisms**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b67866",
   "metadata": {},
   "source": [
    "## Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553dd67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When we say \"attention mechanisms\", what we actually mean is some method of determining the relative importance of different parts of the input, and then influence the model to **attend** to important parts of the input and disregard unimportant parts. In the context of machine translation, we can describe this as working out what words in the input sequence are most relevant to each word in the output sequence. \n",
    "\n",
    "How does this actually work? In general, we need these parts:\n",
    "\n",
    "* Some representation of the output we want to predict\n",
    "\n",
    "* Some representation of our input to our model\n",
    "\n",
    "* An **alignment model** that scores how well a given single input value matches a single output value\n",
    "<br></br>\n",
    "\n",
    "To understand how we will use this, let us consider the RNN encoder-decoder model. We want to calculate some new quantity we can use to improve the performance of the decoder, based on the encoder hidden states and the alignment scores between the encoder hidden states and our decoder outputs. For decoder sequence step $t$, we do the following steps:\n",
    "\n",
    "* Calculate the alignment score between the decoder hidden state $t - 1$ and all encoder hidden states\n",
    "\n",
    "* Find the softmax of the alignment scores to get attention weights for decoder sequence step $t$\n",
    "\n",
    "* Take the weighted sum of the encoder hidden states using the attention weights\n",
    "\n",
    "The output of the weighted sum is used as an input to the decoder for sequence step $t$, alongside the decoder hidden state and the target (or predicted) sequence entry from step $t - 1$. In this example, the alignment calculated is between encoder hidden states and decoder hidden states, to find what encoder hidden states are most relevant to each decoder hidden state. This is illustrated in the schematic below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6db46",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='attn-schematic.png' width=900></img>\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:900px;display:inline-block;vertical-align:top'>\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "*The previous machine translation task, now with added attention. For each decoder step $i$, the alignment scores $\\alpha_{ij}$\n",
    "between encoder states $j$ and the previous decoder hidden state $i - 1$ are found, and the attention-weighted sum of encoder hidden \n",
    "states is calculated as the attention mechanism output. The decoder output is then determined by its previous value \n",
    "and the attention mechanism output.*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ab4c6",
   "metadata": {},
   "source": [
    "What function we use to calculate the alignment scores can have a significant effect on the performance of this approach.\n",
    "A couple of the early examples included:\n",
    "\n",
    "* [Bahdanau attention](https://arxiv.org/abs/1409.0473): use a single-layer neural network, with independent learnable \n",
    "weight matrices for the encoder and decoder hidden states, a tanh activation, and another learnable weight vector to \n",
    "project to a single value\n",
    "\n",
    "* [Luong attention](https://arxiv.org/abs/1508.04025): take a weighted dot product between the encoder and decoder \n",
    "hidden states, where the weights are a learnable matrix\n",
    "\n",
    "Of course, there are many other types of attention we might consider, and we will discuss an important one later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae46fdf",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "To give another example, we will consider how we might use attention in a GNN; you can read about this in more detail in\n",
    "the [Graph Attention Networks paper](https://arxiv.org/abs/1710.10903).\n",
    "\n",
    "You briefly saw this concept last time, when we used the `GATConv` layer to try and improve the performance of our GNN on the Cora dataset.\n",
    "In essence, this approach includes attention in the neighbourhood aggregation:\n",
    "\n",
    "* Rather than using a simple aggregation procedure, we can instead take some weighted sum (or other weighted aggregation method) of the neighbours\n",
    "\n",
    "* Weights are derived according to some attention mechanism between the node of interest and each of its neighbours\n",
    "\n",
    "* Effectively, we learn which of the neighbours are most important for prediction at the target node\n",
    "\n",
    "In the case of this model, to find the alignment score between two nodes the two node embeddings are transformed by a \n",
    "single weight matrix (as is common in other GNN layers), concatenated, and passed through a single-layer neural network\n",
    "with a LeakyReLU activation function that maps the concatenated vector to a single value. Attention weights are then \n",
    "computed as the softmax of these alignment scores over all nodes in the neighbourhood.\n",
    "\n",
    "This is illustrated in the schematic below. \n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 350px; margin: 0;\">\n",
    "<img src='gat-attn-mech.png' width=239 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 350px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Left</strong>: the attention mechanism between a node $i$ and the node $j$, which is a node in the\n",
    "neighbourhood of $i$. Node features are transformed according to the same weight matrix, aggregated and projected\n",
    "by a learnable vector $\\mathbf{a}$. A LeakyReLU activation is applied, and then the softmax over all\n",
    "nodes in the neighbourhood is found to get the attention weights.\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 504px; margin: 0;\">\n",
    "<img src='gat-node-pred.png' width=480 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 504px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Right</strong>: finding the next layer node embedding using an attention mechanism. Attention weights $\\alpha_{ij}$ are calculated between the node of interest $i$ and each of its neighbours $j$ (as well as itself). All the weighted node embeddings are then aggregated to produce the next embedding for the node of interest. \n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "*Schematics illustrating a graph attention mechanism.  Adapted from [[source](https://arxiv.org/abs/1710.10903)].*\n",
    "</center>\n",
    "\n",
    "In fact, the original paper shows that this change to a GNN architecture leads to a significant improvement in \n",
    "performance on both transductive and inductive benchmark tasks relative to previous state-of-the-art GNN models, including:\n",
    "\n",
    "* State-of-the-art performance or better for the three major publication network benchmark datasets: Cora, Citeseer, and Pubmed\n",
    "\n",
    "* A 20% improvement in performance for predicting protein-protein interactions based on graph representations of proteins\n",
    "\n",
    "For more details on this, please see the [corresponding paper](https://arxiv.org/abs/1710.10903)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3b8ef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have discussed the attention mechanisms and their historical introduction, including:\n",
    "\n",
    "* seq2seq problems including machine translation, and the difficulty of traditional RNN encoder-decoder methods\n",
    "\n",
    "* The introduction of attention to machine translation\n",
    "\n",
    "* Graph attention networks and multi-head attention\n",
    "\n",
    "In the next section, we will use our newfound understanding of attention mechanisms to discuss one of the most influential developments in machine learning in recent years: **transformers**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61819dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# What are transformers? <a id='transformers'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fd4026",
   "metadata": {},
   "source": [
    "First proposed in the 2017 paper [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al., \n",
    "the transformer architecture (and its components) is a greatly influential model that completely changed the approach \n",
    "to sequence-based ML tasks, and indeed has found great success in many different applications. \n",
    "\n",
    "As opposed to the complex RNN (or sometimes CNN) models often used for seq2seq tasks, which are often supplemented by \n",
    "attention mechanisms, the transformer architecture instead relies solely on attention mechanisms combined with regular linear layers, rather than any recurrence or convolutions. We will first introduce some of the language used in the original paper and then we will discuss the design of the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd22c1",
   "metadata": {},
   "source": [
    "## Queries, keys, and values\n",
    "\n",
    "As a way to describe attention mechanisms, the terms queries, keys, and values were borrowed from database terminology and in fact were popularised to describe attention mechanisms by the original transformer paper. We can break this down as follows:\n",
    "\n",
    "* Our training dataset consists of key-value pairs, e.g. a list of words with a value assigned to each word\n",
    "\n",
    "* When we predict, we pass a query to the model to get a value\n",
    "\n",
    "* For a given query, an attention mechanism returns a weighted combination of values, based on how well their corresponding keys match the query\n",
    "<br></br>\n",
    "\n",
    "In other words, we find the **alignment score** between the query and our set of keys, and return a weighted sum of the values weighted by the softmax of the **alignment scores**. This is of course the same type of mechanism we discussed in the previous section; we can think of what each of these are in our previous examples:\n",
    "<br></br>\n",
    "\n",
    "* RNN encoder-decoder with attention:\n",
    "    * Find the alignment between the encoder hidden states and the decoder hidden states\n",
    "\n",
    "    * Get a weighted sum of our encoder hidden states\n",
    "\n",
    "    * Both the keys and the values are the encoder hidden states, and the decoder hidden states are the queries\n",
    "\n",
    "* Graph attention networks: \n",
    "    * The weight vector $\\mathbf{a}$ and the LeakyReLU function returns the alignment scores between the projected embeddings for node of interest and each of the neighbouring node (and itself)\n",
    "\n",
    "    * The weighted sum is of the projected node embeddings for each neighbouring node \n",
    "\n",
    "    * Both the keys and values are the projected node embeddings for neighbouring (and self) nodes, and the query is the embedding for the node of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc290b6",
   "metadata": {},
   "source": [
    "To illustrate these in a more practical sense, consider searching for a paper in a database:\n",
    "\n",
    "* We provide some search term that describes what we are looking for, which is the query\n",
    "\n",
    "* The query is compared to the titles and metadata of the papers in the database, which are the keys\n",
    "\n",
    "* The papers corresponding to the keys with the best match to the query (the highest attention weights) are returned, ranked by the attention weights; the papers are the values\n",
    "\n",
    "While in this example the queries, keys, and values are different, they do not have to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8884bcd",
   "metadata": {},
   "source": [
    "## The transformer architecture\n",
    "\n",
    "The figure below shows the transformer architecture as illustrated in the original paper:\n",
    "\n",
    "<center>\n",
    "<img src=\"transformer-architecture.png\" width=600></img>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 580px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*The architecture of the original transformer model, taken from the [original paper]((https://arxiv.org/abs/1706.03762)).*\n",
    "</div></div></div>\n",
    "\n",
    "Currently this may seem a bit incomprehensible, but we will break each part down in turn and then see how it all fits together.\n",
    "\n",
    "While a transformer is built of a encoder and a decoder like previous RNN models, there are three key things that set a transformer apart in compared to earlier seq2seq models (apart from the use of linear layers in place of recurrent or convolutional layers). These include:\n",
    "\n",
    "* The choice of attention function: **scaled dot-product attention**\n",
    "\n",
    "* **Multi-head attention**\n",
    "\n",
    "* **Self attention**\n",
    "\n",
    "It is the combination of these things, and where they are used in the model, that enabled the jump in performance seen using this model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707e380",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "While we briefly mentioned two attention functions earlier (Bahdanau and Luong), the attention mechanism used in transformers is generally established as very computationally efficient and performs similarly to the best attention mechanisms in the literature. This is defined as follows:\n",
    "\n",
    "* Pack a set of queries into a single matrix $Q$\n",
    "\n",
    "* Similarly pack the keys and values into matrices $K$ and $V$\n",
    "\n",
    "* We denote the dimension of the queries and keys as $d_k$, and the dimension of the values as $d_v$\n",
    "\n",
    "* The attention output is then given according to\n",
    "\n",
    "$$\\text{Attention(}Q, K, V\\text{)} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "The normalising factor $\\frac{1}{\\sqrt{d_k}}$ is important as when we have large $d_k$ the dot product can reach very large values, and thus push the softmax output into regions with very small gradients. This can subsequently cause problems with training (similar to the exploding and vanishing gradient problems we have discussed before). This is discussed in a little more detail in the [original paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "In fact, this attention function is very similar to the Luong attention mechanism, but with the extra normalising factor and without an explicit weight matrix in the dot product. This weight projection is done before inputs into scaled dot-product attention, rather than being included in the attentoin function itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea3536",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-head attention\n",
    "\n",
    "So far, we have considered a single attention mechanism in a model. However, what would happen if we had multiple, and how would we go about it, and is this even useful? \n",
    "\n",
    "In fact, the original transformers paper proposed the idea of **multi-head attention**, a way of using multiple attention functions in parallel to incorporate information from different representations simultaneously. In the original paper, this goes as follows:\n",
    "\n",
    "* Typical attention mechanisms would use a single attention function for keys, values and queries with $d_\\text{model}$ dimensions\n",
    "\n",
    "* Instead, do $h$ independent linear projections of the queries, keys, and values, to $d_k$, $d_k$, and $d_v$ dimensions respectively - we refer to each of these sets of projections as an **attention head**\n",
    "\n",
    "* For each attention head, apply the attention function in parallel to produce a $d_v$-dimensional output for each head\n",
    "\n",
    "* Concatenate the outputs from all attention heads and finally projected them to the desired number of dimensions $d_\\text{model}$\n",
    "\n",
    "We can then learn the parameters of the linear projections for each attention head, allowing us to use not just one representation of the keys, queries, and values, but as many as we may want to. This allows us to incorporate more information than we might otherwise, including different ways of looking at the information. \n",
    "\n",
    "In an NLP task, this could be thought of as considering different possible meanings of a single word and then seeing how important other words are to understanding the sentence for each possible meaning of that word. \n",
    "\n",
    "**Note**: while the operation of these linear projections is the same as a linear layer in a neural network, they are explicitly *not* followed by an activation function and instead pass into the attention mechanism. However, the attention mechanism is also a nonlinearity, so we can see this a bit like an activation function here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdefb4f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "We have seen the term \"heads\" before in machine learning, specifically when we talked about graph neural networks; we had different prediction heads for different graph tasks.\n",
    "\n",
    "In fact, in general a head in machine learning refers to applying a different set of operations to the same base data, to reach different outputs. For example:\n",
    "\n",
    "* In graph neural networks, if we wanted e.g. to classify nodes an edges simultaneously:\n",
    "\n",
    "    * We would have a main GNN that finds node embeddings\n",
    "\n",
    "    * We can then feed the node embeddings from the main GNN into two different sets of transformations to get the node and edge classifications\n",
    "\n",
    "    * This is having multiple prediction heads\n",
    "<br></br>\n",
    "* In multi-head attention:\n",
    "\n",
    "    * Each head is a different linear projection of the input queries, keys, and values\n",
    "\n",
    "    * The output of each head learns a different representation of the queries, keys, and values, allowing us to learn different relations in our data simultaneously\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ddd79b",
   "metadata": {},
   "source": [
    "The multi-head attention mechanism used in the original transformers paper is illustrated in the figure below.\n",
    "\n",
    "<center>\n",
    "<img src='mha-schematic.png' width=600></img>\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 700px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*The structure of multi-head attention used in \"Attention is All You Need\". For a number of heads $h$, the input queries, keys, and values are projected $h$ times and the scaled dot-product attention is calculated for each projection. The attention outputs are then concatenated and projected once more to the desired dimensionality, producing the multi-head attention output. Adapted from the [original paper](https://arxiv.org/abs/1706.03762).*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63c9d5",
   "metadata": {},
   "source": [
    "While in principle we use different dimensions for the queries/keys and the values, in practice generally we will take $d_k = d_v$ i.e. set them equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfc56ba",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "If we want to write out a mathematical expression for multi-head attention, it goes as follows:\n",
    "\n",
    "$$\\text{MultiHead}(Q,\\,K,\\,V) = \\text{Concat}(\\text{head}_1,\\,\\dots,\\,\\text{head}_h)\\,\\mathbf{W}^O,$$\n",
    "$$\\text{where head}_i = \\text{Attention}\\left(\\mathbf{W}^Q_i\\,Q, \\,\\, \\mathbf{W}^K_i\\,K, \\,\\, \\mathbf{W}^V_i\\,V\\right).$$\n",
    "\n",
    "Individual symbols are defined as follows:\n",
    "\n",
    "* $\\mathbf{W}^Q_i$ denotes the queries linear projection parameter matrix for attention head $i$, which is a $d_\\text{model} \\times d_k$ matrix\n",
    "\n",
    "* $\\mathbf{W}^K_i$ denotes the keys linear projection parameter matrix for attention head $i$, which is a $d_\\text{model} \\times d_k$ matrix\n",
    "\n",
    "* $\\mathbf{W}^V_i$ denotes the values linear projection parameter matrix for attention head $i$, which is a $d_\\text{model} \\times d_v$ matrix\n",
    "\n",
    "* $\\text{Attention}$ denotes an arbitrary attention function of queries, keys, and values; in the case of transformers, this is the scaled dot-product attention described before\n",
    "\n",
    "* $\\text{Concat}$ denotes a concatenation into a single vector\n",
    "\n",
    "* $\\mathbf{W}^O$ denotes the linear projection matrix from the concatenated individual attention head outputs to the final model dimension, which is a $h d_v \\times d_\\text{model}$ matrix\n",
    "\n",
    "Because each attention head is independent, any calculations across different heads can be parallelised. This can help greatly with the computational cost of training multi-head attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40898c1b",
   "metadata": {},
   "source": [
    "## Self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80da2a",
   "metadata": {},
   "source": [
    "So far, when we have discussed attention between the output sequence and the input sequence. However, there is no reason why we couldn't find attention between any two things - in fact, we can consider so-called **self attention**, where we find attention weights between each element in the input sequence and all other elements in the sequence. \n",
    "\n",
    "This way, we can find what words in the input sequence are most important to understanding each word in the same sequence. \n",
    "\n",
    "This is illustrated in the schematic below.\n",
    "\n",
    "<center>\n",
    "<img src='self-attn-schematic.png' width=800>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 700px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*Schematic illustrating self-attention, for a single query. The input sequence acts as the queries, keys, and values to calculate the output.*\n",
    "</div></div></div>\n",
    "\n",
    "In fact, we have seen self-attention already in the Graph Attention Networks example - each node attends to all of the nodes in its neighbourhood, and itself, to learn which nodes are most important for finding an embedding for a given node. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92666e6",
   "metadata": {},
   "source": [
    "One important thing to note is that we aren't just passing the same sequence in as the keys, queries, and values, but instead **different** linear projections of the same sequence. The most important keys for the queries are found through the attention function, and the weighted sum of the corresponding values is found. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e2340",
   "metadata": {},
   "source": [
    "While similar ideas had been introduced previously for RNN models, it was the transformers paper that introduced the highly-parallelisable self-attention that has been such a success for modern models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87849227",
   "metadata": {},
   "source": [
    "## Applications of attention in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cbabe",
   "metadata": {},
   "source": [
    "The transformer architecture uses attention in several ways. These are highlighted in the transformer architecture diagram below:\n",
    "\n",
    "<center>\n",
    "<img src='transformer-mha-highlight.png' width=800/>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 700px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*The three applications of attention mechanisms in the transformer architecture. **(a)**: encoder self-attention, **(b)**: encoder-decoder attention (sometimes called **cross-attention**), **(c)**: decoder (masked) self-attention*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed6678",
   "metadata": {},
   "source": [
    "\n",
    "These are described as follows:\n",
    "\n",
    "* **Encoder self-attention**: each position in an encoder layer output sequence attends to all positions in the previous encoder output sequence (for the first encoder layer, this is in the input sequence)\n",
    "\n",
    "* **Encoder-decoder attention**: just like in the RNN attention models, allow the decoder output to attend to all positions in the encoder output sequence; this means that the decoder attends over the whole encoded input sequence. In the context of transformers, this is sometimes referred to as **cross-attention**\n",
    "\n",
    "* **Decoder self-attention**: same principle as for the encoder, but an additional constraint is needed to ensure an element in the output sequence is only determined by the previous element in the sequence, not later ones. This is done by masking keys that are not allowed for a given query, i.e. masking out elements that are not earlier in the sequence than the query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb550be",
   "metadata": {},
   "source": [
    "## Layer normalisation\n",
    "\n",
    "<center>\n",
    "<img src='transformer-layer-norm-highlight.png' width=400/>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 450px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*Layer normalisation in the transformer architecture. After each sub-layer of the encoder or decoder, the outputs are summed with the inputs and passed through a layer normalisation to standardise the distribution of features across the layer.*\n",
    "</div></div></div>\n",
    "\n",
    "<!--may need to make more bullet point-y-->\n",
    "\n",
    "Previously, we have discussed how the weights in one layer of a neural network are strongly dependent on the outputs of the neurons in the previous layer, and how we can handle this dependency with methods such as **batch normalisation**. \n",
    "\n",
    "The transformer architecture uses a related but different normalisation approach, called **layer normalisation**. Rather than normalising activations across the batch, this approach normalises activations across the layer, i.e. based on the statistics *within the layer*. To compare:\n",
    "\n",
    "* Batch normalisation: computes correction factors common to all samples in the branch but different for each hidden neuron\n",
    "\n",
    "* Layer normalisation: finds corrections common to each hidden neuron but different for each sample\n",
    "<br></br>\n",
    "\n",
    "As a result, layer normalisation can be applied regardless of batch size. This has some particular benefits for sequence problems:\n",
    "\n",
    "* We generally want to consider each collection of tokens separately from other ones, so we can work e.g. one sentence at a time\n",
    "\n",
    "* Because batch normalisation is over multiple sequences, this normalises over different sequences and separately for each token, so results in issues for test sequences longer than the training sequences\n",
    "\n",
    "* Layer normalisation normalises over all dimensions apart from the batch dimension, so works the same irrespective of sequence length\n",
    "<br></br>\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "To express layer normalisation mathematically, start by defining the following:\n",
    "\n",
    "* $a^l$: the input to the $l$-th hidden layer of a deep feed-forward neural network, referred to as the **activation** of the previous layer\n",
    "* $H$: the number of hidden units in a layer\n",
    "* $\\gamma^l$ and $\\beta^l$: learnable parameters for layer $l$, with the same dimensions as the desired output shape\n",
    "\n",
    "Now we can define the mean and standard deviation in hidden layer $l$ according to\n",
    "\n",
    "$$\\mu^l = \\frac{1}{H}\\sum_{i = 1}^H a^l_i,\\qquad\\quad \\sigma^l = \\sqrt{\\frac{1}{H}\\sum_{i = 1}^H(a^l_i - \\mu^l)^2},$$\n",
    "\n",
    "where $a^l_i$ denotes the $i$-th element of $a^l$, and $\\mu^l$ and $\\sigma^l$ denote the layer mean and standard deviation respectively.\n",
    "\n",
    "Finally, we can write the layer normalisation output according to\n",
    "\n",
    "$$\\text{LayerNorm}(a^l) = \\bar{a}^l = \\frac{a^l - \\mu^l}{\\sigma^l}\\cdot\\gamma^l + \\beta^l$$\n",
    "\n",
    "Note that all multiplication is element-wise, such that $\\bar{a}^l_i = \\frac{a^l_i - \\mu^l}{\\sigma^l} \\cdot \\gamma^l_i + \\beta^l_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99045e8a",
   "metadata": {},
   "source": [
    "## Transformer encoder and decoder\n",
    "\n",
    "Now we will consider how the encoder and decoder are constructed in the transformer architecture:\n",
    "\n",
    "<center>\n",
    "<img src='transformer-enc-dec.png' width=400/>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 400px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*The transformer encoder and decoder structure, from the original architecture diagram.*\n",
    "</div></div></div>\n",
    "\n",
    "Both the transformer encoder and decoder are built of a set of $N = 6$ identical sub-layers, with slightly different sub-layers between the encoder and decoder. These are structured as follows:\n",
    "\n",
    "**Encoder layer**: \n",
    "    \n",
    "* Contains two sub-layers:\n",
    "\n",
    "    1. multi-head self-attention\n",
    "\n",
    "    1. two-layer feed forward (i.e. non-recurrent) neural network with a ReLU activation function, with hidden dimension $d_{ff}$\n",
    "<br></br>\n",
    "* Sums output of each sub-layer with the input to that sub-layer i.e. includes a **residual connection** (like skip connections we saw in GNNs)\n",
    "<br></br>\n",
    "* Passes sum of input and sub-layer output to layer normalisation, such that the final sub-layer output is given as $\\text{LayerNorm}(x + \\text{Sublayer(x)})$, where $\\text{Sublayer}(x)$ denotes the function of the sub-layer itself\n",
    "\n",
    "<center>\n",
    "<img src='enc-layer-schematic.png' height=230>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 700px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*Schematic of a single transformer encoder layer. This is built of two sub-layers: multi-head self-attention and a feed-forward neural network. Residual connections are included around each sub-layer and layer normalisation is applied after each sub-layer. Adapted from the [original paper](https://arxiv.org/abs/1706.03762).*\n",
    "</div></div></div>\n",
    "<!-- encoder layer schematic -->\n",
    "\n",
    "**Decoder layer**:\n",
    "\n",
    "* Contains three sub-layers, two similar to the encoder layers:\n",
    "\n",
    "    1. masked multi-head self-attention, to prevent earlier sequence entries attending to later sequence entries\n",
    "\n",
    "    1. multi-head attention over encoder output, i.e. the encoder output as the keys and values and decoder self-attention output as the queries\n",
    "\n",
    "    1. two-layer feed forward neural network with a ReLU activation function, with hidden dimension $d_{ff}$\n",
    "<br></br>\n",
    "* Like the encoder layers, uses residual connections around each sub-layer followed by layer normalisation\n",
    "<br></br>\n",
    "<center>\n",
    "<img src='dec-layer-schematic.png' height=250>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 850px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*Schematic of a single transformer decoder layer. This is built of three sub-layers: masked multi-head self-attention, multi-head attention over the encoder outputs, and a feed-forward neural newtork.  Residual connections are included around each sub-layer and layer normalisation is applied after each sub-layer. Adapted from the [original paper](https://arxiv.org/abs/1706.03762).*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f0455",
   "metadata": {},
   "source": [
    "For both encoder and decoder layers, the hidden dimension $d_{ff}$ for the feed-forward network is equal to 2048."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4872da",
   "metadata": {},
   "source": [
    "Later in the original paper, they also note that they include a **dropout** layer after each sub-layer, with a probability $p = 0.1$, before the residual connection and layer normalisation. This is to regularise the training and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085323d6",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94b6ca",
   "metadata": {},
   "source": [
    "Because the transformer layers are solely linear, the model will lose information about the order of the sequence unless we do something about it, i.e. manually add some information about relative or absolute positions of tokens in the sequence. This is done with **positional encodings**. \n",
    "\n",
    "<center>\n",
    "<img src='transformer-pos-enc-highlight.png' width=480/>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 460px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*Positional encoding in the transformer architecture. These are added to input sequence to preserve information about the position of each token in the sequence, which are otherwise lost with no recurrent layers.*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca3773",
   "metadata": {},
   "source": [
    "Applied to the embeddings inputted to the encoder and decoder, the positional encoding has the same dimension as the embeddings so they can be summed. \n",
    "\n",
    "For the transformer model, each dimension of the positional encoding is a $\\sin$ or $\\cos$ of the token position, where the wavelength of the sinusoid is determined by the index of the dimension and the total number of dimensions. Because this is periodic, the model can learn relative positions of tokens in the sequence.\n",
    "\n",
    "We can think of this as adding back in explicit locality; while our sequence data has a defined order, because all the actual layers in the transformer architecture are based on linear layers we lose the information about order from the data. Adding positional encoding effectively adds locality back into our data, so we know what tokens are close to what other tokens.\n",
    "\n",
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "\n",
    "Explicitly, the positional encodings from the original paper are given as\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{PE}_{(\\text{pos},\\,i)} &= \\sin\\left(\\text{pos}/10000^{i/d_{\\text{model}}}\\right)\\qquad &i \\text{ even}\\\\\n",
    "\\text{PE}_{(\\text{pos},\\,i)} &= \\cos\\left(\\text{pos}/10000^{(i-1)/d_{\\text{model}}}\\right)\\qquad &i \\text{ odd}\n",
    "\\end{align*}\n",
    "where $\\text{pos}$ denotes the position of the token, $i$ denotes the dimension, and $d_{\\text{model}}$ is the number of dimensions of the embeddings.\n",
    "\n",
    "The reason for proposing this positional embedding is that $\\text{PE}_{\\text{pos} + k}$ is a linear function of $\\text{PE}_{\\text{pos}}$ (for given $i$, $\\text{PE}_{\\text{pos}+k,\\,i}$ is a linear combination of $\\text{PE}_{\\text{pos},\\,i}$ and $\\text{PE}_{\\text{pos},\\,i+1}$, and the same for $\\text{PE}_{\\text{pos}+k,\\,i+1}$). This can allow the model to learn relative positions of tokens in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642eeaad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b08ea",
   "metadata": {},
   "source": [
    "Finally, we can look at the architecture diagram from the original paper:\n",
    "\n",
    "<center>\n",
    "<img src=\"transformer-architecture.png\" width=600></img>\n",
    "</center>\n",
    "<div style=\"text-align:center\">\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 580px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "*The architecture of the original transformer model, taken from the [original paper]((https://arxiv.org/abs/1706.03762)).*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375fce8",
   "metadata": {},
   "source": [
    "Let's break down how each part of this model works. Starting with the encoder:\n",
    "\n",
    "<div style=\"display: flex; justify-content: flex-start; gap: 80px; align-items: flex-start;\">\n",
    "\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 700px; margin-left: 0px;\">\n",
    "<div style=\"margin-top: 0px; text-align: justify; max-width: 700px; line-height: 1.2;\">\n",
    "\n",
    "* The input sequence is converted to a numerical vector and passed through learned embeddings to project to $d_\\text{model}$ dimensions\n",
    "\n",
    "* Position encodings are added to the input embeddings\n",
    "\n",
    "* The embeddings are passed through $N = 6$ encoder layers to produce the encoder outputs, including multi-head self-attention, layer normalisation, and a feedforward neural network\n",
    "\n",
    "Note that unlike in RNNs, the entire input sequence is processed in one go.\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 200px; margin-top: -60px;\">\n",
    "<img src=\"transformer-enc-zoom.png\" width=200/>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<!-- <center>\n",
    "<img src='transformer-enc-zoom.png' width = 300/>\n",
    "</center> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed6cb7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "For the decoder:\n",
    "\n",
    "<div style=\"display: flex; justify-content: flex-center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: center; width: 700px; margin-left: 0px;\">\n",
    "<div style=\"margin-top: 0px; text-align: justify; max-width: 700px; line-height: 1.2;\">\n",
    "\n",
    "* The output sequence (more on this in a moment) is converted to a numerical vector and passed through learned embeddings to project to $d_\\text{model}$ dimensions\n",
    "\n",
    "* Position encodings are added to these embeddings\n",
    "\n",
    "* Output sequence embeddings are passed through $N = 6$ decoder layers which contain the following operations, all followed by adding the residuals and applying layer normalisation:\n",
    "\n",
    "    * Masked multi-head self-attention, so each sequence element only attends to earlier sequence elements\n",
    "\n",
    "    * Multi-head attention over the encoder outputs as keys and values, and decoder embeddings as the queries (i.e. the cross-attention)\n",
    "\n",
    "    * Feed-forward neural network, with the same weights as the encoder\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: center; width: 200px; margin-top: -60px;\">\n",
    "<img src=\"transformer-dec-zoom.png\" width=200/>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "After $N$ decoder layers, the output is passed through a final learned linear transformation and a softmax layer to produce weights for the next token, which are often interpreted as probabilities (although they are not truly probabilities). \n",
    "\n",
    "<center>\n",
    "<img src='transformer-output-zoom.png' width = 200/>\n",
    "</center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "**Note**: the operation of the decoder is slightly different during training and prediction, as follows:\n",
    "\n",
    "* During training, the output sequence used as the first decoder input is the true output, with the so-called **start-of-sequence token** (sometimes referred to as `<SOS>`) prepended i.e. at the start of the sequence\n",
    "\n",
    "    * The whole target sequence is passed through the decoder and the loss is computed between the prediction and the true sequence, often with loss functions like cross entropy\n",
    "\n",
    "* During prediction, the output sequence first inputted is an empty sequence apart from a start-of-sequence token at the start\n",
    "\n",
    "    * The initial output sequence is passed through the decoder, and the token with the maximum value in the softmax output is assigned to be the next sequence entry $\\hat{y}_1$\n",
    "\n",
    "    * The updated sequence with the start-of-sequence token and our first predicted entry $\\hat{y}_1$ is passed into the decoder to predicted the next entry $\\hat{y}_2$\n",
    "\n",
    "    * This procedure is repeated until predictions have been made for every entry in the output sequence\n",
    "<br></br>\n",
    "\n",
    "In training, the whole target sequence is used at once so any mistakes made by the model in early sequence entries do not cause errors in later sequence entries. In contrast, during prediction because we don't know what the output sequence should be we must feed the output back in as input to predict later entries in the output sequence.\n",
    "\n",
    "<!--Could ideally use some schematic here? Maybe illustrating the building of the output sequence in training vs prediction-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152a324",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have covered the transformer architecture, including:\n",
    "\n",
    "* design of the encoder and decoder, and the applications of attention\n",
    "\n",
    "* layer normalisation\n",
    "\n",
    "* positional encodings\n",
    "\n",
    "* operation of the model during training and prediction\n",
    "\n",
    "In the next section, we will discuss more practical elements of transformers including how to implement them in PyTorch, and considerations that are necessary during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfaa265",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Transformers in PyTorch <a id='pytorch-transformers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b605cd",
   "metadata": {},
   "source": [
    "Because all the individual layers with weights in a transformer are just based on linear operations, we don't need any fancy other library to implement these, and instead can just use `torch`. \n",
    "\n",
    "PyTorch does have implementations of the original transformer architecture (and its components) available in `torch.nn`, including but not limited to:\n",
    "\n",
    "* `Transformer`\n",
    "\n",
    "* `TransformerEncoderLayer`\n",
    "\n",
    "* `TransformerDecoderLayer`\n",
    "\n",
    "* `MultiheadAttention`\n",
    "\n",
    "However, the package maintainers specifically recommend not using these and instead implementing things yourself, through a new set of more flexible tools. You can read [this article](https://docs.pytorch.org/tutorials/intermediate/transformer_building_blocks.html) for more details on this, but we will **not** use the pre-defined transformer layers/architecture from `torch.nn`.\n",
    "\n",
    "**Note**: everything we use to build the transformer will be from PyTorch, so all our data structures are Tensors that can store gradients. This is necessary so our model can train and propagate gradients back through, so we will **not** use any `numpy` functions (or anything else that isn't PyTorch) to ensure our model can train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0aed99",
   "metadata": {},
   "source": [
    "However, PyTorch does helpfully provide a nice efficient implementation of the attention mechanism from the original paper, as `scaled_dot_product_attention`. Let's explore this function in some detail, and then we'll try implementing some of the components of the transformer architecture ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871871b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We can find `scaled_dot_product_attention` in `torch.nn.functional`. First, let's do our starting imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c6569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e919dc8",
   "metadata": {},
   "source": [
    "Now we can look at the function signature for `scaled_dot_product_attention`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4cf27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function scaled_dot_product_attention in module torch._C._nn:\n",
      "\n",
      "scaled_dot_product_attention(...)\n",
      "    scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
      "        is_causal=False, scale=None, enable_gqa=False) -> Tensor:\n",
      "    \n",
      "    Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed,\n",
      "    and applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be\n",
      "    specified as a keyword argument.\n",
      "    \n",
      "    .. code-block:: python\n",
      "    \n",
      "        # Efficient implementation equivalent to the following:\n",
      "        def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
      "                is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
      "            L, S = query.size(-2), key.size(-2)\n",
      "            scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
      "            attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
      "            if is_causal:\n",
      "                assert attn_mask is None\n",
      "                temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
      "                attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
      "                attn_bias.to(query.dtype)\n",
      "    \n",
      "            if attn_mask is not None:\n",
      "                if attn_mask.dtype == torch.bool:\n",
      "                    attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
      "                else:\n",
      "                    attn_bias = attn_mask + attn_bias\n",
      "    \n",
      "            if enable_gqa:\n",
      "                key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
      "                value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
      "    \n",
      "            attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
      "            attn_weight += attn_bias\n",
      "            attn_weight = torch.softmax(attn_weight, dim=-1)\n",
      "            attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
      "            return attn_weight @ value\n",
      "    \n",
      "    .. warning::\n",
      "        This function is beta and subject to change.\n",
      "    \n",
      "    .. warning::\n",
      "        This function always applies dropout according to the specified ``dropout_p`` argument.\n",
      "        To disable dropout during evaluation, be sure to pass a value of ``0.0`` when the module\n",
      "        that makes the function call is not in training mode.\n",
      "    \n",
      "        For example:\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            class MyModel(nn.Module):\n",
      "                def __init__(self, p=0.5):\n",
      "                    super().__init__()\n",
      "                    self.p = p\n",
      "    \n",
      "                def forward(self, ...):\n",
      "                    return F.scaled_dot_product_attention(...,\n",
      "                        dropout_p=(self.p if self.training else 0.0))\n",
      "    \n",
      "    Note:\n",
      "    \n",
      "        There are currently three supported implementations of scaled dot product attention:\n",
      "    \n",
      "            - `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning`_\n",
      "            - `Memory-Efficient Attention`_\n",
      "            - A PyTorch implementation defined in C++ matching the above formulation\n",
      "    \n",
      "        The function may call optimized kernels for improved performance when using the CUDA backend.\n",
      "        For all other backends, the PyTorch implementation will be used.\n",
      "    \n",
      "        All implementations are enabled by default. Scaled dot product attention attempts to automatically select the\n",
      "        most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\n",
      "        is used, the following functions are provided for enabling and disabling implementations.\n",
      "        The context manager is the preferred mechanism:\n",
      "    \n",
      "            - :func:`torch.nn.attention.sdpa_kernel`: A context manager used to enable or disable any of the implementations.\n",
      "            - :func:`torch.backends.cuda.enable_flash_sdp`: Globally enables or disables FlashAttention.\n",
      "            - :func:`torch.backends.cuda.enable_mem_efficient_sdp`: Globally enables or disables  Memory-Efficient Attention.\n",
      "            - :func:`torch.backends.cuda.enable_math_sdp`: Globally enables or disables  the PyTorch C++ implementation.\n",
      "    \n",
      "        Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\n",
      "        disable the PyTorch C++ implementation using :func:`torch.nn.attention.sdpa_kernel`.\n",
      "        In the event that a fused implementation is not available, a warning will be raised with the\n",
      "        reasons why the fused implementation cannot run.\n",
      "    \n",
      "        Due to the nature of fusing floating point operations, the output of this function may be different\n",
      "        depending on what backend kernel is chosen.\n",
      "        The c++ implementation supports torch.float64 and can be used when higher precision is required.\n",
      "        For math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\n",
      "    For more information please see :doc:`/notes/numerical_accuracy`\n",
      "    \n",
      "        Grouped Query Attention (GQA) is an experimental feature. It currently works only for Flash_attention\n",
      "        and math kernel on CUDA tensor, and does not support Nested tensor.\n",
      "        Constraints for GQA:\n",
      "    \n",
      "            - number_of_heads_query % number_of_heads_key_value == 0 and,\n",
      "            - number_of_heads_key == number_of_heads_value\n",
      "    \n",
      "    Note:\n",
      "    \n",
      "        In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "    \n",
      "    Args:\n",
      "        query (Tensor): Query tensor; shape :math:`(N, ..., Hq, L, E)`.\n",
      "        key (Tensor): Key tensor; shape :math:`(N, ..., H, S, E)`.\n",
      "        value (Tensor): Value tensor; shape :math:`(N, ..., H, S, Ev)`.\n",
      "        attn_mask (optional Tensor): Attention mask; shape must be broadcastable to the shape of attention weights,\n",
      "            which is :math:`(N,..., L, S)`. Two types of masks are supported.\n",
      "            A boolean mask where a value of True indicates that the element *should* take part in attention.\n",
      "            A float mask of the same type as query, key, value that is added to the attention score.\n",
      "        dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied\n",
      "        is_causal (bool): If set to true, the attention masking is a lower triangular matrix when the mask is a\n",
      "            square matrix. The attention masking has the form of the upper left causal bias due to the alignment\n",
      "            (see :class:`torch.nn.attention.bias.CausalBias`) when the mask is a non-square matrix.\n",
      "            An error is thrown if both attn_mask and is_causal are set.\n",
      "        scale (optional float, keyword-only): Scaling factor applied prior to softmax. If None, the default value is set\n",
      "            to :math:`\\frac{1}{\\sqrt{E}}`.\n",
      "        enable_gqa (bool): If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False.\n",
      "    \n",
      "    Returns:\n",
      "        output (Tensor): Attention output; shape :math:`(N, ..., Hq, L, Ev)`.\n",
      "    \n",
      "    Shape legend:\n",
      "        - :math:`N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}`\n",
      "        - :math:`S: \\text{Source sequence length}`\n",
      "        - :math:`L: \\text{Target sequence length}`\n",
      "        - :math:`E: \\text{Embedding dimension of the query and key}`\n",
      "        - :math:`Ev: \\text{Embedding dimension of the value}`\n",
      "        - :math:`Hq: \\text{Number of heads of query}`\n",
      "        - :math:`H: \\text{Number of heads of key and value}`\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "        >>> # Optionally use the context manager to ensure one of the fused kernels is run\n",
      "        >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n",
      "        >>>     F.scaled_dot_product_attention(query,key,value)\n",
      "    \n",
      "    \n",
      "        >>> # Sample for GQA for llama3\n",
      "        >>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "        >>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n",
      "        >>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n",
      "    \n",
      "    \n",
      "    .. _FlashAttention-2\\: Faster Attention with Better Parallelism and Work Partitioning:\n",
      "        https://arxiv.org/abs/2307.08691\n",
      "    .. _Memory-Efficient Attention:\n",
      "        https://github.com/facebookresearch/xformers\n",
      "    .. _Grouped-Query Attention:\n",
      "        https://arxiv.org/pdf/2305.13245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F.scaled_dot_product_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad398d4",
   "metadata": {},
   "source": [
    "This has a lot of information, so let's pick out the most important parts: the arguments for the function. These are as follows:\n",
    "\n",
    "* `query`, `key`, and `value` : tensors containing the queries, keys, and values respectively\n",
    "\n",
    "* `attn_mask` : an optional tensor, which specifies any combination of specific keys or queries that should be masked out for attention calculation, i.e. **not** used to calculate the attention weight\n",
    "\n",
    "* `dropout_p` : probability of dropout, which is applied to the attention weights but before finding the weighted sum of the values. Note: this **always** applies regardless of if your model is in training or evaluation mode, so you need to make sure you have a check to set `dropout_p` to 0 if you are in evaluation mode.\n",
    "\n",
    "* `is_causal` : if the attention should be causal or not, i.e. should earlier sequence elements attend to later sequence elements. If this is set to `True` and `attn_mask` is passed, this will cause an error.\n",
    "\n",
    "* `scale` : scaling factor applied prior to the softmax, with the default value of $\\frac{1}{{\\sqrt{d_k}}}$ (or $\\frac{1}{\\sqrt{E}}$, using the PyTorch documentation notation), where $d_k$ (or $E$) refers to the dimensionality of the keys and queries\n",
    "\n",
    "There is an additional argument called `enable_gqa`, which enables an experimental feature called Grouped Query Attention to reduce memory usage and decrease compute time, but we won't worry too much about this, especially as it is only working for some select methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fcded",
   "metadata": {},
   "source": [
    "Let's look specifically at the main inputs we will specify: `query`, `key`, and `value`. These are tensors that take the following shapes:\n",
    "\n",
    "\\begin{align*}\n",
    "\\texttt{query}\\text{ : shape} &= (N,\\,h_q,\\,L,\\,d_k) \\\\\n",
    "\\texttt{key}\\text{ : shape} &= (N,\\,h,\\,S,\\,d_k) \\\\\n",
    "\\texttt{value}\\text{ : shape} &= (N,\\,h,\\,S,\\,d_v), \\\\\n",
    "\\end{align*}\n",
    "where each symbol is defined as follows:\n",
    "\n",
    "* $N$ : batch size\n",
    "\n",
    "* $h_q$, $h$ : number of heads for queries and keys/values respectively; generally, these are equivalent (these are only different for Grouped Query Attention, beyond the scope of this notebook)\n",
    "\n",
    "* $L$ : length of the target sequence\n",
    "\n",
    "* $S$ : length of the source sequence\n",
    "\n",
    "* $d_k$ : dimensionality of the embedded queries and keys, referred to as $E$ in the PyTorch documentation\n",
    "\n",
    "* $d_v$ : dimensionality of the embedded values, referred to as $E_v$ in the PyTorch documentation\n",
    "\n",
    "Here we have used the notation from the PyTorch documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50542eed",
   "metadata": {},
   "source": [
    "Let's now try using this, to see what the output looks like. We'll start by specifying our embedding dimensions, source and target sequence lengths, batch size, and number of heads, and generating some random data for queries, keys, and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29144e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #32 sequences in a batch\n",
    "h = h_q = 4 # 4 attention heads\n",
    "S = L = 10 # source and target sequence are both length 10\n",
    "d_k = d_v = 16 # same embedding dimensionality for queries/keys and values, 16-dim vectors\n",
    "torch.manual_seed(0) # fix random seed for value generation\n",
    "\n",
    "query = torch.rand(batch_size, h_q, L, d_k)\n",
    "key = torch.rand(batch_size, h, S, d_k)\n",
    "value = torch.rand(batch_size, h, S, d_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a66e90",
   "metadata": {},
   "source": [
    "Now that we've defined our data and our parameters, let's try calculating the scaled dot product attention and look at the output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46295210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "sdpa = F.scaled_dot_product_attention(query, key, value)\n",
    "print(sdpa.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15705d65",
   "metadata": {},
   "source": [
    "We can see our attention output has shape $(N, h, L, d_v)$, but of course this is straightforward when $S = L$ and $d_k = d_v$. What happens if these are not equivalent? \n",
    "\n",
    "Note we could in principle not have $h = h_q$, but that is the subject of the `enable_gqa` keyword that is beyond the scope of this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a3fc5",
   "metadata": {},
   "source": [
    "Let's consider a simple text summarisation problem, where we want to summarise a 10 token sequence with 1 output token, i.e. $S = 10$ and $L = 1$. We'll also set the value embedding dimensionality to 8 instead of 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e102f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([32, 4, 1, 16])\n",
      "\n",
      "Keys shape: torch.Size([32, 4, 10, 16])\n",
      "\n",
      "Values shape: torch.Size([32, 4, 10, 8])\n",
      "\n",
      "SDPA output shape: torch.Size([32, 4, 1, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "h = h_q = 4\n",
    "S = 10\n",
    "L = 1\n",
    "d_k = 16\n",
    "d_v = 8\n",
    "torch.manual_seed(0)\n",
    "\n",
    "query = torch.rand(batch_size, h_q, L, d_k)\n",
    "key = torch.rand(batch_size, h, S, d_k)\n",
    "value = torch.rand(batch_size, h, S, d_v)\n",
    "sdpa = F.scaled_dot_product_attention(query, key, value)\n",
    "print(f'Queries shape: {query.shape}\\n')\n",
    "print(f'Keys shape: {key.shape}\\n')\n",
    "print(f'Values shape: {value.shape}\\n')\n",
    "print(f'SDPA output shape: {sdpa.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cceada",
   "metadata": {},
   "source": [
    "Now we can properly see the shape of the attention output: $(N, h, L, d_v)$ i.e. batch size and number of heads are the same as all the inputs, it has the sequence length from the queries, and the embedding dimensionality of the values. \n",
    "\n",
    "Note: this function is written with mult-head attention in mind, as it can take inputs from each head simultaneously rather than us needing to run it separately for each attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f10a81",
   "metadata": {},
   "source": [
    "## Implementing the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999151ec",
   "metadata": {},
   "source": [
    "We will now step through implementing the transformer, piece by piece. You will get to fill in a lot of the code yourself, using what you have learnt on this course so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec067cf",
   "metadata": {},
   "source": [
    "### Multi-head attention in PyTorch\n",
    "\n",
    "Firstly, let's step through calculating an example multihead attention output for a given set of queries, keys, and values. We start by defining our tensors and the dimensionality of our model; we'll use the same model dimensions as in the original paper, and consider we want to look at 10-length source and target sequences.\n",
    "\n",
    "Note: in line with existing implementations, we will allow queries, keys, and values to have **different** dimensions when we pass them into the multi-head attention block, and we will project from these dimensions to the same embedding dimension. In other words, we have assumed $d_k = d_v$, which we previously noted was most common in practice. We will denote the input dimensions as $E_q$, $E_k$, and $E_v$ respectively. \n",
    "\n",
    "Also, because we are projecting to $h$ different heads, we will project each tensor to $d_\\text{model}$ and assume each head has a dimension of $d_\\text{head} = d_\\text{model}/h$. We then can say that $d_k = d_v = d_\\text{head}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1dfa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "E_q = E_k = E_v = 512 # input query, key, value embedding dimensions\n",
    "d_model = 512 # model dimensions\n",
    "h = 8 # number of heads\n",
    "S = L = 10 # source and target sequence lengths\n",
    "d_head = d_model // h # dimension per head, // means we want an integer result\n",
    "\n",
    "Q = torch.randn(batch_size, L, d_model)\n",
    "K = torch.randn(batch_size, S, d_model)\n",
    "V = torch.randn(batch_size, S, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f0af4",
   "metadata": {},
   "source": [
    "Our query, key and value inputs have a dimension of $d_{\\text{model}}$, which we need to project to the relevant shape. We will also assume that we want the final output of our multi-head attention to have the same dimensionality as the input.\n",
    "\n",
    "Now, let's define our linear projections to project to each attention head. To do this, we want to project from the $d_{\\text{model}}$ input to the relevant dimensions, $h$ times. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b1df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_proj = nn.Linear(E_q, d_model)\n",
    "K_proj = nn.Linear(E_k, d_model)\n",
    "V_proj = nn.Linear(E_v, d_model)\n",
    "q, k, v = Q_proj(Q), K_proj(K), V_proj(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967aea5",
   "metadata": {},
   "source": [
    "Each of these linear layers will project from $d_\\text{model}$ to $h$ times the relevant dimension for the queries, keys, and values. In other words, all of our attention heads are effectively in one tensor; this is more efficient than looping $h$ times, but does mean we will need to reshape our tensors in order to apply our attention mechanism. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6604af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape queries\n",
    "q = q.unflatten(-1, [h, d_head]) # (batch_size, L, h, d_head)\n",
    "q = q.transpose(1, 2) # (batch_size, h, L, d_head)\n",
    "# reshape keys\n",
    "k = k.unflatten(-1, [h, d_head]) # (batch_size, S, h, d_head)\n",
    "k = k.transpose(1, 2) # (batch_size, h, S, d_head)\n",
    "# reshape values\n",
    "v = v.unflatten(-1, [h, d_head]) # (batch_size, S, h, d_head)\n",
    "v = v.transpose(1, 2) # (batch_size, h, S, d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d655099",
   "metadata": {},
   "source": [
    "`unflatten(-1, [h, d_head])` is used to reshape the last dimension of each tensor into two dimensions, with shape `[h, d_head]`. We can double-check the shape of each to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50be3106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([32, 8, 10, 64])\n",
      "\n",
      "Keys shape: torch.Size([32, 8, 10, 64])\n",
      "\n",
      "Values shape: torch.Size([32, 8, 10, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Queries shape: {q.shape}\\n')\n",
    "print(f'Keys shape: {k.shape}\\n')\n",
    "print(f'Values shape: {v.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc733cd0",
   "metadata": {},
   "source": [
    "We can see that each of these have a the right shape, with the batch size first, then the number of heads, then the sequence length, and finally the dimension per head. \n",
    "\n",
    "Now we've got these in the right shape, we can find the attention output using `scaled_dot_product_attention`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47011959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: torch.Size([32, 8, 10, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attn_output = F.scaled_dot_product_attention(q, k, v)\n",
    "print(f'Attention output shape: {attn_output.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b20321",
   "metadata": {},
   "source": [
    "Now we have the attention output, we need to concatenate each head together so we can apply the final linear transformation. We want to get the shape to be $(N, L, h\\cdot d_\\text{head})$, i.e. we need to combine the axes for the heads and for the dimensions. We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b7d0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape after concatenating heads: torch.Size([32, 10, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attn_output = attn_output.transpose(1, 2) # (batch_size, L, h, d_head)\n",
    "attn_output = attn_output.flatten(-2) # (batch_size, L, h * d_head)\n",
    "print(f'Attention output shape after concatenating heads: {attn_output.shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d285f",
   "metadata": {},
   "source": [
    "Finally, we need to apply the final linear transformation, so the attention output from each head can be aggregated to produce new output. We want the final output to have the same final dimension as the input, so we can easily chain layers together. We will therefore set our output dimension equal to $E_q$. As a result, we do our final projection as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af91590",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = E_q\n",
    "out_proj = nn.Linear(h * d_head, d_out)\n",
    "final_output = out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33607953",
   "metadata": {},
   "source": [
    "Finally, let's compare the input shapes and the final output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "852bf441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query shape: torch.Size([32, 10, 512])\n",
      "Input key shape: torch.Size([32, 10, 512])\n",
      "Input value shape: torch.Size([32, 10, 512])\n",
      "Final output shape: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "print(f'Input query shape: {Q.shape}')\n",
    "print(f'Input key shape: {K.shape}')\n",
    "print(f'Input value shape: {V.shape}')\n",
    "print(f'Final output shape: {final_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d11ad0",
   "metadata": {},
   "source": [
    "And so we can see the final output shape is exactly what we would expect, ready to be passed into the next layer in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de55798",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Now you have seen how we can do multihead attention in PyTorch, fill in the gaps in the class definition below to define a multihead attention layer that we can use in PyTorch models. Remember the key steps of multi-head attention:\n",
    "\n",
    "* Project input queries, keys, and values to the total embedding dimension size $d_\\text{model}$\n",
    "\n",
    "* Reshape each projected input to separate out each head in the tensor\n",
    "\n",
    "* Find the scaled dot product attention\n",
    "\n",
    "* Reshape the attention output\n",
    "\n",
    "* Apply the output linear transformation to the attention output\n",
    "\n",
    "Note the `attn_mask` and `is_causal` arguments for the `forward` function. `attn_mask` is necessary if we have a batch of sequences of different lengths, such that some of the sequence elements are just padding and should be ignored for calculating attention, and `is_causal` is necessary if we want to ensure earlier sequence elements cannot attend to later sequence elements (like we will need for decoder self-attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec8befcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, E_q, E_k, E_v, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        # Require embed_dim  = dim_per_head * num_heads i.e. divisible by num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # Define projections of queries, keys, and values\n",
    "        self.q_proj = nn.Linear(E_q, d_model)\n",
    "        self.k_proj = nn.Linear(E_k, d_model)\n",
    "        self.v_proj = nn.Linear(E_v, d_model)\n",
    "        # Define output projection\n",
    "        out_dim = E_q\n",
    "        self.out_proj = nn.Linear(d_model, out_dim)\n",
    "        # Define dimensions per head\n",
    "        self.d_head = d_model // num_heads\n",
    "        \n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask = None, is_causal = False):\n",
    "        Q = self.q_proj(queries)\n",
    "        K = self.k_proj(keys)\n",
    "        V = self.v_proj(values)\n",
    "        # Reshape to correct shapes for scaled_dot_product_attention\n",
    "        Q = Q.unflatten(-1,[self.num_heads, self.d_head]).transpose(1,2) # (N, h, L, d_head)\n",
    "        K = K.unflatten(-1,[self.num_heads, self.d_head]).transpose(1,2) # (N, h, S, d_head)\n",
    "        V = V.unflatten(-1,[self.num_heads, self.d_head]).transpose(1,2) # (N, h, S, d_head)\n",
    "        # Find attention output\n",
    "        attn_output = F.scaled_dot_product_attention(Q, K, V, attn_mask = attn_mask, is_causal = is_causal)\n",
    "        attn_output = attn_output.transpose(1,2).flatten(-2)\n",
    "        # Output projection\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079473b8",
   "metadata": {},
   "source": [
    "How can we make this into multi-head self-attention? That is in fact straightforward: we just pass the same tensor as our queries, keys, and values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8934d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_input = 64\n",
    "self_attn = MultiHeadAttention(E_q=d_input, E_k=d_input, E_v=d_input, d_model=512, num_heads=8)\n",
    "x = torch.rand(32, 10, d_input)  # (batch_size, seq_length, d_input)\n",
    "output = self_attn(x, x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26dad8e",
   "metadata": {},
   "source": [
    "### Transformer layers\n",
    "\n",
    "Now that we have a multi-head attention implementation, we can implement the other parts of the transformer. Let's first go through the steps of a single encoder layer, then we can again implement it as an `nn.Module` subclass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30e601",
   "metadata": {},
   "source": [
    "From the architecture diagram above, a single encoder layer has the following steps:\n",
    "\n",
    "* Input batch of sequences with dimension $d_\\text{model} = 512$\n",
    "\n",
    "* Pass into multi-head self-attention with 8 heads, apply dropout after the attention layer\n",
    "\n",
    "* Sum the attention input with the attention output and pass through a layer normalisation\n",
    " \n",
    "* Pass layer-normalised attention layer output into a two-layer feed-forward neural network (FFN), with a ReLU activation function and a dropout layer, with hidden dimension $d_{ff} = 2048$, and apply dropout on the output\n",
    "\n",
    "* Sum the FFN input and output and apply layer normalisation\n",
    "\n",
    "* Return the second layer normalisation output\n",
    "\n",
    "All dropout layers use $p = 0.1$.\n",
    "\n",
    "We proceed by first generating our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b32bf08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 512]) : (batch_size, seq_len, d_model)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "d_model = 512 # model dimensions\n",
    "d_ff = 2048 #FFN hidden dimension\n",
    "dropout_p = 0.1 #dropout probability\n",
    "num_heads = 8 # number of heads\n",
    "seq_len = 10 #sequence length\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f'Input shape: {x.shape} : (batch_size, seq_len, d_model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e6f44",
   "metadata": {},
   "source": [
    "Now we can define the components necessary for the attention part of the encoder layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2220dca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after MHA, dropout, and layer norm: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# If you didn't get the MultiheadAttention class working, uncomment the following lines instead:\n",
    "# self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, \n",
    "#                                   dropout=dropout_p, batch_first=True)\n",
    "self_attn = MultiHeadAttention(E_q=d_model, E_k=d_model, E_v=d_model, d_model=d_model, num_heads=num_heads)\n",
    "attn_dropout = nn.Dropout(p=dropout_p)\n",
    "attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "attn_output = self_attn(x, x, x)\n",
    "attn_output = attn_dropout(attn_output)\n",
    "x = attn_norm(x + attn_output)\n",
    "print(f'Output shape after MHA, dropout, and layer norm: {x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbcf45",
   "metadata": {},
   "source": [
    "Note that we kept `x` unchanged until after finding the attention output, and then we summed `x` and `attn_output`. This is the residual connection we described before.\n",
    "\n",
    "`nn.LayerNorm` is a layer normalisation object, which requires us to pass a dimensionality to it when we instantiate it. \n",
    "\n",
    "Next, we need to pass `x` through a feed-forward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01f7760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after FFN, dropout, and layer norm: torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "# Define FFN layers\n",
    "ffn_layer1 = nn.Linear(d_model, d_ff)\n",
    "ffn_activation = nn.ReLU()\n",
    "ffn_dropout1 = nn.Dropout(p=dropout_p)\n",
    "\n",
    "ffn_layer2 = nn.Linear(d_ff, d_model)\n",
    "ffn_dropout2 = nn.Dropout(p=dropout_p)\n",
    "\n",
    "ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "# Feedforward network\n",
    "ffn_output = ffn_dropout1(ffn_activation(ffn_layer1(x)))\n",
    "ffn_output = ffn_dropout2(ffn_layer2(ffn_output))\n",
    "x = ffn_norm(x + ffn_output) # FFN residual connection\n",
    "print(f'Output shape after FFN, dropout, and layer norm: {x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ec0cf",
   "metadata": {},
   "source": [
    "And those are the necessary steps to pass through a single transformer encoder layer. We can see the final output is the same shape as our input, ready to feed into another encoder layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfd18ef",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Now, fill the gaps in the class definition below to define a transformer encoder layer for use in a PyTorch model. Remember the key steps:\n",
    "\n",
    "* Multi-head self attention, including:\n",
    "    \n",
    "    * A multi-head attention layer where queries, keys, and values are the same tensor\n",
    "    \n",
    "    * Dropout layer after multi-head attention\n",
    "\n",
    "    * Residual connection after the dropout layer\n",
    "\n",
    "    * Layer normalisation\n",
    "\n",
    "* A two-layer feedforward network, including:\n",
    "\n",
    "    * 1st linear layer projecting $d_\\text{model}$ to $d_{ff}$\n",
    "\n",
    "    * ReLU activation function\n",
    "\n",
    "    * Dropout layer after activation\n",
    "\n",
    "    * 2nd linear layer projecting $d_{ff}$ to $d_\\text{model}$\n",
    "\n",
    "    * Dropout layer after 2nd linear layer\n",
    "\n",
    "    * Residual connection after the dropout layer\n",
    "\n",
    "    * Layer normalisation\n",
    "\n",
    "Note the `src_mask` and `is_causal` arguments for the `forward` method, which should be passed for the multi-head attention call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21d046df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize a Transformer encoder layer.\n",
    "        Parameters:\n",
    "        - d_model: dimension of the model\n",
    "        - num_heads: number of attention heads\n",
    "        - d_ff: dimension of the feed-forward network\n",
    "        - dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_model, d_model, d_model, num_heads)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask = None, is_causal = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer encoder layer.\n",
    "        Parameters:\n",
    "        - src: input tensor of shape (batch_size, seq_length, d_model)\n",
    "        - src_mask: mask for the tensor input, to mask out keys that should be excluded\n",
    "                    from attention calculations\n",
    "        - is_causal: whether to apply causal masking in self-attention\n",
    "        \"\"\"\n",
    "        x = src\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, attn_mask = src_mask, is_causal = is_causal)\n",
    "        x = self.attn_norm(x + self.attn_dropout(attn_output))\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.ffn_norm(x + self.dropout2(ffn_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d5952",
   "metadata": {},
   "source": [
    "Now we have an encoder layer, we should also prepare a decoder layer. This is mostly the same as the encoder layer, but the self-attention layer is masked, so will need to have `is_causal = True`, and we have an additional attention block (with associated dropout, residual connection, and layer normalisation) that finds the cross attention between the encoder outputs, as keys and values, and the decoder hidden states as the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e63e9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Fill the gaps in the class definition below to define a transformer decoder layer for use in a PyTorch model. Remember the key steps:\n",
    "\n",
    "* Multi-head masked self attention, including:\n",
    "    \n",
    "    * A multi-head attention layer where queries, keys, and values are the same tensor, with `is_causal = True`\n",
    "    \n",
    "    * Dropout layer after multi-head attention\n",
    "\n",
    "    * Residual connection after the dropout layer\n",
    "\n",
    "    * Layer normalisation\n",
    "<br></br>\n",
    "* Multi-head cross-attention, including:    \n",
    "\n",
    "    * A multi-head attention layer where queries are the input to the decoder layer and the keys and values are the final encoder output\n",
    "    \n",
    "    * Dropout layer after multi-head attention\n",
    "\n",
    "    * Residual connection after the dropout layer\n",
    "\n",
    "    * Layer normalisation\n",
    "<br></br>\n",
    "* A two-layer feedforward network, including:\n",
    "\n",
    "    * 1st linear layer projecting $d_\\text{model}$ to $d_{ff}$\n",
    "\n",
    "    * ReLU activation function\n",
    "\n",
    "    * Dropout layer after activation\n",
    "\n",
    "    * 2nd linear layer projecting $d_{ff}$ to $d_\\text{model}$\n",
    "\n",
    "    * Dropout layer after 2nd linear layer\n",
    "\n",
    "    * Residual connection after the dropout layer\n",
    "\n",
    "    * Layer normalisation\n",
    "\n",
    "Note the extra arguments for the `forward` method: `tgt_mask`, `enc_mask`, `tgt_is_causal`, and `enc_out_is_causal`; these are to indicate any necessary masking for the decoder self-attention and cross-attention, and if the scaled dot product attention should be causaul or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3110795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Masked self-attention block\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_model, d_model, d_model, num_heads)\n",
    "        self.self_attn_dropout = nn.Dropout(p = dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Cross attention block\n",
    "        self.cross_attn = MultiHeadAttention(d_model, d_model, d_model, d_model, num_heads)\n",
    "        self.cross_attn_dropout = nn.Dropout(p = dropout)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # FFN block\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ffn_dropout1 = nn.Dropout(p = dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.ffn_dropout2 = nn.Dropout(p = dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, enc_out, tgt_mask = None, enc_mask = None, tgt_is_causal = True, enc_out_is_causal = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer decoder layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - tgt: input tensor of shape (batch_size, seq_length, d_model)\n",
    "        - enc_out: encoder output tensor of shape (batch_size, enc_seq_length, d_model)\n",
    "        - src_mask: mask for the src tensor input\n",
    "        - enc_mask: mask for the encoder output tensor input\n",
    "        \"\"\"\n",
    "        x = tgt\n",
    "        # Masked self-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, attn_mask = tgt_mask, is_causal = tgt_is_causal)\n",
    "        x = self.self_attn_norm(x + self.self_attn_dropout(self_attn_output))\n",
    "        \n",
    "        # Cross-attention\n",
    "        cross_attn_output = self.cross_attn(x, enc_out, enc_out, attn_mask = enc_mask, is_causal = enc_out_is_causal)\n",
    "        x = self.cross_attn_norm(x + self.cross_attn_dropout(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.linear2(self.ffn_dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.ffn_norm(x + self.ffn_dropout2(ffn_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d1c71",
   "metadata": {},
   "source": [
    "Now we have implemented the main components of the transformer architecture!\n",
    "\n",
    "Of course, we have put everything together into a single architecture. To do this, we also need:\n",
    "\n",
    "* Layers to embed input and output sequences to pass into the encoder and decoder respectively\n",
    "\n",
    "* Positional encodings to add to the embedded sequences\n",
    "\n",
    "* The output linear transformation and softmax layers\n",
    "\n",
    "We will illustrate each of these, and then you will write an `nn.Module` for a whole transformer model. To start with, we can use `nn.Embedding` from PyTorch to embed our input/output sequences. For demonstration, let's consider an NLP problem:\n",
    "\n",
    "* Assume we have a limited vocabulary consisting of just 5 words, from the sentence \"my new cat is black\"; this means our vocab size $d_\\text{vocab}$ = 5\n",
    "\n",
    "* We need to convert this to a numerical vector, so we will assign an integer to each unique word in sentence\n",
    "\n",
    "* Finally, we can pass the integer sequence through `nn.Embedding` to embed to $d_\\text{model}$ dimensions\n",
    "\n",
    "For this example, we will use $d_\\text{model} = 16$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35561ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Define example sequence\n",
    "sequence = [word for word in \"My new cat is black\".split(' ')]\n",
    "\n",
    "# Encode words\n",
    "word_mapping = {word : i for i, word in enumerate(sequence)}\n",
    "mapped_sequence = torch.tensor([word_mapping[word] for word in sequence])\n",
    "\n",
    "# Find embeddings\n",
    "embedding_dim = 16\n",
    "d_vocab = len(word_mapping)\n",
    "embeddings = nn.Embedding(num_embeddings=d_vocab, embedding_dim=embedding_dim)\n",
    "embedded_sequence = embeddings(mapped_sequence)\n",
    "print(embedded_sequence.shape)  # Should be (sequence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d7339",
   "metadata": {},
   "source": [
    "Inside our actual transformer, we would only want to include the `nn.Embedding` part, as this is the actual part of the model; finding a vocab mapping to integers is a task for the data pre-processing. In situations where our sequences are numerical values, we won't need to do the mapping to a set of integers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87864c42",
   "metadata": {},
   "source": [
    "To find positional encodings as described in the paper, we can use the following module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baa03d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        # Dropout after adding positional encoding\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Create positional encoding matrix up to max_len to save\n",
    "        # computation time when calling forward()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        # equivalent to e^(i*-ln(10000)/d_model) = e^(ln(10000^(-i/d_model))) = 1/10000^(i/d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        # fill even embedding dims with sine, odd embedding dims with cosine\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        # Need to include this line to ensure the positional encoding is transferred\n",
    "        # to the same device as the model, in case we use GPU\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3bdca9",
   "metadata": {},
   "source": [
    "Let's test it on our embedded sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b750dc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Assume our max sequence length is 10\n",
    "pos_encode = PositionalEncoding(d_model=16, dropout=0.1, max_len=10)\n",
    "embed_seq_batch = embedded_sequence.unsqueeze(0)  # (1, seq_length, embedding_dim)\n",
    "pos_encoded_seq = pos_encode(embed_seq_batch)\n",
    "print(pos_encoded_seq.shape)  # Should be (1, sequence_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f1b870",
   "metadata": {},
   "source": [
    "We know the final output of the decoder needs to be passed through a linear transformation and then a softmax to retrieve weights for all possible tokens for all elements in the sequence; this means our final transformation has to go from $d_\\text{model}$ to $d_\\text{vocab}$, i.e. it is essentially an inverse of the embedding transformation. We can therefore easily add a linear layer after our decoder, followed by a softmax, to get the final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b841c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Now that we have all of the pieces we need to construct a full transformer architecture in PyTorch, fill in the gaps in the class definition below to do so. Remember the key steps:\n",
    "\n",
    "* Embed the input sequence to $d_\\text{model}$ dimensions\n",
    "\n",
    "* Pass the embedded input sequence through $N$ transformer encoder layers, with $d_\\text{model}$ embedding dimensions, $h$ heads, and a FFN hidden dimension of $d_{ff}$\n",
    "\n",
    "* Embed the output (target) sequence to $d_\\text{model}$ dimensions\n",
    "\n",
    "* Pass the embedded output sequence and the final encoder output through $N$ transformer decoder layers, with $d_\\text{model}$ embedding dimensions, $h$ heads for both attention blocks, and a FFN hidden dimension of $d_{ff}$\n",
    "\n",
    "* Pass the final decoder output through the output linear transformation and softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67094c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(d_vocab, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.output_linear = nn.Linear(d_model, d_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, src_is_causal = False, tgt_is_causal = True):\n",
    "        # Embed and add positional encoding to source\n",
    "        src = self.input_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask, src_is_causal)\n",
    "        enc_out = src\n",
    "        # Embed and add positional encoding to target\n",
    "        tgt = self.input_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, enc_out, tgt_mask, None, tgt_is_causal, src_is_causal)\n",
    "        # Output linear transformation & softmax\n",
    "        output = self.output_linear(tgt)\n",
    "        output = F.softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9bfa1f",
   "metadata": {},
   "source": [
    "Now we've got a full transformer architecture! While we have gone through constructing this, the example problems you can practically solve in these sessions simply don't need a full transformer encoder-decoder architecture. Instead, we will use simpler architectures to solve these problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632eadb",
   "metadata": {},
   "source": [
    "## An aside on learning rate warm-up\n",
    "\n",
    "A common technique used for training transformers is so-called **learning rate warm-up**. This is when the learning rate starts at 0, and is gradually increased to the desired value over the first few iterations. As a result, the initial learning is with much smaller steps rather than starting with large steps. Without this process, it is common for transformer gradients can be very large early on in training, causing poor results. You can read more about this in the literature, e.g. [this paper](https://arxiv.org/pdf/1908.03265) on how warm-up improves performance with the Adam optimizer.\n",
    "\n",
    "The code snippet below defines a learning rate scheduler that combines warm-up with a cosine learning rate decay. This implementation is borrowed from the [University of Amsterdam tutorial on transformers](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html). \n",
    "\n",
    "The code block that follows is an example of how this might be used in a training loop, but **won't run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1fbfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + math.cos(math.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS BLOCK WILL NOT RUN, AND IS FOR ILLUSTRATIVE PURPOSES ONLY\n",
    "\n",
    "# LR schedular usage example\n",
    "model = some_model\n",
    "optimizer = optim.Adam(model.parameters(), lr = some_lr)\n",
    "scheduler = CosineWarmupScheduler(optimizer, warmup = 10, max_iters=100)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for batch in data_loader:\n",
    "        # forward pass\n",
    "        ...\n",
    "        # compute loss\n",
    "        ...\n",
    "        # backward pass and optimization step\n",
    "        ...\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72107fcf",
   "metadata": {},
   "source": [
    "In other words, our learning rate scheduler connects to our optimizer, and then after we have iterated over all of our training batches in a given epoch we need to call `.step()` on the scheduler. This will update the learning rate as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4b434a",
   "metadata": {},
   "source": [
    "Now, we have all the tools we need to train a simple transformer for a relatively straightforward problem: reversing the order of a list. \n",
    "\n",
    "This exercise is borrowed from the same [UvA tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html) as the learning rate scehduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be989881",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Exercise\n",
    "\n",
    "The code cell below defines a dataset consisting of a sequence of random numbers in a given range, where the objective is to learn how to reverse the order of the sequence. We will tackle this with a very simple transformer architecture, just using a single encoder layer with only 1 head. \n",
    "\n",
    "Firstly, we generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0474b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class ReverseDataset(data.Dataset):\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels\n",
    "    \n",
    "n_categories = 10\n",
    "seq_length = 16\n",
    "dataset = ReverseDataset(num_categories=n_categories, seq_len=seq_length, size=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dba72b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Split this dataset into a training and test dataset, with 50000 training samples and 10000 test samples.\n",
    "\n",
    "After the dataset split, make DataLoaders for each with a batch size of 128. Make sure that the training DataLoader shuffles data during iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e64db591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for training/test split and DataLoaders\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_train = 50000\n",
    "n_test = 10000\n",
    "train_dataset, test_dataset = data.random_split(dataset, [n_train, n_test])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last = True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896dfaf",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now we have our data, we need to define a model. Fill out the class template below to define a simple transformer model for this problem, with the following architecture:\n",
    "\n",
    "* Embedding layer to go from the sequence input dimension `input_dim` to the model dimension `d_model`\n",
    "\n",
    "* Positional encoding layer\n",
    "\n",
    "* `n_layer` transformer encoder layers, where the FFN hidden dimension `d_ff` = 2*`d_model`\n",
    "\n",
    "* An output linear transformation from `d_model` to `n_classes`\n",
    "\n",
    "After writing the class, make an instance of the model with the following parameter values:\n",
    "\n",
    "* `input_dim` = `n_categories`\n",
    "\n",
    "* `d_model` = 32\n",
    "\n",
    "* `n_heads` = 1\n",
    "\n",
    "* `n_classes` = `n_categories`\n",
    "\n",
    "* `n_layer` = 1\n",
    "\n",
    "* `dropout` = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e2f9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_classes, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, d_model*2, dropout) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6ecd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "\n",
    "model = ReversePredictor(\n",
    "    input_dim=n_categories,\n",
    "    d_model = 32,\n",
    "    n_heads = 1,\n",
    "    n_classes = n_categories,\n",
    "    n_layers = 1,\n",
    "    dropout = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6a889",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now, train the model for 10 epochs. Use the following training parameters:\n",
    "\n",
    "* Train for 10 epochs\n",
    "\n",
    "* The Adam optimizer, with a learning rate of 0.001\n",
    "\n",
    "* CosineWarmUp learning rate scheduler, with `warmup` = 50 and `max_iters` = `n_epochs` $\\times$ `len(train_loader)`\n",
    "\n",
    "* Use CrossEntropyLoss\n",
    "\n",
    "Your training loop should be the same as normal, with the exception of stepping the learning rate scheduler after processing all batches in an epoch. \n",
    "\n",
    "**Note**: when we get the output from the model it will have shape (`batch_size`, `seq_length`, `n_categories`), and the target will have shape (`batch_size`, `seq_length`). `CrossEntropyLoss` requires the model output have shape (`batch_size`, `n_categories`, `seq_length`), so make sure to transpose the correct axes before passing through the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf0d788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 2.4366, Train Accuracy: 9.80%\n",
      "Epoch 2/10, Train Loss: 2.3912, Train Accuracy: 9.88%\n",
      "Epoch 3/10, Train Loss: 2.3293, Train Accuracy: 10.85%\n",
      "Epoch 4/10, Train Loss: 2.2848, Train Accuracy: 14.02%\n",
      "Epoch 5/10, Train Loss: 2.1950, Train Accuracy: 20.39%\n",
      "Epoch 6/10, Train Loss: 1.8657, Train Accuracy: 34.12%\n",
      "Epoch 7/10, Train Loss: 1.2247, Train Accuracy: 54.42%\n",
      "Epoch 8/10, Train Loss: 0.6454, Train Accuracy: 75.88%\n",
      "Epoch 9/10, Train Loss: 0.3021, Train Accuracy: 89.62%\n",
      "Epoch 10/10, Train Loss: 0.1430, Train Accuracy: 94.74%\n"
     ]
    }
   ],
   "source": [
    "# Your training loop here\n",
    "\n",
    "n_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineWarmupScheduler(optimizer, warmup=50, max_iters = n_epochs * len(train_loader))\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    n_correct = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.transpose(1,2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_correct += (output.argmax(dim=-1) == targets).sum().item()\n",
    "    scheduler.step()\n",
    "    accuracy = n_correct / (len(train_loader.dataset) * seq_length)\n",
    "    return total_loss / len(train_loader), accuracy\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch()\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468bd57",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Finally, evaluate the performance of your trained model on the test dataset. To get the predicted sequence from the model outputs, find the `torch.argmax` of the model output along the final axis. Calculate both the test loss and the test acccuracy. How accurate is the model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b298b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0997, Test Accuracy: 96.36%\n"
     ]
    }
   ],
   "source": [
    "# Your evaluation code here\n",
    "\n",
    "def eval_model():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            loss = criterion(output.transpose(1,2), targets)\n",
    "            total_loss += loss.item()\n",
    "            n_correct += (output.argmax(dim=-1) == targets).sum().item()\n",
    "    accuracy = n_correct / (len(test_loader.dataset) * seq_length)\n",
    "\n",
    "    return total_loss / len(test_loader), accuracy\n",
    "\n",
    "test_loss, test_accuracy = eval_model()\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d0094",
   "metadata": {},
   "source": [
    "The next notebook will include more exercises looking at different problems using transformers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
