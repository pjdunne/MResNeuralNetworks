{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca4be49",
   "metadata": {},
   "source": [
    "# Transformers in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4d59e",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index <a id='index'></a>\n",
    "1. [Introduction](#intro)\n",
    "1. [Transformer code blocks](#code-blocks)\n",
    "1. [List reversing, continued](#list-reverse)\n",
    "1. [LHC jet tagging using transformers](#jet-tagging)\n",
    "\n",
    "\n",
    "**Note**: to make the most of this notebook, being able to run on your GPU for training is important. Make sure you are running this notebook in the container for GPU usage you were shown earlier in the course (if you weren't able to get this working or can't remember how to do it, adaptations for exercises will be given to ensure code doesn't take too long to run)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba275299",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Introduction <a id='intro'></a> [^](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cd8b2",
   "metadata": {},
   "source": [
    "Now that you've seen how transformers work and how we can implement them using PyTorch, we will try approaching a few different problems using transformers, and explore some of the individual components in a bit more detail. This includes:\n",
    "\n",
    "* Visualising attention outputs for our list reversing example\n",
    "\n",
    "* Increasing the number of heads to handle a more complex list reversing problem\n",
    "\n",
    "* Demonstrating the performance improvement using transformers vs standard neural networks for an LHC jet tagging problem\n",
    "\n",
    "The next section will give you code blocks for the parts we need for our transformer architectures, which you can use for the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde55588",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Transformer code blocks <a id='code-blocks'></a> [^](#index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e0087",
   "metadata": {},
   "source": [
    "While you will have written your own code last time, for this notebook use the code blocks in this section. The majority of them will be the same as in the worked example version of the previous notebook, with the exception of the MultiHeadAttention class, which here has an option to return the attention output as well as the final output.\n",
    "\n",
    "We will also define our own version of `scaled_dot_product_attention` here, as the PyTorch implementation can only return the output, whereas we want to be able to visualise the attention weights. This implementation is based on the example given in the [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html), with added functionality to return the attention weights. We will call this new class `SDPA`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e6be5",
   "metadata": {},
   "source": [
    "Firstly, our standard set of imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662c2a6",
   "metadata": {},
   "source": [
    "Now, we will define `SDPA`, `MultiHeadAttention`, and `PositionalEncoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly copied from https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html,\n",
    "# but modified to return attention weights as well.\n",
    "def SDPA(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias = attn_mask + attn_bias\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value, attn_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as worked example of previous notebook, but modified to use custom SDPA function\n",
    "# and to return attention weights if needed.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, E_q, E_k, E_v, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        # Require embed_dim  = dim_per_head * num_heads i.e. divisible by num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        # Define projections of queries, keys, and values\n",
    "        self.q_proj = nn.Linear(E_q, d_model)\n",
    "        self.k_proj = nn.Linear(E_k, d_model)\n",
    "        self.v_proj = nn.Linear(E_v, d_model)\n",
    "        # Define output projection\n",
    "        out_dim = E_q\n",
    "        self.out_proj = nn.Linear(d_model, out_dim)\n",
    "        # Define dimensions per head\n",
    "        self.d_head = d_model // num_heads\n",
    "        \n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask = None, is_causal = False, return_attn_weights = False):\n",
    "        Q = self.q_proj(queries)\n",
    "        K = self.k_proj(keys)\n",
    "        V = self.v_proj(values)\n",
    "        # Reshape to correct shapes for scaled_dot_product_attention\n",
    "        Q = Q.unflatten(-1,[self.num_heads, self.d_head]).transpose(1,2) # (N, h, L, d_head)\n",
    "        K = K.unflatten(-1,[self.num_heads, self.d_head]).transpose(1,2) # (N, h, S, d_head)\n",
    "        V = V.unflatten(-1,[self.num_heads, self.d_head]).transpose(1,2) # (N, h, S, d_head)\n",
    "        # Find attention output\n",
    "        attn_output, attn_weight = SDPA(Q, K, V, attn_mask = attn_mask, is_causal = is_causal)\n",
    "        attn_output = attn_output.transpose(1,2).flatten(-2)\n",
    "        # Output projection\n",
    "        o = self.out_proj(attn_output)\n",
    "        if return_attn_weights:\n",
    "            return o, attn_weight\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding is the same as before\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        # Dropout after adding positional encoding\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Create positional encoding matrix up to max_len to save\n",
    "        # computation time when calling forward()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        # equivalent to e^(i*-ln(10000)/d_model) = e^(ln(10000^(-i/d_model))) = 1/10000^(i/d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        # fill even embedding dims with sine, odd embedding dims with cosine\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        # Need to include this line to ensure the positional encoding is transferred\n",
    "        # to the same device as the model, in case we use GPU\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x + self.pe[:,:x.size(1)].view(self.pe.size(0),x.size(1),*[1 for _ in x.shape[2:-1]], self.pe.size(-1))\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3890c7",
   "metadata": {},
   "source": [
    "Next, we define `TransformerEncoderLayer`, `TransformerDecoderLayer`, and `TransformerModel`, with additional helper functions to extract attention weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerEncoderLayer and TransformerDecoderLayer same as in worked example with an\n",
    "# added utility function for each to get the attention weights for all attention layers\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize a Transformer encoder layer.\n",
    "        Parameters:\n",
    "        - d_model: dimension of the model\n",
    "        - num_heads: number of attention heads\n",
    "        - d_ff: dimension of the feed-forward network\n",
    "        - dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_model, d_model, d_model, num_heads)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask = None, is_causal = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer encoder layer.\n",
    "        Parameters:\n",
    "        - src: input tensor of shape (batch_size, seq_length, d_model)\n",
    "        - src_mask: mask for the tensor input, to mask out keys that should be excluded\n",
    "                    from attention calculations\n",
    "        - is_causal: whether to apply causal masking in self-attention\n",
    "        \"\"\"\n",
    "        x = src\n",
    "        # Self-attention\n",
    "        attn_output = self.self_attn(x, x, x, attn_mask = src_mask, is_causal = is_causal)\n",
    "        x = self.attn_norm(x + self.attn_dropout(attn_output))\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.ffn_norm(x + self.dropout2(ffn_output))\n",
    "        return x\n",
    "\n",
    "    def get_attention_maps(self, x, mask = None, is_causal = False):\n",
    "        _, attn_maps = self.self_attn(x, x, x, attn_mask = mask, is_causal = is_causal, return_attn_weights = True)\n",
    "        return attn_maps\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Masked self-attention block\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_model, d_model, d_model, num_heads)\n",
    "        self.self_attn_dropout = nn.Dropout(p = dropout)\n",
    "        self.self_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Cross attention block\n",
    "        self.cross_attn = MultiHeadAttention(d_model, d_model, d_model, d_model, num_heads)\n",
    "        self.cross_attn_dropout = nn.Dropout(p = dropout)\n",
    "        self.cross_attn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # FFN block\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ffn_dropout1 = nn.Dropout(p = dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.ffn_dropout2 = nn.Dropout(p = dropout)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, enc_out, tgt_mask = None, enc_mask = None, tgt_is_causal = True, enc_out_is_causal = False):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer decoder layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - tgt: input tensor of shape (batch_size, seq_length, d_model)\n",
    "        - enc_out: encoder output tensor of shape (batch_size, enc_seq_length, d_model)\n",
    "        - src_mask: mask for the src tensor input\n",
    "        - enc_mask: mask for the encoder output tensor input\n",
    "        \"\"\"\n",
    "        x = tgt\n",
    "        # Masked self-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, attn_mask = tgt_mask, is_causal = tgt_is_causal)\n",
    "        x = self.self_attn_norm(x + self.self_attn_dropout(self_attn_output))\n",
    "        \n",
    "        # Cross-attention\n",
    "        cross_attn_output = self.cross_attn(x, enc_out, enc_out, attn_mask = enc_mask, is_causal = enc_out_is_causal)\n",
    "        x = self.cross_attn_norm(x + self.cross_attn_dropout(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.linear2(self.ffn_dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.ffn_norm(x + self.ffn_dropout2(ffn_output))\n",
    "        return x\n",
    "    \n",
    "    def get_attention_maps(self, tgt, enc_out, tgt_mask = None, enc_mask = None,\n",
    "                           tgt_is_causal = True, enc_out_is_causal = False):\n",
    "        _, self_attn_maps = self.self_attn(tgt, tgt, tgt, attn_mask = tgt_mask,\n",
    "                                          is_causal = tgt_is_causal, return_attn_weights = True)\n",
    "        _, cross_attn_maps = self.cross_attn(tgt, enc_out, enc_out, attn_mask = enc_mask,\n",
    "                                            is_causal = enc_out_is_causal, return_attn_weights = True)\n",
    "        return self_attn_maps, cross_attn_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Transformer model, with utility to get attention maps\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(d_vocab, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) \n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        self.output_linear = nn.Linear(d_model, d_vocab)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask = None, tgt_mask = None, src_is_causal = False, tgt_is_causal = True):\n",
    "        # Embed and add positional encoding to source\n",
    "        src = self.input_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder:\n",
    "            src = layer(src, src_mask, src_is_causal)\n",
    "        enc_out = src\n",
    "        # Embed and add positional encoding to target\n",
    "        tgt = self.input_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder:\n",
    "            tgt = layer(tgt, enc_out, tgt_mask, None, tgt_is_causal, src_is_causal)\n",
    "        # Output linear transformation & softmax\n",
    "        output = self.output_linear(tgt)\n",
    "        output = F.softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def get_attention_maps(self, src, tgt, src_mask = None, tgt_mask = None, src_is_causal = False, tgt_is_causal = True):\n",
    "        # Get attention maps from all encoder and decoder layers\n",
    "        enc_self_attn_maps = []\n",
    "        dec_self_attn_maps = []\n",
    "        dec_cross_attn_maps = []\n",
    "        \n",
    "        # Embed and add positional encoding to source\n",
    "        src = self.input_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder:\n",
    "            attn_map = layer.get_attention_maps(src, src_mask, src_is_causal)\n",
    "            enc_self_attn_maps.append(attn_map)\n",
    "            src = layer(src, src_mask, src_is_causal)\n",
    "        enc_out = src\n",
    "        \n",
    "        # Embed and add positional encoding to target\n",
    "        tgt = self.input_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder:\n",
    "            self_attn_map, cross_attn_map = layer.get_attention_maps(tgt, enc_out, tgt_mask, None, tgt_is_causal, src_is_causal)\n",
    "            dec_self_attn_maps.append(self_attn_map)\n",
    "            dec_cross_attn_maps.append(cross_attn_map)\n",
    "            tgt = layer(tgt, enc_out, tgt_mask, None, tgt_is_causal, src_is_causal)\n",
    "        \n",
    "        return enc_self_attn_maps, dec_self_attn_maps, dec_cross_attn_maps    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33ba870",
   "metadata": {},
   "source": [
    "Finally, we also include the CosineWarmUp learning rate scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + math.cos(math.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf672f9",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# List reversing, continued\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0ba9c",
   "metadata": {},
   "source": [
    "Continuing the list reversing example from before, we wil start by training a model in the same way as last time, and then we will look at the attention weights to see if they tell us anything interesting about what the model has learned. This roughly follows the [UvA tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html).\n",
    "\n",
    "To start with, we will reproduce the dataset code and provide a model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af093182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset generation\n",
    "class ReverseDataset(data.Dataset):\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "\n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels\n",
    "    \n",
    "# Create dataset\n",
    "torch.manual_seed(0)\n",
    "n_categories = 10\n",
    "seq_length = 16\n",
    "dataset = ReverseDataset(num_categories=n_categories, seq_len=seq_length, size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8347ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train/test datasets and create DataLoaders:\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_train = 50000\n",
    "n_test = 10000\n",
    "train_dataset, test_dataset = data.random_split(dataset, [n_train, n_test])\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last = True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReversePredictor class definition, modified to have a get_attention_maps method\n",
    "class ReversePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_classes, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, d_model*2, dropout) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "    \n",
    "    def get_attention_maps(self, src):\n",
    "        attn_maps = []\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.transformer:\n",
    "            attn_map = layer.get_attention_maps(x)\n",
    "            attn_maps.append(attn_map)\n",
    "            x = layer(x)\n",
    "        return attn_maps\n",
    "    \n",
    "# Instantiate your model\n",
    "model = ReversePredictor(\n",
    "    input_dim=n_categories,\n",
    "    d_model = 32,\n",
    "    n_heads = 1,\n",
    "    n_classes = n_categories,\n",
    "    n_layers = 1,\n",
    "    dropout = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b001d14b",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Replicate your training loop from the previous notebook to train the model. Use the same training parameters, as follows:\n",
    "\n",
    "* Train for 10 epochs\n",
    "\n",
    "* The Adam optimizer, with a learning rate of 0.001\n",
    "\n",
    "* CosineWarmUp learning rate scheduler, with `warmup` = 50 and `max_iters` = `n_epochs` $\\times$ `len(train_loader)`\n",
    "\n",
    "* Use CrossEntropyLoss\n",
    "\n",
    "Your training loop should be the same as normal, with the exception of stepping the learning rate scheduler after processing all batches in an epoch. \n",
    "\n",
    "**Note**: when we get the output from the model it will have shape (`batch_size`, `seq_length`, `n_categories`), and the target will have shape (`batch_size`, `seq_length`). `CrossEntropyLoss` requires the model output have shape (`batch_size`, `n_categories`, `seq_length`), so make sure to transpose the correct axes before passing through the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77867816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your training code here\n",
    "\n",
    "n_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineWarmupScheduler(optimizer, warmup=50, max_iters = n_epochs * len(train_loader))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_correct = 0\n",
    "    for _, (data, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.transpose(1,2), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_correct += (output.argmax(dim=-1) == targets).sum().item()\n",
    "    scheduler.step()\n",
    "    accuracy = n_correct / (len(train_loader.dataset) * seq_length)\n",
    "    return total_loss / len(train_loader), accuracy\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78848a18",
   "metadata": {},
   "source": [
    "Now that we have a trained model, let's visualise the attention weights for our encoder self-attention, to see what the model has learned. First, we need to get the attention maps out from our trained model, for one sample from our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38462321",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target = test_dataset[0]\n",
    "attn_maps = model.get_attention_maps(sample.unsqueeze(0)) # need unsqueeze to add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcab9f",
   "metadata": {},
   "source": [
    "Now, let's define a function to visualise attention maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a41ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention_map(attn_maps, input_data, **plot_kwargs):\n",
    "    attn_maps = [m.detach().squeeze(0) for m in attn_maps] # remove batch dimension\n",
    "    if input_data is not None:\n",
    "        input_data = input_data.detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = list(range(attn_maps[0].shape[-1]))\n",
    "    \n",
    "    num_heads, seq_length, _ = attn_maps[0].shape\n",
    "    num_layers = len(attn_maps)\n",
    "    fig_size = 6 if num_heads == 1 else 4.5\n",
    "    fig, axs = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size), dpi = 300)\n",
    "    if num_layers == 1:\n",
    "        axs = [axs]\n",
    "    if num_heads == 1:\n",
    "        axs = [[a] for a in axs]\n",
    "    for row in range(num_layers):\n",
    "        for col in range(num_heads):\n",
    "            ax = axs[row][col]\n",
    "            im = ax.imshow(attn_maps[row][col].view(seq_length, seq_length), **plot_kwargs, origin = 'lower', vmin = 0)\n",
    "            ax.set_xticks(list(range(seq_length)))\n",
    "            ax.set_xticklabels(list(range(seq_length)), fontsize = 12)\n",
    "            ax.set_yticks(list(range(seq_length)))\n",
    "            ax.set_yticklabels(list(range(seq_length)), fontsize = 12)\n",
    "            ax.set_xlabel('Sequence index',fontsize = 16)\n",
    "            ax.set_ylabel('Sequence index',fontsize = 16)\n",
    "            ax.set_title(f'Layer {row + 1}, Head {col + 1}',fontsize = 24)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_attention_map(attn_maps, sample, cmap='viridis')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50501d38",
   "metadata": {},
   "source": [
    "What can we see from this? Consider sequence index 0 on the x-axis; the highest attention weight for that sequence element is for the sequence element at index 15, i.e. the opposite end of the list. \n",
    "\n",
    "In fact, the off-diagonal structure (i.e. on the diagonal from top-left to bottom-right) tells us for a given sequence element, the highest attention weight is for the sequence element in the reversed index, exactly as we would hope to see for this problem. This gives us a nice view of how attention works and what it might mean physically, although in general attention weights will be much less obviously interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd847cf",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# LHC jet tagging using transformers <a id='jet-tagging'></a> [^](#index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60075b6f",
   "metadata": {},
   "source": [
    "Now that we've seen an example of using transformers, let's try tackling a real-life problem from particle physics.\n",
    "\n",
    "At the LHC, the products of particle collisions are not directly detected, but instead form jets of additional particles. One of the key stages in analysis of data collected is to identify what type of particle initiated a jet, so individual particle events can be isolated for analyses. \n",
    "\n",
    "To identify the original particle, the information we have to work with is generally some characteristics of the jet, and measurements of the particles produced by the jet. \n",
    "\n",
    "To tackle this problem, we will work with a dataset of high transverse momentum (i.e. high momentum perpendicular to the beamline at the LHC) jets produced by simulations of proton-proton collisions. This dataset is a common benchmark dataset for fast ML applications, necessary for triggering to decide what data should be kept or not.\n",
    "\n",
    "We will treat this data like a sequence of particles from the jet, with three features per particle describing the magnitude of the particle momentum and its direction of motion. While we have up to 150 particles available per jet, we will use a maximum of 32. You can read more about this dataset in the [corresponding paper](https://arxiv.org/pdf/1709.08705).\n",
    "\n",
    "The slide below is from a presentation using a transformer model with this dataset for jet tagging, given by Lauri Laatu at EPS-HEP 2025. You can see the presentation slides [here](https://indico.in2p3.fr/event/33627/contributions/154676/).\n",
    "\n",
    "<center>\n",
    "<img src='jet-tag-slide.png' width=900/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd6190",
   "metadata": {},
   "source": [
    "To start with, let's load in the data and inspect one sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "jet_tag_data = np.load('jet_tag_data.npy', allow_pickle=True).item()\n",
    "train_dataset = data.TensorDataset(torch.from_numpy(jet_tag_data['train']['X'].astype(np.float32)), torch.from_numpy(jet_tag_data['train']['y'].astype(np.int64)))\n",
    "test_dataset = data.TensorDataset(torch.from_numpy(jet_tag_data['test']['X'].astype(np.float32)), torch.from_numpy(jet_tag_data['test']['y'].astype(np.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = test_dataset[0]\n",
    "print(src.shape, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2391d",
   "metadata": {},
   "source": [
    "Each data sample is a sequence of length 32, where each sequence element has 3 features. The target is a numerical encoding of the jet class. For simplicity, we have restricted this data sample to just two classes, corresponding to top quark and QCD events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745fa76",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now that we have the data loaded in, you should define DataLoaders for the train and test datasets respectively. Use a batch size of 128 and make sure the train DataLoader shuffles the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d77845",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now, let's train a transformer encoder on this data. Use the following architecture for your transformer:\n",
    "\n",
    "* Input embedding layer to $d_\\text{model}$ = 16; note because we have continuous values as our features, this needs to be a `nn.Linear` layer rather than `nn.Embedding`\n",
    "\n",
    "* Add positional encodings\n",
    "\n",
    "* 2 encoder layers, each with 2 heads and an FFN hidden dimension $d_{ff}$ = 16\n",
    "\n",
    "* After the encoder layers, take the mean of the outputs along `dim = 1`\n",
    "\n",
    "* A 2 layer output MLP with a ReLU activation function\n",
    "\n",
    "Train this model for 10 epochs, using the `Adam` optimizer, `CrossEntropyLoss` as your loss function, and a `CosineWarmUp` learning rate scheduler with `warmup = 20` and `max_iters` = `epochs * len(train_loader)`. Remember to step the scheduler after iterating through all the batches in the training epoch.\n",
    "\n",
    "After training, find the performance on the test dataset, including the classification accuracy. How well have you done?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f25461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition code\n",
    "\n",
    "class JetTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads, d_ff, n_classes, n_layers, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Sequential(nn.Linear(d_model, d_model),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(dropout),\n",
    "                                    nn.Linear(d_model, n_classes))\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        x_pooled = torch.mean(x, dim = 1)\n",
    "        out = self.fc_out(x_pooled)\n",
    "        return out\n",
    "    \n",
    "#Â Use the data to define seq_length\n",
    "num_particles, num_features = src.shape\n",
    "\n",
    "\n",
    "model = JetTransformer(input_dim = num_features,\n",
    "                       d_model = 16,\n",
    "                       n_heads = 2,\n",
    "                       d_ff = 16,\n",
    "                       n_classes = 2,\n",
    "                       n_layers = 2,\n",
    "                       dropout = 0.1)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c12c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & evaluation code\n",
    "\n",
    "# THIS MAY TAKE SOME TIME TO RUN; took 4.5 mins on Macbook Pro with M3 Pro processor\n",
    "\n",
    "def train_epoch_jet(model, train_loader, test_loader, optimizer, criterion, scheduler = None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_correct = 0\n",
    "    for _, (data, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_correct += (output.argmax(dim=-1) == targets).sum().item()\n",
    "    accuracy = n_correct / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = 0\n",
    "        test_n_correct = 0\n",
    "        for _, (data, targets) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            test_loss = criterion(output, targets)\n",
    "            total_test_loss += test_loss.item()\n",
    "            test_n_correct += (output.argmax(dim=-1) == targets).sum().item()\n",
    "        test_accuracy = test_n_correct / len(test_loader.dataset)\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(train_loader), accuracy, total_test_loss / len(test_loader), test_accuracy \n",
    "\n",
    "n_epochs = 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = CosineWarmupScheduler(optimizer, warmup=50, max_iters = n_epochs * len(train_loader))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc, test_loss, test_acc = train_epoch_jet(model, train_loader, test_loader, optimizer, criterion, scheduler)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2%}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec2304",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "If that didn't take too long to train, feel free to play about with the model hyperparameters to see if you can improve the performance. Try changing the number of heads, $d_{ff}$, $d_\\text{model}$, etc.\n",
    "\n",
    "Otherwise, you can instead try using a standard neural network for the same problem. Try the following architecture:\n",
    "\n",
    "* Input linear layer of `num_particles * num_features` to $d_\\text{model}$ = 16, and flatten the inputs after the batch dimension so you can properly use the linear model\n",
    "\n",
    "* Hidden layer from 16 inputs to 32 outputs\n",
    "\n",
    "* Output layer from 32 inputs to 2 outputs, for the final classes\n",
    "\n",
    "* Use ReLU activation functions throughout\n",
    "\n",
    "Evaluate the test performance and compare it to your transformer. How does it compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN model code\n",
    "\n",
    "class JetFFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        self.activ = nn.ReLU()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i-1], hidden_dims[i]) for i in range(1, len(hidden_dims))])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = src.view(src.size(0), -1)  # Flatten the input\n",
    "        x = self.activ(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activ(layer(x))\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "    \n",
    "ffn_model = JetFFN(input_dim = num_particles * num_features,\n",
    "                   hidden_dims = [16, 32],\n",
    "                   output_dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN training\n",
    "n_epochs = 10\n",
    "ffn_train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "ffn_test_loader = data.DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "ffn_optimizer = optim.Adam(ffn_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc, test_loss, test_acc = train_epoch_jet(ffn_model, ffn_train_loader, ffn_test_loader, ffn_optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2%}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cb6626",
   "metadata": {},
   "source": [
    "You are likely to see that the FFN performs similarly (but slightly worse), but takes a lot less time to train. This is because we have used a much simplified variant of this dataset for the purposes of time, here; if we were to try training on the full dataset, we might have a harder time with the FFN vs the transformer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
