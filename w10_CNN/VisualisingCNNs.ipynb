{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10172542-bf25-4efc-bf2e-dad7b1d0337d",
   "metadata": {},
   "source": [
    "# Visualising CNNs\n",
    "\n",
    "### Introduction to Pretrained PyTorch models \n",
    "### Based on lecture by Dr. Antonin Vacheret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fc4613",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Index: <a id='index'></a>\n",
    "1. [Pre-trained Legacy computer vision classifier models](#PTL)\n",
    "1. [AlexNet Model](#ANM)\n",
    "1. [Resnet 101](#101)\n",
    "1. [Convolution](#LTA)\n",
    "1. [Visualise CNN](#CNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739023fe-e01f-441c-8453-e7321168edee",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "A quick run through some basics of pyTorch starting from a quick exploration of the models readily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b573018-6238-4fac-8a73-762f825dcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6020b-3b42-472d-ab94-c3fe000b85d4",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## I. Pre-trained Legacy computer vision classifier models [^](#index)\n",
    "<a id='PCL'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cbece-348b-4b08-b44d-cc2016876c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122f1ef-4db2-4f65-be38-0eb16ebb86d5",
   "metadata": {},
   "source": [
    "This is the famous AlexNet [^](#index) <a id='ANM'></a> model that shaked the field of machine learning in 2012:\n",
    "https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "\n",
    "Note: the lowercase models have fixed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee1823-fb81-4623-9cdd-80bfa76bcf34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alexnet_function = models.AlexNet() # this is the \"empty shell\" of Alexnet\n",
    "alexnet_trained = models.alexnet(pretrained=True) # fixed artchitecture already pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4bd96d-9156-4012-af29-8765cf16cbec",
   "metadata": {},
   "source": [
    "This one is **Resnet 101** [^](#index) <a id='101'></a> which stands for **residual network**. This one is the 101 layer version.\n",
    "https://arxiv.org/abs/1512.03385\n",
    "It has beaten several benchmark in 2015 and started the deep learning revolution. It is trained on imagenet with 1.2M images on 1000 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438cb0d-d6b3-436f-9569-4f5f46cf17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet101(pretrained=True) # beware this is taking on average a few mins to download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad10bd",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Convolution [^](#index)\n",
    "<a id='LTA'></a>\n",
    "\n",
    "*From homl...*\n",
    "\n",
    "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s. In the last few years, thanks to the increase in computational power, the amount of available training data for training deep nets, CNNs have man‐ aged to achieve superhuman performance on some complex visual tasks. They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful at many other tasks, such as voice recognition or natural language processing (NLP); however, we will focus on visual applications for now.\n",
    "\n",
    "In this chapter we will present where CNNs came from, what their building blocks look like, and how to implement them using TensorFlow and Keras. Then we will dis‐ cuss some of the best CNN architectures, and discuss other visual tasks, including object detection (classifying multiple objects in an image and placing bounding boxes around them) and semantic segmentation (classifying each pixel according to the class of the object it belongs to)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf14bfb-666a-4f53-9c4d-0a5f27681e83",
   "metadata": {},
   "source": [
    "Let's take a look at a high def picture of a dog. You can replace this one with your prefered one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf89bc5-0895-46c4-adfe-01e6f98d0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"img/mydoge.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abc99c-11d4-4314-86a6-11a5f586441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4988eb-a248-4688-99f0-561da20db560",
   "metadata": {},
   "source": [
    "Importing high-definition image from img folder but now defining some **transformation** first (a very powerful feature of pytorch!) to preprocess the image and get the right input size for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de002aa6-dae6-48b4-9662-46030784167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2f5fd-e8f7-4ca4-a09a-2ef13da1e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = preprocess(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab83669-2a26-44c7-a0ef-ebaf6f8bfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7cb35-bd52-49b3-819f-6113a1575cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_t[2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2c21-f439-4438-be7b-54824209946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2545531-aac5-4076-8246-0310e5b1beb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet.eval() # putting the model in inference mode (no training of the weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93452d2e-9081-405c-b800-e03c76f3c764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = resnet(batch_t)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276649d-9485-4a8e-8e73-e54c5359ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores  = out.detach().numpy()\n",
    "plt.plot(scores[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0dd58-b237-4d80-9c61-eeba3fd594c5",
   "metadata": {},
   "source": [
    "#### Now an operation involving a massive 44.5M parameters has just taken place !\n",
    "This has produced a vector of a 1000 score, one for each label of the imagenet training set. Let's get the file that has the imagenet list of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737885f-8b65-4db3-8d10-6461c957cd76",
   "metadata": {},
   "source": [
    "We need now to figure out what was the ranking for our dog picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec7f28-6893-4520-a8e9-46f28d10283f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/data/imagenet_classes.txt') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8e8e5-4409-4386-8010-66cbd20be7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, index = torch.max(out, 1) # this returns the value and index of the higest score\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f9c2b-63e6-4821-a530-c58fd2a166f7",
   "metadata": {},
   "source": [
    "**Resnet** gives us a score, but what we are interested in is more something like a the probability of being of a certain category. We will use the **softmax function** for that (multi-class classifier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487aeef-9ad7-4511-b4b2-c668e3ec3e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] # only one dimension, [0] is to return one value.\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26168c52-8190-46ef-be3a-5f8d6ea0ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[index[0]], percentage[index[0]].item() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee1ee4-906d-4263-b950-629dba65c5a3",
   "metadata": {},
   "source": [
    "Exercises:\n",
    "\n",
    "* Sort the output so the five highest probabilities come out from the resnet outpout\n",
    "    \n",
    "* Dowload alexnet and look at the output for our dog image. Which model is best ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8a895",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## I-a. Visualize CNNs [^](#index)\n",
    "<a id='CNN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdac30",
   "metadata": {},
   "source": [
    "The hidden `Conv2d` layers are able to extracted by the following part. They and Their weights are stored in `conv_layers` and `weights`. Most of the `Conv2d` layers are contained in `Sequential`, those layers are extracted as `grandchildren`.\n",
    "If you want to check outputs from `MaxPool2d`, plase replace `nn.Conv2d` to `nn.MaxPool2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = []\n",
    "conv_layers = []\n",
    "maxpooling_layers = []\n",
    "resnet_children=list(resnet.children())\n",
    "for children in resnet_children:\n",
    "    if (type(children) == nn.Conv2d) or (type(children) == nn.MaxPool2d):\n",
    "        #print(children)\n",
    "        #weights.append(children.weight)\n",
    "        conv_layers.append(children)\n",
    "    elif type(children) == nn.Sequential:\n",
    "        for gen in list(children):\n",
    "            for grandchildren in list(gen.children()):\n",
    "                if (type(grandchildren) == nn.Conv2d) or (type(grandchildren) == nn.MaxPool2d):\n",
    "                    #print(type(grandchildren))\n",
    "                    #weights.append(grandchildren.weight)\n",
    "                    conv_layers.append(grandchildren)\n",
    "#print('len(weights):', len(weights))\n",
    "print('len(conv_layers):', len(conv_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64635a",
   "metadata": {},
   "source": [
    "The feature maps for `batch_t` are obtained in the following part. The outputs from each `Conv2d` layer are stored in `outputs_from_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_from_layer = []\n",
    "img_from_prev_layer = batch_t # a tensor containing a batch of image data\n",
    "\n",
    "for layer in conv_layers:\n",
    "    img_from_prev_layer = layer(img_from_prev_layer)\n",
    "    outputs_from_layer.append(img_from_prev_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8538bf",
   "metadata": {},
   "source": [
    "The followings are example of the visualized feature maps.\n",
    "   * 1st Conv2d layer\n",
    "      * All 64 filters. \n",
    "      * The most active filter and the least active filter.\n",
    "   * 50th Conv2d layer\n",
    "      * Picked up 64 filters. \n",
    "      * The most active filter and the least active filter.\n",
    "   * 99th (last) Conv2d layer\n",
    "      * Picked up 64 filters. \n",
    "      * The most active filter and the least active filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c10389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature maps of the first Conv2d layer\n",
    "# There are 64 filters\n",
    "\n",
    "layer_number = 0\n",
    "feature_maps = outputs_from_layer[layer_number].detach().numpy()\n",
    "figs, axes = plt.subplots(8, 8, figsize=[16,16])\n",
    "for i in range(feature_maps.shape[1]):\n",
    "    feature_map = feature_maps[0,i,:,:]\n",
    "    axes[int(i/8), int(i%8)].set_title('idx: {0}'.format(i))\n",
    "    axes[int(i/8), int(i%8)].imshow(feature_map)\n",
    "\n",
    "plt.tight_layout()\n",
    "figs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0286318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most active filter in the first Conv2d layer\n",
    "layer_number = 0\n",
    "fmaps = outputs_from_layer[layer_number].detach().numpy()\n",
    "\n",
    "output_from_filters = fmaps.sum(axis=3).sum(axis=2)\n",
    "idx_max = output_from_filters.argmax()\n",
    "max = output_from_filters.max()\n",
    "idx_min = output_from_filters.argmin()\n",
    "min = output_from_filters.min()\n",
    "\n",
    "print('Max, idx: ', max, idx_max)\n",
    "print('Min, idx: ', min, idx_min)\n",
    "\n",
    "img_max = fmaps[0,idx_max,:,:]\n",
    "img_min = fmaps[0,idx_min,:,:]\n",
    "\n",
    "figs, axes = plt.subplots(1,2, figsize=[8,16])\n",
    "axes[0].set_title('Max, idx {0}'.format(idx_max))\n",
    "axes[0].imshow(img_max)\n",
    "axes[1].set_title('Min, idx {0}'.format(idx_min))\n",
    "axes[1].imshow(img_min)\n",
    "figs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ceeab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature maps of the 20th Conv2d layer\n",
    "# Pickup 128 filters\n",
    "\n",
    "pickup_idx = [2*x for x in range(64)]\n",
    "layer_number = 20\n",
    "feature_maps = outputs_from_layer[layer_number].detach().numpy()\n",
    "figs, axes = plt.subplots(8, 8, figsize=[16,16])\n",
    "for i in pickup_idx:\n",
    "    feature_map = feature_maps[0,i,:,:]\n",
    "    axes[int(i/2/8), int(i/2%8)].set_title('idx: {0}'.format(i))\n",
    "    axes[int(i/2/8), int(i/2%8)].imshow(feature_map)\n",
    "\n",
    "plt.tight_layout()\n",
    "figs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most active filter in the 20th layer\n",
    "layer_number = 20\n",
    "fmaps = outputs_from_layer[layer_number].detach().numpy()\n",
    "\n",
    "output_from_filters = fmaps.sum(axis=3).sum(axis=2)\n",
    "idx_max = output_from_filters.argmax()\n",
    "max = output_from_filters.max()\n",
    "idx_min = output_from_filters.argmin()\n",
    "min = output_from_filters.min()\n",
    "\n",
    "print('Max, idx: ', max, idx_max)\n",
    "print('Min, idx: ', min, idx_min)\n",
    "\n",
    "img_max = fmaps[0,idx_max,:,:]\n",
    "img_min = fmaps[0,idx_min,:,:]\n",
    "\n",
    "figs, axes = plt.subplots(1,2, figsize=[8,16])\n",
    "axes[0].set_title('Max, idx {0}'.format(idx_max))\n",
    "axes[0].imshow(img_max)\n",
    "axes[1].set_title('Min, idx {0}'.format(idx_min))\n",
    "axes[1].imshow(img_min)\n",
    "figs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature maps of the last Conv2d layer\n",
    "# There are 2048 filters\n",
    "\n",
    "pickup_idx = [32*x for x in range(64)]\n",
    "layer_number = 100\n",
    "feature_maps = outputs_from_layer[layer_number].detach().numpy()\n",
    "figs, axes = plt.subplots(8, 8, figsize=[16,16])\n",
    "for i in pickup_idx:\n",
    "    feature_map = feature_maps[0,i,:,:]\n",
    "    axes[int(i/32/8), int(i/32%8)].set_title('idx: {0}'.format(i))\n",
    "    axes[int(i/32/8), int(i/32%8)].imshow(feature_map)\n",
    "\n",
    "plt.tight_layout()\n",
    "figs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa09518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most active filter in the 100th layer\n",
    "layer_number = 100\n",
    "fmaps = outputs_from_layer[layer_number].detach().numpy()\n",
    "\n",
    "output_from_filters = fmaps.sum(axis=3).sum(axis=2)\n",
    "idx_max = output_from_filters.argmax()\n",
    "max = output_from_filters.max()\n",
    "idx_min = output_from_filters.argmin()\n",
    "min = output_from_filters.min()\n",
    "\n",
    "print('Max, idx: ', max, idx_max)\n",
    "print('Min, idx: ', min, idx_min)\n",
    "\n",
    "img_max = fmaps[0,idx_max,:,:]\n",
    "img_min = fmaps[0,idx_min,:,:]\n",
    "\n",
    "figs, axes = plt.subplots(1,2, figsize=[8,16])\n",
    "axes[0].set_title('Max, idx {0}'.format(idx_max))\n",
    "axes[0].imshow(img_max)\n",
    "axes[1].set_title('Min, idx {0}'.format(idx_min))\n",
    "axes[1].imshow(img_min)\n",
    "figs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4066c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
