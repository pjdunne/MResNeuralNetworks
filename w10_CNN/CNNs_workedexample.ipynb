{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index: <a id='index'></a>\n",
    "1. [What is a CNN?](#what)\n",
    "1. [Features of Convolutional Neural Networks](#features)\n",
    "1. [Stopping the Growth, Stride and Pooling](#stopping)\n",
    "1. [Building and Training an Image Classifier](#building)\n",
    "1. [Training on GPU](#training)\n",
    "    1. [Training on Multiple GPUs](#multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial borrows heavily from the following tutorials:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "https://github.com/pytorch/examples/blob/main/mnist\n",
    "\n",
    "More information on the syntax of putting together models in pytorch, including how to declare each of the layer types, can be found here:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html\n",
    "\n",
    "A lot of the instructional material at the beginning of the notebook is from here:\n",
    "\n",
    "https://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## What is a CNN? <a id='what'></a>\n",
    "\n",
    "Let's imagine using a conventional neural network to classify large images. To use all the information each pixel in our image would be an input variable to our model. For colour images we'd need to multiply that by 3 for the red, green and blue channels.\n",
    "\n",
    "Calculate how many weights you had to train in the 512 neuron first layer of the network we looked at last time. As a reminder that network had 28x28 gray-scale images as its input. Now calculate how many weights you'd have to train for a 1920x1080 colour image with 512 fully connected neurons in its first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate some big numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are usually not looking for underlying behaviour with that many degrees of freedom so if you did manage to train that many parameters, you would likely overfit.\n",
    "\n",
    "### What's the solution?\n",
    "We know that our pixels are likely to share structure with the pixels near them in the image. Let's use this and structure our model differently. \n",
    "\n",
    "Regular neural nets are made up of several successive fully connected hidden layers that take all of the neurons in the previous layer as their input. \n",
    "\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/nn1/neural_net2.jpeg\"/>\n",
    "\n",
    "Convolutional neural networks are made up of **'volumes'**. These have **height** and **width** as you might expect from a 2D image. They also have **'depth'**. This depth is different from what we meant when we talked about the number of layers in a conventional neural network.\n",
    "\n",
    "At the input layer this depth is often 3, to account for the fact that there are red, green and blue channels in the image.\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/cnn/cnn.jpeg\" alt=\"Alternative text\" />\n",
    "\n",
    "Now we have our input organised into this volume, we need to know what we do to it to go to the next layer and learn something about our image.\n",
    "\n",
    "### **Convolutions**\n",
    "The core operation of a convolutional neural network is the convolution. We can view our volume that we defined in the previous section as a 3D matrix. What convolutions do is to take the 3D matrix and combine it with another 3D matrix called a 'kernel'. A 2D example of convolution is shown below.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/720/1*r13ZUdVTQwVuhDPmo3JKag.png\" alt=\"From: https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939\" />\n",
    "\n",
    "In simple terms, if your kernel has size NxN, it takes all of the pixels in an NxN grid around a point in your image and performs a weighted sum. You then move it one pixel over and take the same weighted sum, over and over again until you've done it to the whole image.\n",
    "\n",
    "In a convolutional neural network, your weights that you must train are the entries in the kernel. \n",
    "\n",
    "Kernel convolution is not only used in CNNs, but is also a key element of many other Computer Vision algorithms. In summary it is a process where we take a small matrix of numbers (called kernel or filter), we pass it over our image and transform it based on the values from filter. Subsequent feature map values are calculated according to the following formula, where the input image is denoted by f and our kernel by h. The indexes of rows and columns of the result matrix are marked with m and n respectively. The convolution can be explained using eqn:\n",
    "\n",
    "$$G[m,n] = (f * h)[m,n] = \\sum_{j}\\sum_{k} h[j,k] f[m-j, n-k]$$\n",
    "\n",
    "A more visual representation can be shown by:\n",
    "\n",
    "![convolution](https://miro.medium.com/v2/resize:fit:1400/1*32zCSTBi3giSApz1oQV-zA.gif)\n",
    "\n",
    "Let's see what certain kernels do to images to see why this sort of operation might be useful.\n",
    "\n",
    "To start we'll need to get some images to work with. We'll use pytorch's torchvision package to download an image from the Caltech 101 database of images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    print(npimg)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "kerneltestset = torchvision.datasets.Caltech101(root='./data', \n",
    "                                        download=True, transform=transform)\n",
    "kerneltestloader = torch.utils.data.DataLoader(kerneltestset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "#transform = transforms.Compose([transforms.ToTensor(), \n",
    "#                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "kerneltestset = torchvision.datasets.Caltech101(root='./data', \n",
    "                                        download=True, transform=transform)\n",
    "kerneltestloader = torch.utils.data.DataLoader(kerneltestset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "kerneltestiter = iter(kerneltestloader)\n",
    "images, labels = next(kerneltestiter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our image through some example convolutions. There's a good list of things to try here:\n",
    "https://en.wikipedia.org/wiki/Kernel_(image_processing) and I'd also try a Sobel edge detection kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nb_channels = 3\n",
    "\n",
    "#The tensor here is the kernel we'll apply to the image\n",
    "\n",
    "weights = torch.tensor([[-1., 0., 1.],\n",
    "                        [-2., 0., 2.],\n",
    "                        [-1., 0., 1.]])\n",
    "\n",
    "\n",
    "weights = weights.view(1, 1, 3, 3).repeat(1, nb_channels, 1, 1)\n",
    "\n",
    "conv = nn.Conv2d(3, 1,3,1,bias=False,padding=1)\n",
    "conv.weight=torch.nn.Parameter(weights)\n",
    "\n",
    "output = conv(images)\n",
    "\n",
    "print(\"Input\")\n",
    "imshow(torchvision.utils.make_grid(images, normalize=True, nrow=3))\n",
    "print(\"Output\")\n",
    "imshow(torchvision.utils.make_grid(output, normalize=True,nrow=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Features of Convolutional Neural Networks <a id='features'></a>\n",
    "\n",
    "### Translational invariance\n",
    "One important aspect of convolutions is that they're 'translationally invariant'. Because the kernel only ever acts locally on one part of the image, it doesn't care where in the image a particular pattern is located. i.e. If the picture above was shifted 10 pixels to the right, you'd still get the same output, just also shifted 10 pixels to the right. That means if you find a kernel that is good at identifying a particular feature, it will find that feature wherever it is in the image.\n",
    "\n",
    "### It's still a neural network\n",
    "Convolutional neural networks are still neural networks, so you still have to put an activation function after each layer to make sure the result couldn't just be simplified into a linear transformation.\n",
    "\n",
    "### The output of each layer can be much larger than the one before\n",
    "If you have a single kernel then you end up with a volume the same size as the one you started. However, you can also have several kernels per layer in which case your volume grows by a factor of the number of kernels you use.\n",
    "\n",
    "In the image above of our volume, you can see that the second layer has a much bigger depth than the first, and this is exactly because one usually has several kernels per layer. Next we'll look at how to deal with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Stopping the Growth, Stride and Pooling <a id='stopping'></a>\n",
    "We started off with CNNs because we didn't want a huge number of variables in our layers. However, we see above that our CNN will grow by a factor of the number of kernels we use every layer. If we used 10 kernels and had a 5 layer network this would be a very large increase in size by the final layer. We also eventually need to reduce the size of our network down to our desired output.\n",
    "\n",
    "There are three main ways to do this:\n",
    "* Have a standard 'fully connected' neural network layer at the end of the model\n",
    "* Increase the 'stride'\n",
    "* Use 'pooling'\n",
    "\n",
    "The first option has exactly the same problems we were trying to get away from to start with, so we'll need to try something else first.\n",
    "\n",
    "### Stride\n",
    "Instead of moving your kernel across the image one pixel at a time, you move N pixels at a time. This results in the height and width of your output volume being a factor of N smaller. However, if the stride is too big you could miss features\n",
    "\n",
    "### Pooling\n",
    "Instead of changing how you do the convolution, pooling takes the output from a layer of the network and processes the volume to reduce its size before passing it onto the next layer. Typically this works by splitting the image up into small blocks or 'pools' and performing a mathematical operation on each pool that results in a single number.\n",
    "\n",
    "The most common pooling is 'max pooling' where you just take the largest value in the pool, as shown below:\n",
    "\n",
    "<img src=\"https://cs231n.github.io/assets/cnn/maxpool.jpeg\" height=\"50\" />\n",
    "\n",
    "This method has the advantage that if your kernel has found a very localised feature, you won't dilute its response to that feature.\n",
    "\n",
    "Other options are 'average pooling' where you just take the mean, but this is less favoured as max pooling generally gives better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Building and Training an Image Classifier <a id='building'></a>\n",
    "\n",
    "Let's put all of this together into a model. We'll use the CIFAR10 dataset which has smaller images than the Caltech101 set that we used above. We will do the following steps in order:\n",
    "\n",
    "1. Load and normalize some the training and test datasets from the CIFAR10 repository using\n",
    "   ``torchvision``\n",
    "2. Define a Convolutional Neural Network\n",
    "3. Define a loss function\n",
    "4. Train the network on the training data\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and normalize CIFAR10\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data,\n",
    "you can use standard python packages that load data into a numpy array.\n",
    "Then you can convert this array into a ``torch.*Tensor``.\n",
    "\n",
    "Specifically for vision, pytorch has a package called\n",
    "``torchvision``, that has data loaders for common datasets such as\n",
    "ImageNet, CIFAR10, MNIST, etc. and data transformers for images, e.g.,\n",
    "``torchvision.datasets`` and ``torch.utils.data.DataLoader``. This avoids writing boilerplate code.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset.\n",
    "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
    "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of\n",
    "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "We should transform them to Tensors of normalized range [-1, 1].\n",
    "\n",
    "We've put the training set and its loader in for you. Add the test set below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of the training images (remember that you applied normalisation to the original images that will need undoing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a Convolutional Neural Network\n",
    "Define a convolutional neural network for 3-channel images that has two convolution layers, separated by a 2x2 max pooling layer, then three fully connected linear layers. Your first layer should have an output size of 6 and use 5x5 kernels. The second layer should have 16 output channels and also use 5x5 kernels. The linear layers should have output sizes of 120, 84 and 10 respectively.\n",
    "\n",
    "All layers should use a relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define a Loss function and optimizer\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the network\n",
    "\n",
    "Write the training loop for your network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly save our trained model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://pytorch.org/docs/stable/notes/serialization.html)\n",
    "for more details on saving PyTorch models.\n",
    "\n",
    "### 5. Test the network on the test data\n",
    "\n",
    "We have trained the network for 2 passes over the training dataset.\n",
    "But we need to check if the network has learnt anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network\n",
    "outputs, and checking it against the ground-truth. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display an image from the test set to get familiar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load back in our saved model (note: saving and re-loading the model\n",
    "wasn't necessary here, we only did it to illustrate how to do so):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are energies for the 10 classes.\n",
    "The higher the energy for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index of the highest energy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results seem pretty good.\n",
    "\n",
    "Now write a function to see how the network performed on the full dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks way better than chance, which is 10% accuracy (randomly picking\n",
    "a class out of 10 classes).\n",
    "Seems like the network learnt something.\n",
    "\n",
    "Let's write a test to see what are the classes that performed well, and the classes that did\n",
    "not perform well:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so what next?\n",
    "\n",
    "Try adjusting the parameters of the network (number of layers, type of layers, max vs average pooling etc.) and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Your code here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a web browser based tool that lets you play around with some of the hyperparameters of a CNN model trained on the CIFAR-10 dataset here: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\n",
    "\n",
    "For information I've also included information below on training neural networks on GPUs\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "## Training on GPU <a id='training'></a>\n",
    "Just like how you transfer a Tensor onto the GPU, you transfer the neural\n",
    "net onto the GPU.\n",
    "\n",
    "Let's first define our device as the first visible cuda device if we have\n",
    "CUDA available:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "The rest of this section assumes that ``device`` is a CUDA device.\n",
    "\n",
    "Then these methods will recursively go over all modules and convert their\n",
    "parameters and buffers to CUDA tensors:\n",
    "\n",
    "```python\n",
    "\n",
    "    net.to(device)\n",
    "```\n",
    "\n",
    "Remember that you will have to send the inputs and targets at every step\n",
    "to the GPU too:\n",
    "\n",
    "```python\n",
    "\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "```\n",
    "Why don't I notice MASSIVE speedup compared to CPU? Because your network\n",
    "is really small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try increasing the width of your network (argument 2 of\n",
    "the first ``nn.Conv2d``, and argument 1 of the second ``nn.Conv2d`` –\n",
    "they need to be the same number), see what kind of speedup you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals achieved**:\n",
    "\n",
    "- Understanding PyTorch's Tensor library and neural networks at a high level.\n",
    "- Train a small neural network to classify images\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "## Training on Multiple GPUs <a id='multiple'></a>\n",
    "If you want to see even more MASSIVE speedup using all of your GPUs,\n",
    "please check out :doc:`data_parallel_tutorial`.\n",
    "\n",
    "### If you have a Mac with an arm Processor \n",
    "How to use the GPU/machine learning cores:\n",
    "https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9df1b77b0caf646f19509570eac5ef5a3592ebd6cb99175979cb74b7b24a8bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
