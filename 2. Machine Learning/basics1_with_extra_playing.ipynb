{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week X - Basics of Machine Learning (ML)\n",
    "\n",
    "## Preliminaries: Good Books for this course\n",
    "\n",
    "There are 3 books that I would recommend. The books are:\n",
    "\n",
    "1) [The one hundred-page Machine Learning book](http://themlbook.com/) by \n",
    "Andriy Burkov. This is the best conceptual book on ML that I have read. It also does a reasonable jobs of the maths. This book is free to try before you buy (a great idea).\n",
    "\n",
    "**then two O'REILLY books:**\n",
    "\n",
    "2) [Hands on machine learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by Aurélien Géron. This is the best hands on book that I have read and has sufficient detail to take you into the understanding of the algorithms. This book also has a good collection of the original papers cited and links to them on their website. There is also much information on the [github](https://homl.info). If you only buy one then this one covers the most. **If there was a textbook for this course, this would be it.**\n",
    "\n",
    "3) [Introduction to Machine Learning with python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/). This is a more introductory level than one above, however it is really well written and takes you through things more carefully and I find is very good. I tend to use the two as complementary.\n",
    "\n",
    "There are plenty of other good more theoretical ML books but the above are practical, but still having enough theory for you to understand them. In this course we only have time to teach you the basics and to touch on a few techniques. These books will take you further.\n",
    "\n",
    "**Note:** I will take examples from the last two of these books (on publically available data) and will reference them appropriately. For 2) I will write {homl} and for 1) I will write {imlp}.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section One: Types of Machine Learning\n",
    "\n",
    "In machine learning you build a model from the data that you have. There are many different sorts of machine learning and there are many different ways of categorising them.\n",
    "\n",
    "### Supervised, Unsupervised and Reinforcement Learning\n",
    "\n",
    "Most of the machine learning that we carry out in the physical sciences (and in the world at large) is **supervised learning**. This is were you build a model using data with a **known output**. The historical data is then used as way of trainint the model. **Unsupervised learning** is where you **don't have a known output** and you are trying to build a model based on other properties for example by clustering the data. You can also get semisupervised learning which is somehwere between the two where you have algorithms that cope with partially labelled data.\n",
    "\n",
    "**Reinforcemnt learning** is somewhat different in that it has agents that **perform a (sometimes complex) series of tasks** and learn to do the tasks well through a series of **trial and error**. If you have ever played a game against a ML based computer then is almost certainly how it was trained.\n",
    "\n",
    "### Classification and Regression\n",
    "\n",
    "The two main uses of ML in the physical sciences are classification and regression in supervised ML. In **classification** you are trying to **distinguish between different sorts entity** (say pictures of cats from dogs or the energy deposits left by electron versus muons in a detector) whereas in **regression** you are trying to predict **what a value will be** (say the return on an investment, or energy of a cosmic ray based on observation of scintillation light and the muons that it produced). Simply, classification tells you what it is whereas regression tells you what value it has.\n",
    "\n",
    "While these sound conceptually different the same approaches can often be used for both as you will.\n",
    "\n",
    "### Instance-based and Model-based Learning\n",
    "\n",
    "The purpose of ML is to be able to take the data that you have and generalise it to new data. In **instance-based learning** you simply look at the data that you have and in some way say that it is (either in classification or regression) **the same as the \"nearest\" (calculated in some way) data that you already have** to it. \n",
    "\n",
    "In **model-based learning** you take a few instances and build a **more general model**. This model is **refined** through **training and validation**. Model-based ML models often have **internal parameters that are not seen in the outside world -- these are called hyperparameters**.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "overfit.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Two: Challenges in ML\n",
    "\n",
    "This part is a paraphrase of what is said the same section of {homl}, I even pinch their example. They say it much more eloquently but mine is more succinct.\n",
    "\n",
    "### Insuficient Training Data\n",
    "\n",
    "Without the training data there is not much that you can do in the way of MLO. Often data sets of thousands of events are required to build a reasonable model. There are sometimes things you can do to lower the amount of data you need, for example by making your datasets as simple as possible -- but basically not enough data poor ML.\n",
    "\n",
    "### Unrepresentative (Biased) Training Data\n",
    "\n",
    "If there is a systematic bias in your training data then your model will not generalise well and will produce wrong results. Some form of regularisation can be used to mitigate this to some extent (see later).\n",
    "\n",
    "### Poor quality data\n",
    "\n",
    "If your data are noisy with lots of errors and outliers then your model is not going to be reliable. \n",
    "\n",
    "You may have missing fields (one of your instruments wasn't working that day) at which point you need to decide what to do about the missing data. There are a variety of approaches but two common ones are to set it to the median value so that it should have very little influence or to do the opposite and to set it to a value outside the normal range so that the model \"learns\" not to use it. There are pros and cons to both.\n",
    "\n",
    "### Irrelevant features\n",
    "\n",
    "If your data contains lots of data that is irrelevant it can sometimes swamp the model and mean that the data containing the relevant information is lost. An important part of ML is coming up with an appropriate and good set of features. This is sometimes called feature engineering. This can involve selecting only the most important data features  or combining features into more appropriate new features. For example if you have a feature that is a function of $r$, where $r=\\sqrt{x^2+y^2}$ it is better to use $r$ than $x$ and $y$ separately.\n",
    "\n",
    "### Overfitting the training data\n",
    "\n",
    "We came across something similar with minuit. \n",
    "\n",
    "This is generally when your model is more complex than the data you have can easily train. An example taken straight from {homl} is the figure below:\n",
    "![overfit.png](attachment:overfit.png)\n",
    "\n",
    "In generalyou should keep your model as simple as possible to fit the data well.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "As you can guess this is the opposite of overfitting and is when your model is too simple to fit the data that you have. For example if you are trying to fit your data with a linear model but really the data form a parabolic distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Validating\n",
    "\n",
    "The only way to know if your model has generalised well is to validate it on data. The most normal way to do that is to separate your data into training data and testing data. You use the training data to train your model and the testing data to test how well it has generalised. If there is a great difference in the performance  of the model on the training data compared to the performance on the testing data it shows that your model has been overtrained and is picking out features that are specific to your training data. \n",
    "\n",
    "*What fraction of your data should you use to train and what fraction to test?* There is no single answer to this question, but in most cases it tupically around 70:30 or 80:20 training:testing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Three: Simple Regression Algorithms\n",
    "\n",
    "Remeber that regression is trying to predict the value of some quantity by looking at a data set of similar data. Note that this is different from the fitting that your were performing with iminuit last week. There you had an implicit assumption that there is an underlying truth that the data would follow in the absence of noise and statistical effects, here all that is assumed is that these data vary in a similar way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pylab as pl\n",
    "\n",
    "x = 2 * np.random.rand(120, 1)\n",
    "y = 4 + 3 * x + np.random.randn(120, 1)\n",
    "z = x*x+y*np.random.randn(120,2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) # note that sklearn can split your data into samples for you.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pl.plot(x_train,y_train,\"*\")\n",
    "pl.plot(x_test,y_test,\"o\")\n",
    "pl.show()\n",
    "\n",
    "# looks a reasonable mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up a really simple regression algorithm and see what it would predict -- with different algorithm parameters. We will start off off with a k nearest neighbours regression algorithm. Look up the algorithm online it is used a lot more than you might think in the real world so it worth understanding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# pick values for k nearest neighbors and weighting.\n",
    "k=18 # number of NN taken into acount\n",
    "\n",
    "#weights=\"distance\" # how are the weighted\n",
    "weights=\"uniform\"\n",
    "weight=\"distance\"\n",
    "model=neighbors.KNeighborsRegressor(n_neighbors=k,weights=weights)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "# first lets see what line would predict from the training data \n",
    "xl=np.linspace(0,2,1000).reshape(-1,1)\n",
    "pl.plot(x_train,y_train,\"*\")\n",
    "pl.plot(x_test,y_test,\"o\")\n",
    "pl.plot(xl,model.predict(xl))\n",
    "\n",
    "# calculate the rms error for testing data\n",
    "pred=model.predict(x_test) #make prediction on test set\n",
    "error = np.sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "\n",
    "print(\"the error is\",error)\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, x_train, y_train, cv=3) # so this has 3 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (short)\n",
    "\n",
    "KNN is very useful if you just want a general parametisation and are not making assumptions about the linearity or otherwise of the fit.\n",
    "\n",
    "Turn the program above into a loop and investigate how well this algorithm does as a function of NN and weighting. Try changing the number of points may be from 120 to 1200 and see how things change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "\n",
    "Now, the the simple least squares linear model can be solved analytically as it is of closed form. {homl} will show you how to do that, however here we will just treat like another algorithm. Also this is nolonger the case when the number of entry points becomes very large as it becomes too computationally intensive. The different gradient decent methods are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train, y_train)\n",
    "print(lin_reg.intercept_, lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.plot(x_train,y_train,\"*\")\n",
    "pl.plot(xl,lin_reg.predict(xl))\n",
    "pred=lin_reg.predict(x_test) #make prediction on test set\n",
    "error = np.sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "\n",
    "print(\"the error is\",error)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "This is an example taken straight from {homl} however, like so much in that book I think that it is clearly explained (although my explanation is shorter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some data\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in reality I would probably not try to perform linear regression on this but would use another form of regression. Remember we are only trying to predict the values **NOT** trying to fit for an underlying physical model. However, this does provide some useful clues as to how to do things and so I am keeping this here.\n",
    "\n",
    "The first thing that you will notice is that what is done is to use sklearn.preprocessing to add additional features. In this case the squares of the of the numbers. The preprocessing functions in sklearn are really useful and powerful. Sadly we don't have time to go through them here but you should know that they exist. The description for this one can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) do take some time to look through them and familiarise your self with them.\n",
    "\n",
    "With this extra feature you can perform linear regression on the expanded data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "print(X_poly[1],X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "print(lin_reg.intercept_, lin_reg.coef_) # note that coeff now has 2 components as it is now a plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "y_new = lin_reg.predict(X_new_poly)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so what happens if you add more polynomial features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The code below shows how you can tell that there is overfitting going on - I will discuss these with you when we get to this stage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    plt.legend(loc=\"upper right\", fontsize=14)   # not shown in the book\n",
    "    plt.xlabel(\"Training set size\", fontsize=14) # not shown\n",
    "    plt.ylabel(\"RMSE\", fontsize=14)              # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "plt.axis([0, 80, 0, 3])                         # not shown in the book\n",
    "\n",
    "plt.show()                                      # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"lin_reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "plt.axis([0, 80, 0, 3])           # not shown\n",
    "\n",
    "plt.show()                        # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "In regularised regression you apply a cost function that stops your overfitting going out of control (or at least helps to constrain it). We don't have time to cover this in detail but you should be familiar  with the names so that you know what to look up if you need them (I very rarely have). Common forms are:\n",
    "\n",
    "* Ridge Regularisation\n",
    "\n",
    "* Lasso Regularisation\n",
    "\n",
    "* Elastic Net - somewhere between the other two)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Investigate the student data from last week using different forms of regression. First see how G3 varies with G1 and G2 and then how it works with a combined G1 & G2.\n",
    "\n",
    "The example [here](https://datatofish.com/multiple-linear-regression-python/) might be useful. I must confess that I haven't actually run this example but it looks similar to what I am looking for at quick glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
