{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31eeae81",
   "metadata": {},
   "source": [
    "# Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3df59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Index <a id='index'></a>\n",
    "1. [Introduction: what are graphs?](#what) <!-- Basic definitions, kinds of tasks -->\n",
    "1. [Graphs in Python with PyTorch Geometric](#python-graphs)\n",
    "1. [Intro to GNNs](#embeddings)\n",
    "1. [Building and training GNNs using PyTorch Geometric](#pyg-gnn) <!--Message passing, aggregation, prediction heads -->\n",
    "1. [Exercises](#exercises)\n",
    "1. [Appendices](#appendices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692c90f",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Introduction: what are graphs? <a id='what'></a>[^](#index)\n",
    "\n",
    "(Brief aside: there is a nice textbook on GNNs called \"Graph Representation Learning\", which has a prepublication draft available for free online at [https://www.cs.mcgill.ca/~wlh/grl_book/](https://www.cs.mcgill.ca/~wlh/grl_book/))\n",
    "\n",
    "So far in our study of neural networks, we have covered two extremes: fully connected neural networks, where every input to a layer is connected to every output, and convolutional neural networks, where only local points contribute to prediction at a given point. Somewhere between using global features and only using local features lies a method applicable to many problems, so long as we can structure the data in a particular way. This method is referred to as **graph neural networks** (**GNNs**). \n",
    "\n",
    "Before we can discuss GNNS, we first need to understand what we mean by graphs. In general, we use graphs to represent and solve problems where the data is composed of objects and relationships between them, both of which can be described in many ways.\n",
    "\n",
    "For the purposes of this notebook, we will deal with graphs that are composed of:\n",
    "\n",
    "* **nodes**: objects that generally have a set of feature values\n",
    "\n",
    "* **edges**: links between pairs of nodes\n",
    "\n",
    "Where we put edges in our graph determines links between nodes, and any pair of nodes does not have to have an edge between them. For a specific node, the list of nodes directly connected to it is referred to as the **neighbourhood** of the node. We also refer to nodes that you must traverse $k$ edges to reach as being $k$ **hops** away, and the **$k$-hop neighbourhood** denotes the set of these nodes.\n",
    "\n",
    "Consider a black & white image where we say any pixel in the image is a node. Because the image has a rectangular structure, every node will have links to 4 adjacent nodes (apart from pixels at the edge of the image). Pixels that are not adjacent will not have edges between them. A schematic of a basic graph can be seen below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee3b87",
   "metadata": {},
   "source": [
    "\n",
    "<center>\n",
    "<img src=\"plots/undirected_graph.png\" width=400 align=\"center\">\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:450px;display:inline-block;vertical-align:top;margin-top: 10px;'>\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "*A graphic of a simple graph with 4 nodes. We would say node 1 has edges linking to nodes 2 and 3; node 2 to 1 and 3; node 3 to 1, 2, and 4; and node 4 to just node 3 [[source](https://study.com/academy/lesson/weighted-graphs-implementation-dijkstra-algorithm.html)]* \n",
    "</div></div></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c9570",
   "metadata": {},
   "source": [
    "\n",
    "The features at each node describe that node, and the edges represent some link between nodes. What those features are, or what that link is, depends entirely on the data we are considering. Some common examples of graph data:\n",
    "\n",
    "* Social networks: nodes are people, and edges represent people who are \"friends\" on social media\n",
    "\n",
    "<center>\n",
    "<img src=\"plots/social_networks.png\" width=400 align=\"center\">\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:480px;display:inline-block;vertical-align:top;margin-top: 10px;'>\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "*A graphic representing a social network as a graph, where people are nodes and connections are represented by edges [[source](https://medium.com/analytics-vidhya/social-network-analytics-f082f4e21b16)]*\n",
    "</div></div></div>\n",
    "\n",
    "\n",
    "* Molecules: nodes are atoms, and edges represent chemical bonds in the molecule\n",
    "\n",
    "<center>\n",
    "<img src=\"plots/Caffeine_structure.png\" align=\"center\" width=400>\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:480px;display:inline-block;vertical-align:top;margin-top:10px;'>\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "*A graphic illustrating the structure of a caffeine molecule, where we can interpret atoms as nodes and chemical bonds as edges [[source](https://en.wikipedia.org/wiki/Molecular_graph)].*\n",
    "\n",
    "*Note that this graphic omits displaying carbon or hydrogen atoms, which is a convention used in chemistry.*\n",
    "</div></div></div>\n",
    "\n",
    "* The London Underground map: nodes are stations, and edges are routes between stations\n",
    "\n",
    "<center>\n",
    "<img src=\"plots/tube_map.png\" align=\"center\" width=400>\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:480px;display:inline-block;vertical-align:top;margin-top:10px;'>\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "*The London Underground map [[source](https://content.tfl.gov.uk/standard-tube-map.pdf)]. Treating as a graph, stations are nodes and routes between stations are edges.*\n",
    "</div></div></div>\n",
    "\n",
    "You can see there are many possible types of data that we can represent as graphs that could have interesting or challenging problems to tackle. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9921e18",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Formally, we denote a graph $G = (V, E)$ as having a set of nodes (or vertices) $V$ and a set of edges between these nodes $E$. The edge between node $u$ and node $v$ is denoted as the pair of vertices ($u$, $v$). \n",
    "\n",
    "In general we could have multiple edges between the same pair of nodes, an edge could be directional and so only relate in one direction, or a node could have an edge leading to itself, but for the purposes of this notebook we will restrict ourselves to simple graphs. \n",
    "\n",
    "One of the most common ways to then represent graphs is through an **adjacency matrix**. We assume the ordering of nodes is fixed, and then we define the elements $A_{uv}$ of the adjacency matrix $\\mathbf{A}$ as follows:\n",
    "\n",
    "$$A_{uv} = \\begin{cases}\n",
    "        1,\\qquad (u,\\,v) \\in E \\\\\n",
    "        0,\\qquad \\text{otherwise},\n",
    "\n",
    "      \\end{cases}$$\n",
    "where the adjacency matrix $\\mathbf{A}$ is a $|V| \\times |V|$ dimensional matrix and $|V|$ is the number of nodes in the graph. We say that $A_{uv}$ is the adjacency matrix entry corresponding to nodes $u$ and $v$. \n",
    "\n",
    "In other words, our adjacency matrix has a value of 1 for index pairs where there is an edge between those nodes, and a value of 0 if there is not an edge between them. \n",
    "\n",
    "In principle we could also have weighted edges, such that our adjacency matrix values are arbitrary real values rather than 0 or 1. We could do this if we want to represent some quantity of the link between nodes, e.g. in the Tube map example we could choose the travel time expected between two nodes. \n",
    "\n",
    "The adjacency matrix is symmetric in this case, but in general does not have to be - if we have a graph with directed edges, then the existence of an edge ($u$, $v$) does not require an edge ($v$, $u$) and as such the adjacency matrix can be non-symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5fe58",
   "metadata": {},
   "source": [
    "While here we have restricted our discussion to simple graphs, we can in general have much more complex graphs, including labels on edges, multiple types of edges, and more. For more details, see [Appendix A](#appendix-a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0e9ae",
   "metadata": {},
   "source": [
    "## Graph features\n",
    "\n",
    "In our graphs, as well as the list of nodes and edges, we often have **feature information**. This could describe information about our nodes, our edges, or even the entire graph. These features could be continuous-valued, discrete-valued, or categorical, like in any problem we try to handle with machine learning. To give some examples of possible features we might have, we will consider our London Underground example. Some possible features:\n",
    "\n",
    "* Node features, i.e. descriptors of stations:\n",
    "    * Continuous: average number of passengers through the station per day, average number of trains per hour\n",
    "    * Discrete: number of platforms at the station, number of station exits\n",
    "    * Categorical: if the station has step-free access or not, what zone the station is in\n",
    "<br></br>\n",
    "\n",
    "* Edge features, i.e. features of routes between stations:\n",
    "    * Continuous: average time taken to traverse the route, average number of passengers on that route per day\n",
    "    * Discrete: number of carriages per train, the year the route started running\n",
    "    * Categorical: if the train is air-conditioned or not, if the route crosses a zone boundary\n",
    "<br></br>\n",
    "\n",
    "* Graph features, i.e. features that describe the whole London Underground:\n",
    "    * Continuous: average delay to journeys per day, average fare paid per traveler per year\n",
    "    * Discrete: number of different lines running, number of stations undergoing engineering work\n",
    "    * Categorical: what lines currently have delays\n",
    "\n",
    "\n",
    "There is a lot of information we can extract from graphs depending on exactly what our data is, but we need to be careful about what exactly we use depending on what we want to do - too much information can always massively increase computation time and reduce performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdba8c",
   "metadata": {},
   "source": [
    "## Tasks for ML with graph data\n",
    "\n",
    "So far we have talked about problems that generally have pretty straightforward groupings into supervised, unsupervised, semi-supervised and reinforcement learning. While often we will use graph data in supervised learning problems, we can also have unsupervised or semi-supervised problems. \n",
    "\n",
    "Some of the standard tasks we consider for ML with graph data include:\n",
    "\n",
    "* Supervised:\n",
    "    * **node classification or regression**:  classifying or regressing some values for nodes e.g. identifying if an account on social media is a bot or not\n",
    "\n",
    "    * **predicting the existence of edges**: for a given pair of nodes, predicting if an edge exists between those two nodes e.g. suggesting friends on social media\n",
    "\n",
    "    * **graph classification or regression**: applying standard methods to whole graphs e.g. regressing toxicity of a molecule, classifying a computer program as malware or not based on a graph representation of its syntax and information flow\n",
    "\n",
    "* Unsupervised: \n",
    "    * **node clustering**: finding groups of nodes within the graph that are similar e.g. searching for communities of paper authors that are likely to collaborate from a citation graph\n",
    "\n",
    "    * **graph clustering**: finding similar groups of whole graphs, e.g. grouping molecular graphs for medications to find similar drugs\n",
    "\n",
    "\n",
    "In fact in general we may actually refer to graph-based supervised problems as semi-supervised, because while we might have labels for some of our nodes we also make use of the structure of the graph and surrounding nodes, even if we don't have labels for the neighbouring nodes. For more information, see [Appendix B](#appendix-b).\n",
    "\n",
    "The key take-away this is that the graph structure is just as, if not more important, than the specific node feature values for good performance on a graph. We will see this more as we learn about graph neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdea242",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have covered:\n",
    "\n",
    "* What graph data is\n",
    "\n",
    "* Some examples of graph data\n",
    "\n",
    "* What kind of features we might have in a graph\n",
    "\n",
    "* The types of tasks we might be trying to solve using graph data\n",
    "\n",
    "In the next section, we will introduce the PyTorch Geometric library, which is how we can work with graph data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084eff77",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "# Graphs in Python with PyTorch Geometric <a id='python-graphs'></a>[^](#index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8607bae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "So far we have considered how we can define graph data, and talked about what ML tasks we might be doing. While there are a few different libraries to handle graph data in Python, the most commonly used and the one we will use on this course is **PyTorch Geometric** (sometimes called **PyG**). This builds on top of PyTorch and implements many standard methods and architectures for learning on graph data. \n",
    "\n",
    "Note: this introduction is based on the introduction in the [PyTorch Geometric documentation](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html).\n",
    "\n",
    "In this section, we will discuss some basics of PyTorch Geometric around how we can define different types of graphs and how we can load things in. First thing we will do is import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279244f",
   "metadata": {},
   "source": [
    "As PyTorch Geometric is built on PyTorch, we can make use of the same `Tensor` objects we have seen before for fully-connected and convolutional neural networks. This will allow us to store gradients when we talk about graph neural networks later. \n",
    "\n",
    "When describing a graph programatically, because an adjacency matrix is typically sparse and graphs may be very large it is often inefficient and impractical to use an adjacency matrix. Instead, to define a graph, we use:\n",
    "\n",
    "* matrix of node features, shape $(N_{\\text{nodes}},\\,N_{\\text{features}})$\n",
    "* array of edge indices, shape $(2,\\,N_{\\text{edges}})$, where the values for each edge are the indices of the nodes connected by that edge\n",
    "\n",
    "The schematic below illustrates an example graph, the adjacency matrix for that graph, and the corresponding edge index:\n",
    "\n",
    "<center>\n",
    "<img src='plots/graph-edges-schematic.png' width=800/>\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:700px;display:inline-block;vertical-align:top;margin-top: 10px;'>\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "*Illustration of a graph, its adjacency matrix, and its edge index. We can see that for just 10 nodes, with only a few edges we require 100 values to store the adjacency matrix whereas we need just 20 for the edge index.*\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd983c7",
   "metadata": {},
   "source": [
    "\n",
    "In PyTorch Geometric, this is done with the `torch_geometric.data.Data` class. This class functions overall much like a dictionary, with elements corresponding to descriptors of our graph that we can look up, but also contains some functions for analysing graph structures, as well as some basic PyTorch tensor functionality like moving to a GPU etc. \n",
    "\n",
    "As well as specifying the node features and edges, we can specify other information about our graph when we instantiate a `Data` object. Some of the arguments we can pass include:\n",
    "\n",
    "* `data.x` : the node feature matrix\n",
    "\n",
    "* `data.edge_index` : the index pairs defining graph edges\n",
    "\n",
    "* `data.edge_attr` : features of our edges, shape ($N_{\\text{edges}},\\,N_{\\text{edge features}}$)\n",
    "\n",
    "* `data.y` : target that we are interested in, which can be arbitrary shape; could be e.g. node-level targets like a class per node, or graph-level targets like some descriptors or properties of the whole graph (depending on the problem we want to handle)\n",
    "\n",
    "One `Data` object generally represents one graph, although this is not always the case. We will now create a simple graph with 3 nodes and 2 edges:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a00fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]])\n",
    "x = torch.tensor([[-1], [0], [1]], dtype = torch.float)\n",
    "y = torch.tensor([[0], [0], [1]])\n",
    "data = Data(x = x, y = y, edge_index = edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ace9cd",
   "metadata": {},
   "source": [
    "For the `edge_index` we have specified, the top row is a list of start nodes, and the bottom row is the list of end nodes. \n",
    "\n",
    "However, you can see this construction could be a problem if we have many edges in our graph, and it would be very easy to make mistakes when typing out your edge index. We can alternatively specify an edge index as a set of (start, end) index pairs, but then need to apply some operations to return it to the right state to be used by our graph. The following code cell shows two **equivalent** ways of writing our edge index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]])\n",
    "\n",
    "edge_index = torch.tensor([[0, 1],\n",
    "                           [1, 2],\n",
    "                           [1, 0], \n",
    "                           [2, 1]]).t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2dddd",
   "metadata": {},
   "source": [
    "We can see that if we write the edge index as a set of index pairs, we need to apply a couple of operations to it. These include:\n",
    "\n",
    "* `.t()`: transpose the tensor, to take it from shape $(N_{\\text{edges}},\\,2)$ to $(2,\\,N_{\\text{edges}})$\n",
    "\n",
    "* `.contiguous()`: make the tensor *continguous in memory*. When we transpose a tensor, PyTorch just changes the references of each element rather than their position in memory, so neighbouring elements of the tensor are not necessarily next to each other in memory. Some operations require that our tensors *are* contiguous in memory, so this operation is applied just to be safe.\n",
    "\n",
    "You can of course choose how you want to construct your edge index, but if you do choose to use index pairs then make sure you apply the necessary operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9534d44",
   "metadata": {},
   "source": [
    "A key point to note here: when we specify `edge_index`, each entry specifies an edge going from the first index to the second index. This means that for an undirected graph, we must specify each edge twice, once in each direction. If we have a directed graph, we only need to specify the edge in the direction it goes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773a4f2",
   "metadata": {},
   "source": [
    "Because `Data` objects behave mostly like dictionaries, we can get our inputs back easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node features: {data['x']}\")\n",
    "\n",
    "print(f\"Edge index: {data['edge_index']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7b542",
   "metadata": {},
   "source": [
    "We also have some useful utility functions, which we can run below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ca44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of nodes\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "\n",
    "# Get number of edges\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "\n",
    "# Get number of node features\n",
    "print(f\"Number of node features: {data.num_node_features}\")\n",
    "\n",
    "# Check if any nodes are isolated i.e. connected to no edges\n",
    "print(f\"Any isolated nodes: {data.has_isolated_nodes()}\")\n",
    "\n",
    "# Check if there are any self-loops i.e. any node is connected to itself\n",
    "print(f\"Any self-loops: {data.has_self_loops()}\")\n",
    "\n",
    "# Check if the graph is directed\n",
    "print(f\"Is the graph directed: {data.is_directed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cb603",
   "metadata": {},
   "source": [
    "You can find more information on the specific methods for the `Data` object in the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971453d7",
   "metadata": {},
   "source": [
    "## A quick aside on graph visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe6e85",
   "metadata": {},
   "source": [
    "Once we have a graph, we could of course manually generate a visualisation using `matplotlib` (or another plotting library of your choice), but there are other libraries available in Python that provide some nice tools for visualising graphs to make things easier. \n",
    "\n",
    "One such library is `networkx`, a Python package for analysis of graph data. While this has many other useful tools in network analysis fields, for our purposes we will stick to the plotting tools that exist. An example of visualising our small graph using `networkx` is shown in the code cell below, and you can read more in [the documentation](https://networkx.org/documentation/stable/tutorial.html#drawing-graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example graph visualisation\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "# First, convert torch_geometric graph to a networkx graph:\n",
    "G = to_networkx(data, to_undirected = True) # can use to_undirected as our graph is undirected\n",
    "\n",
    "# Define graph layout; this is random with constraints to ensure the graph looks nice\n",
    "layout = nx.spring_layout(G, seed = 0)\n",
    "\n",
    "# Plot the graph\n",
    "# Color nodes based on the class of the node\n",
    "nx.draw_networkx(G, pos = layout, node_color = data.y, cmap = 'Set2', with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1982b748",
   "metadata": {},
   "source": [
    "Because this code just plots the graph on a matplotlib `Axis` object, we can then add any other kinds of plot formatting we want to in the same way we have done matplotlib plotting before. \n",
    "\n",
    "Generally this works fine for small graphs, but plots can of course become very crowded if we have large graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877ed80",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Example\n",
    "\n",
    "Using the PyTorch Geometric `Data` constructor, define a graph in Python with the following nodes and edges:\n",
    "\n",
    "* 10 nodes, each with 12 features:\n",
    "    * 10 features are one-hot encoded node IDs i.e. node 1 has the first feature equal to 1 and all others 0, node 2 has first node equal to 0, second to 1, rest 0, etc.\n",
    "    * Last 2 features are randomly generated values between 0 and 1\n",
    "\n",
    "* Each node should have 1 of 3 classes, randomly assigned\n",
    "\n",
    "* Edges given by this adjacency matrix:\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Remember that to instantiate a `Data` object you need a list of edge indices, i.e. node index pairs for each edge. This graph is symmetric and so each edge should be represented twice in your edge index list, once for each direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example solution\n",
    "import torch\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x = torch.cat([torch.eye(10),torch.rand((10,2))],dim = 1)\n",
    "y = torch.randint(low = 0, high = 4, size = (10,))\n",
    "edge_index = torch.tensor([[1, 5],[5, 1],\n",
    "                           [1, 6],[6, 1],\n",
    "                           [1, 9], [9, 1],\n",
    "                           [2, 4], [4, 2],\n",
    "                           [2, 7], [7, 2],\n",
    "                           [2, 8], [8, 2],\n",
    "                           [3, 5], [5, 3],\n",
    "                           [3, 6], [6, 3],\n",
    "                           [6, 8], [8, 6],\n",
    "                           [6, 9], [9, 6],\n",
    "                           [7, 9], [9, 7]\n",
    "                           ]).T.contiguous()\n",
    "\n",
    "data = Data(x, edge_index = edge_index, y = y)\n",
    "\n",
    "# optional visualisation\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "G = to_networkx(data, to_undirected = True)\n",
    "nx.draw_networkx(G, pos = nx.spring_layout(G, seed = 2), node_color = data.y, cmap='Set2', with_labels = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7536b7d",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Much like in PyTorch, when we come to actually set up our data for any kind of machine learning we want to collate it into some convenient structure for loading training, validation, and test data. \n",
    "\n",
    "In keeping with the PyTorch definitions, PyTorch Geometric also calls this object the `Dataset`. To see how this works, we will use a couple standard benchmarking datasets for graph learning problems, namely one where we want to learn a property of a whole graph, and one where we want to classify nodes. These are available in PyTorch Geometric in the `torch_geometric.datasets` module. \n",
    "\n",
    "We will start with the ENZYMES dataset from the [TUDataset collection](https://chrsmrrs.github.io/datasets/). This is derived from the [BRENDA database of enzymes](https://pubmed.ncbi.nlm.nih.gov/14681450/) where the graph for each enzyme describes the structural, sequential and chemical information in the same manner as first proposed [here](https://pubmed.ncbi.nlm.nih.gov/15961493/). The target to predict is what chemical reaction is catalysed by the enzyme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99240d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "# This will download the ENZYMES dataset\n",
    "dataset = TUDataset(root='data', name = 'ENZYMES')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f09fd",
   "metadata": {},
   "source": [
    "The PyG `Dataset` in general has similar syntax to the `Dataset` class in PyTorch, with some of the added features we have for `Data` objects in PyG and some other utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of graphs in the dataset\n",
    "print(f\"Dataset contains {len(dataset)} graphs\")\n",
    "\n",
    "# Get number of classes\n",
    "print(f\"Dataset contains {dataset.num_classes} classes\")\n",
    "\n",
    "# Get number of node features\n",
    "print(f\"Dataset has {dataset.num_node_features} node features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf1097",
   "metadata": {},
   "source": [
    "We can index the dataset to get individual graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef250ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dataset[0]\n",
    "print(graph)\n",
    "\n",
    "# Check if graph is directed\n",
    "print(f\"Is graph undirected: {graph.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29376d4",
   "metadata": {},
   "source": [
    "From this we know that this graph:\n",
    "\n",
    "* Has 37 nodes, each with 3 features\n",
    "* Is undirected, so has 168/2 = 84 undirected edges\n",
    "* Is assigned to exactly 1 class\n",
    "\n",
    "Also, we know that this data object is holding only 1 graph-level target, no node or edge-level targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7296d64",
   "metadata": {},
   "source": [
    "Unlike the PyTorch `Dataset`, where we have to access `Dataset.data` and `Dataset.target` (or manually create `Subset` objects, see [the documentation](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Subset)) to take slices of the dataset, we can instead take slices straight from the `Dataset` in PyTorch Geometric. Let's define train, validation, and test datasets in a 80:10:10 ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset[:480]\n",
    "valid_set = dataset[480:540]\n",
    "test_set = dataset[540:]\n",
    "\n",
    "print(f\"Train set has {len(train_set)} graphs\")\n",
    "print(f\"Validation set has {len(valid_set)} graphs\")\n",
    "print(f\"Test set has {len(test_set)} graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f08891",
   "metadata": {},
   "source": [
    "If we want to shuffle the dataset, we can use the `shuffle` method. We can check this by comparing the first graph in the shuffled dataset with the first graph we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_shuffled = dataset.shuffle()\n",
    "graph_shuffled = dataset_shuffled[0]\n",
    "\n",
    "print(graph)\n",
    "print(graph_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21543a9a",
   "metadata": {},
   "source": [
    "As we can see, these two must be different graphs as they have a different number of nodes and edges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c124a85",
   "metadata": {},
   "source": [
    "Now we will try looking at a dataset for a semi-supervised node classification: the [Cora dataset](https://link.springer.com/article/10.1023/A:1009953814988). This dataset is a citation graph of 2708 computer science papers, with 7 different classes. Each publication is described by a word vector with values of 1 or 0, indicating the presence or absence of the correspdoning word in a dictionary of 1433 unique words. If a given publication has $N$ 1s in its node feature vector, it means it has $N$ words from the dictionary in the publication.\n",
    "\n",
    "The Cora dataset was one of the datasets used in a paper describing a semi-supervised learning framework with node embeddings called [Planetoid (Predicting Labels And Neighbors with Embeddings Transductively Or Inductively from Data)](https://arxiv.org/abs/1603.08861); we will talk about **embeddings** in the context of graph neural networks in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4faa72e",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"plots/cora_vis.png\" width=500 align=\"center\">\n",
    "</center>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:600px;display:inline-block;vertical-align:top;margin-top:10px;'>\n",
    "\n",
    "*Visualisation of the structure of the Cora dataset [[source](https://graphsandnetworks.com/the-cora-dataset/)].*\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7a2ff",
   "metadata": {},
   "source": [
    "As before, this dataset can be downloaded & loaded easily using PyG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa02956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root='data', name = 'Cora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db040f",
   "metadata": {},
   "source": [
    "We will examine the dataset now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61162130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset contains {len(dataset)} entries\")\n",
    "print(f\"Dataset has {dataset.num_classes} classes\")\n",
    "print(f\"Dataset has {dataset.num_node_features} node features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63a839",
   "metadata": {},
   "source": [
    "We can see that this dataset has only 1 graph in it; we can look at this in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec13f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "print(data)\n",
    "\n",
    "print(f\"Is the graph undirected: {data.is_undirected()}\")\n",
    "print(f\"Does the graph have any self loops: {data.has_self_loops()}\")\n",
    "print(f\"Any isolated nodes: {data.has_isolated_nodes()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c44d5",
   "metadata": {},
   "source": [
    "Some key observations:\n",
    "\n",
    "* The graph has 2708 nodes\n",
    "* The graph is undirected\n",
    "* No self loops in the graph\n",
    "* No isolated nodes in the graph\n",
    "\n",
    "We also have some other attributes in this `Data` object that we haven't seen before - namely, `train_mask`, `val_mask`, and `test_mask`. This is because we are looking at a node-level problem here - so we need to separate the **nodes** into training, validation, and test datasets. This is done by having a boolean array for each of these, hence why each of `train_mask`, `val_mask`, and `test_mask` is an array of length 2708, the number of nodes in the graph.\n",
    "\n",
    "We can find the number of nodes in each of the subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training nodes: {data.train_mask.sum().item()}\")\n",
    "print(f\"Number of validation nodes: {dataset.val_mask.sum().item()}\")\n",
    "print(f\"Number of test nodes: {data.test_mask.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8d2b3",
   "metadata": {},
   "source": [
    "Note that the sum of the training, validation, and test datasets is not equal to the total number of graphs in the dataset; in general we may not want to use the default training/validation/test masks defined for a dataset loaded in PyG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46928d",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a33ea",
   "metadata": {},
   "source": [
    "As in PyTorch, we can use a `DataLoader` object in PyTorch Geometric to more efficiently load data for training. This also has some additional subtleties we will discuss. The general syntax for `torch_geometric.loader.DataLoader` is similar to the equivalent in PyTorch, which we will show with the ENZYMES dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbfa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='data', name = 'ENZYMES', use_node_attr = True)\n",
    "loader = DataLoader(dataset, batch_size = 32, shuffle = True)\n",
    "\n",
    "for idx, batch in enumerate(loader):\n",
    "    if idx==0:\n",
    "        print(batch)\n",
    "        print(f\"Number of graphs in batch: {batch.num_graphs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2bb23",
   "metadata": {},
   "source": [
    "The `DataBatch` class inherits from the `Data` class, but adds an additional attribute `batch`; this maps each node to its respective graph in the batch. Otherwise `DataBatch` essentially aggregates the nodes and edges from all of the graphs in the batch; `batch.num_nodes` is equal to the sum of the number of nodes across all graphs in the batch.\n",
    "\n",
    "You can also then apply functions across the batch from `torch_scatter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import scatter\n",
    "\n",
    "for idx, batch in enumerate(loader):\n",
    "    if idx == 0:\n",
    "        print(data)\n",
    "        print(f\"Number of graphs in batch: {batch.num_graphs}\")\n",
    "        x = scatter(batch.x, batch.batch, dim = 0, reduce = 'mean')\n",
    "        print(f\"Shape of batched result: {x.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff736322",
   "metadata": {},
   "source": [
    "We can see the output has shape $(N_{\\text{batch}},\\,N_{\\text{node features}})$. We have calculated the mean of each node feature per graph. For more operations that can be applied in this way, see the `torch_scatter` [documentation](https://pytorch-scatter.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc2b83",
   "metadata": {},
   "source": [
    "There are also a number of other types of loader functions in PyTorch Geometric, for different sampling methods and handling of very large graphs to batch nodes in subgraphs. You can read about this in the [`torch_geometric.loader` documentation](https://pytorch-geometric.readthedocs.io/en/stable/modules/loader.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cb4c1",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f103987",
   "metadata": {},
   "source": [
    "As we saw for CNNs, it is useful to be able to apply transforms to our data as we load it in; this could be to augment our dataset such as adding noise, normalising feature scales, or converting input data into the type of data we need for our ML method. \n",
    "\n",
    "<!-- for some reason can't get torch_cluster to work so can't do point cloud -> graph example -->\n",
    "\n",
    "Much like `torchvision.transforms`, we can use `torch_geometric.transforms` for graph data. For example, consider our London Underground example, and let us assign a position to each node based on the position of the station relative to Imperial College. Some transformations we could then apply include:\n",
    "\n",
    "* `RandomJitter` : add random translations to all nodes\n",
    "\n",
    "* `NormalizeFeatures` : normalise the node features\n",
    "\n",
    "* `Distance` : add a feature to all edges corresponding to the Euclidean distance between the nodes that edge links\n",
    "\n",
    "We can also combine a chain of transformations like in PyTorch using `Compose`, and we can pass these to `Dataset` objects to transform our data as it is read in. There are many options for transforms, which you can read more about in the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4055368",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have covered the basics of using graphs in Python with PyTorch Geometric, including:\n",
    "\n",
    "* How graphs are represented in PyTorch Geometric\n",
    "\n",
    "* `Datasets` and `DataLaoders` for graph data\n",
    "\n",
    "* Transforms for pre-processing our data\n",
    "\n",
    "In the next section, we will discuss how we structure graph neural networks for graph data tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ec6ec",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Intro to GNNs <a id='embeddings'></a>[^](#index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79388e",
   "metadata": {},
   "source": [
    "Now we have an understanding of how we define graph data and what we want to achieve with graph learning, we can get to how we actually tackle these problems. \n",
    "\n",
    "We have **a lot** of information in a graph, but not all of it may be useful, and often in machine learning we have to come up with features from our data which can be difficult and time-consuming, with no guarantee you come up with the \"best\" features to solve your problem. Instead, for graph data, we try to skip any feature engineering and instead learn a **representation** of the graph. \n",
    "\n",
    "What does this mean? The basic idea is as follows:\n",
    "\n",
    "* If nodes are similar in the graph, node labels should be similar or the same\n",
    "\n",
    "* It is hard to make features from a network, and can take a lot of time\n",
    "\n",
    "* If instead we map each node and its local structure to some $d$-dimensional vector, we can summarise information more effectively\n",
    "\n",
    "We call these $d$-dimensional vectors **node embeddings**, and we refer to the $d$-dimensional space they occupy as the **embedding space**. \n",
    "\n",
    "We can make an analogy with the neural networks we have looked at previously:\n",
    "\n",
    "* After we apply our linear or convolutional layers but before we reach the output layer, our data is in some hidden state\n",
    "\n",
    "* The embedding space is the graph equivalent of this hidden state\n",
    "\n",
    "\n",
    "If two nodes are \"similar\" in structure and features in the graph, then they should be geometrically close in the embedding space.\n",
    "\n",
    "How can we determine what the embeddings for our graph should be? That is where the **encoder-decoder** structure comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9269c",
   "metadata": {},
   "source": [
    "## Encoder-Decoder structure\n",
    "\n",
    "What do we mean by encoder-decoder? We want to define two operations that we chain together to act on our graph nodes:\n",
    "\n",
    "* **encoder** : transforms a given node to the embedding space, depending on the features of the node and its structure\n",
    "\n",
    "* **decoder** : maps a pair of node embeddings to some similarity score in the original graph\n",
    "\n",
    "The operation of the decoder is typically fixed, e.g. taking a dot product between the two embeddings. As a result, we want to optimize the encoder such that the decoder output is approximately equal to the similarity measure we consider. \n",
    "\n",
    "A graphic illustrating the action of an encoder can be seen below.\n",
    "\n",
    "<center>\n",
    "<img src=\"plots/node_embeddings.png\" align=\"centre\" width=600></img>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:500px;display:inline-block;vertical-align:top;margin-top:10px;'>\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "*A schematic illustrating an encoder mapping nodes from a graph into an embedding space [[source](https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/node-representation-learning)].*\n",
    "</div></div></div>\n",
    "\n",
    "We can define our embeddings in a number of ways, starting from so-called **shallow embeddings** where we learn a single embedding for every node, which are discussed in more detail in [Appendix C](#appendix-c). However, like we saw with neural networks a single layer can only perform so well. Instead, for better performance in graph learning tasks we turn to **deep embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacdc30",
   "metadata": {},
   "source": [
    "### Deep embeddings\n",
    "\n",
    "The basic idea of a deep encoder is similar to the idea that motivates deep neural networks: if we have several consecutive layers of non-linear operations, we should be able to approximate any function and so find an optimal embedding for our problem. \n",
    "\n",
    "You might think that we could just go straight to applying a standard fully-connected neural network or a CNN, but we encounter an issue unique to graph data: graphs have no \"true\" ordering. This means two things:\n",
    "\n",
    "* Any representation of the whole graph should be independent of the graph ordering\n",
    "\n",
    "* Any representation of a node should be independent of the graph ordering\n",
    "\n",
    "This leads to two separate but equally important characteristics for any function that we apply to our graph data to learn an embedding: **permutation-invariance** and **permutation-equivariance**. Any of the graph layers we will use from PyTorch Geometric will be one of these so we don't need to worry about it too much here. More details about these properties can be found in [Appendix D](#appendix-d).\n",
    "\n",
    "The layers we have used so far in fully-connected and convolutional neural networks do not satisfy this condition; instead, we must define new layers for graph neural networks which rely on so-called **message passing** and **aggregation** of information from neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e988ee1",
   "metadata": {},
   "source": [
    "## GNN layers: message passing and aggregation\n",
    "\n",
    "Now we understand the requirements for layers in a deep encoder, we need to work out how to put this together into something that can learn node embeddings for our graph. \n",
    "\n",
    "A general GNN layer defines two main parts:\n",
    "\n",
    "* how we summarise information from a node, referred to as the **message** from that node\n",
    "\n",
    "* how we combine the messages from neighbouring nodes to calculate an embedding for the node of interest, referred to as **aggregation**\n",
    "\n",
    "We can define either of these in almost whatever way we like, so long as all of our operations are either permutation-invariant or permutation-equivariant. \n",
    "\n",
    "For fully-connected NNs and CNNs, we used linear layers and convolutional layers to combine inputs to produce an output. For graphs, we use message passing followed by aggregation to combine information from neighbouring nodes to produce an embedding for a given node. This is illustrated in the following schematic.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 500px; margin: 0;\">\n",
    "<img src='plots/message-pass-agg-graph.png' width=500 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 500px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Left</strong>: The graph to be considered, where we want to find an embedding for node B.\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 500px; margin: 0;\">\n",
    "<img src='plots/message-pass-agg-flow.png' width=500 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 500px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Right</strong>: To calculate an embedding for node B, first we find the message from each its neighbouring nodes and then aggregate them.\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:500px;display:inline-block;vertical-align:top'>\n",
    "<div style=\"text-align:justify;\">\n",
    "\n",
    "*Illustration of the message passing procedure to find an embedding for a given node.*\n",
    "</div></div></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68bb39",
   "metadata": {},
   "source": [
    "We can think about this process in analogy with convolutional layers in CNNs:\n",
    "\n",
    "* In a CNN, we \"slide\" our kernel across the image and find the convolution of the kernel and the image pixels it covers\n",
    "\n",
    "* In a GNN, when we do message passing from a node's neighbourhood, we effectively find the convolution of the neighbouring nodes and the layer weights\n",
    "\n",
    "This comparison is illustrated in the schematic below. Please note that while we call the operation across the neighbouring nodes a convolution, it is not necessarily a convolution in general; we will see this for the first example GNN layer we discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88233fc8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='plots/image-vs-graph-conv.png' width = 1000/>\n",
    "</center>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 400px; margin: 0;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 400px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Left</strong>: example of a convolutional kernel on an image from the CIFAR10 dataset, to show how image pixels are aggregated in a convolutional layer.\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 400px; margin: 0;\">\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 400px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Right</strong>: example of graph message passing, to show how neighbouring nodes are aggregated for a graph convolution.\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:500px;display:inline-block;vertical-align:top'>\n",
    "<div style=\"text-align:justify; margin-top: 10px\">\n",
    "\n",
    "*Comparison of input aggregation in an image convolution and a graph convolution.*\n",
    "\n",
    "</div></div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36760962",
   "metadata": {},
   "source": [
    "We will illustrate the general GNN principle by way of example with the simplest GNN layer, based on the original GNN models proposed by [Merkwirth and Lengauer (2005)](https://pubs.acs.org/doi/10.1021/ci049613b) and [Starselli et al. (2009)](https://ieeexplore.ieee.org/document/4700287)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164cce5",
   "metadata": {},
   "source": [
    "### An example GNN layer\n",
    "\n",
    "As discussed, we need to define the message we pass from a node, and how we aggregate messages from nodes. We will start with the functional form of the embedding update, and then discuss how each component works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d7630",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FFCCCB\">\n",
    "\n",
    "Let the embedding at layer $k$ for node $v$ and node $u$ be denoted as $\\mathbf{h}_v^{(k)}$ and $\\mathbf{h}_u^{(k)}$ respectively. We also define the following:\n",
    "\n",
    "* $\\mathbf{h}_v^0$ = $\\mathbf{x}_v$ i.e. the first layer embedding for node $v$ is the vector of node features for node $v$\n",
    "\n",
    "* $\\text{N}(v)$ denotes the neighbourhood of $v$ i.e. the set of nodes that are directly connected to $v$\n",
    "\n",
    "The embedding for node $v$ in layer $k+1$ is given according to:\n",
    "\n",
    "$$\\mathbf{h}_v^{(k+1)} = \\sigma\\left(\\sum_{u\\,\\in\\,\\text{N}(v)} \\left[\\mathbf{W}_{\\text{neigh}}^{(k)} \\mathbf{h}_u^{(k)}\\right] + \\mathbf{W}_{\\text{self}}^{(k)} \\mathbf{h}_v^{(k)} + \\mathbf{b}^{(k)}\\right),$$\n",
    "\n",
    "\n",
    "where all symbols are defined as follows:\n",
    "\n",
    "* $\\sum_{u\\,\\in\\,\\text{N}(v)}$ denotes a sum over all nodes in the neighbourhood of $v$\n",
    "\n",
    "* $\\mathbf{W}_{\\text{neigh}}^{(k)}$ and $\\mathbf{W}_{\\text{self}}^{(k)} \\in \\mathbb{R}^{d_k \\times d_{k\\,-\\,1}}$ are the layer $k$ weight matrices for neighbouring nodes and the node of interest respectively; $d_k$ and $d_{k\\,-\\,1}$ denote the dimension of the embedding space at layer $k$ and $k - 1$ respectively\n",
    "\n",
    "* $\\mathbf{b}^{(k)} \\in \\mathbb{R}^{d_k}$ denotes a learnable bias vector for layer $k$\n",
    "\n",
    "* $\\sigma$ denotes a nonlinear activation function like ReLU, sigmoid, etc. to add non-linearity to our operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6669c1",
   "metadata": {},
   "source": [
    "So from this, we can see the process of a single layer for a given node goes as follows:\n",
    "\n",
    "* Calculate the message for each node in the neighbourhood of the relevant node\n",
    "\n",
    "* Sum the neighbourhood messages\n",
    "\n",
    "* Add the message of the relevant node from its previous layer embedding and a bias vector\n",
    "\n",
    "* Apply a non-linear activation function to the aggregated messages\n",
    "\n",
    "We can chain layers of this kind together in order to produce a deep graph neural network. Like in a CNN, as we go deeper in our GNN our embeddings become more and more abstract and are influenced by nodes further and further away. \n",
    "\n",
    "A schematic illustrating how we find the second layer embedding for a given node in an example graph is shown below.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 30px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 450px; margin: 0;\">\n",
    "<img src='plots/feat-init-graph-pass.png' width=450 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 450px; font-style: italic; line-height: 1.2;\">\n",
    "<div style=\"height: 1.5px;\"></div>\n",
    "<strong>Left</strong>: the graph considered to derive a second layer embedding for node B.\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 500px; margin: 0;\">\n",
    "<img src='plots/hidden2-embed-pass.png' width=500 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 500px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Right</strong>: contributions to the 2nd layer embedding $h^{(2)}_B$ from the 1st layer embeddings $h^{(1)}_i$ of its neighbouring nodes. Each node 1st layer embedding has contributions from the node features $X_i$ from their neighbouring nodes.\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:500px;display:inline-block;vertical-align:top'>\n",
    "\n",
    "*Illustration of the message passing procedure to find an embedding for a given node.*\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3e20e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "Note: sometimes, rather than explicitly defining self weight matrix $\\mathbf{W}_{\\text{self}}^{(k)}$, we may instead change the summation from over $u \\in \\text{N}(v)$ to $u \\in \\text{N}(v) \\cup v$ i.e. we sum over the union of the neighbourhood of $v$ and $v$ itself. This has the following consequences: \n",
    "\n",
    "* Effectively this adds self-loops to the adjacency matrix of our graph\n",
    "\n",
    "* We can then use the same weight parameters for both the neighbourhood and the node's own embedding\n",
    "\n",
    "* This reduces the number of parameters we need to learn and can make training more efficient. \n",
    "\n",
    "However, we can lose information doing this as we can no longer separately learn contributions from the neighbourhood and the node's previous embedding, so this may not always be the best approach and it is in general problem-dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e6169",
   "metadata": {},
   "source": [
    "\n",
    "While here we have only considered our messages being the output of a linear layer applied to our node embeddings, in general we could include any of the layers in our neural network toolbox, including:\n",
    "\n",
    "* Dropout\n",
    "\n",
    "* Batch normalisation\n",
    "\n",
    "* Activation functions\n",
    "\n",
    "We group these methods and the linear layer we have considered already as transformations of the node embedding, and then we apply whatever aggregation function we have chosen across the transformed node embeddings in the neighbourhood. \n",
    "\n",
    "In fact, if can even be worth replacing our simple single linear layer with an MLP, if we need more capacity in our network. This becomes more necessary for complex problems where a few GNN layers is not enough - see the next section.\n",
    "\n",
    "The parameters we learn in training our GNN are the parameters of the message passing network, which are reused for every node. We could in general make this network as deep as we like to improve the performance of our GNN, using everything we have learned so far about building neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92423672",
   "metadata": {},
   "source": [
    "## Stacking GNN layers\n",
    "\n",
    "For both fully-connected and convolutional neural networks, we know that increasing the depth increases the complexity of the functions we can model with the network. However, this principle does not apply to graph neural networks - specifically, if we stack too many GNN layers, we encounter an issue referred to as the **oversmoothness problem**. This is explained as follows:\n",
    "\n",
    "* For $k$ GNN layers, each node embedding has contributions from nodes $k$\n",
    "    * In the first layer, contributions from neighbouring nodes are just the node features\n",
    "    * In the second layer the contributions from neighbouring nodes are the 1st layer embeddings, which are determined by node features from their neighbouring nodes; this means our node of interest has contributions from the features of nodes of next-nearest neighbours (i.e. a 2 hop neighbourhood)\n",
    "    * This continues as we add more layers\n",
    "\n",
    "* As $k$ increases, the **receptive field** of each node increases - this is the set of nodes that contribute to the $k^{\\text{th}}$ layer embedding at our node\n",
    "\n",
    "* For large $k$ (relative to the size of the graph), each node can have contributions from most of the rest of the graph\n",
    "\n",
    "* As a result, with many GNN layers all embeddings can converge to the same value\n",
    "\n",
    "The schematic below shows two nodes of interest, and their overlapping neighbours as we increase the number of hops i.e. as we add more layers to our GNN. With just 3 GNN layers, the two nodes already overlap on nearly all of the nodes in the graph!\n",
    "\n",
    "<center>\n",
    "<img src='plots/oversmoothness.png' width=800></img>\n",
    "</center>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:760px;display:inline-block;vertical-align:top'>\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "*An example 50 node network with two nodes of interest, illustrating what nodes are neighbours shared between the nodes of interest for increasing number of hops. While for a 1 hop neighbourhood they only have 1 node in common, by the time we have a 3 hop neighbourhood nearly every node in the network is a common neighbour for our two nodes of interest.* \n",
    "\n",
    "</div></div></div>\n",
    "\n",
    "This isn't to say we can't or shouldn't chain GNN layers together, but we need to be careful how many we use. There are a few methods we can use to mitigate this:\n",
    "\n",
    "* Increase the complexity of individual layers, e.g. replace message linear layer with a full multilayer perception\n",
    "\n",
    "* Add non-GNN layers before/after the GNN layers, e.g. MLPs to preprocess node features to a lower dimension\n",
    "\n",
    "* Add **skip connections**: routes by which embedding values can be passed to the next layer without applying the function in between; we can interpret this as instead of applying a function $F$ to an input $\\mathbf{x}$ and just returning $F(\\mathbf{x})$, we instead return $F(\\mathbf{x}) + \\mathbf{x}$ \n",
    "    \n",
    "    * This is motiviated by observations that embeddings in earlier layers can sometimes be more informative for e.g. classification than those in later layers\n",
    "\n",
    "    * This essentially creates a mixture of models, of shallow and deep GNNs\n",
    "\n",
    "    * We can even have skip connections from every layer to the end, so the final prediction is based on the embeddings output at every layer rather than just the final layer embeddings\n",
    "\n",
    "    * GCN layers implicitly do this by adding the relevant node's previous layer embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1c83f",
   "metadata": {},
   "source": [
    "## Training and prediction with GNNs\n",
    "\n",
    "So far we have talked about how we can define a GNN, but how can we train it, and how can we make predictions to solve the kinds of tasks we want to solve? To start with, let's recap a few types of task we might be trying to solve:\n",
    "\n",
    "* node-level prediction: classification, regression, clustering\n",
    "\n",
    "* edge-level prediction: classification, regression, predicting edge existence\n",
    "\n",
    "* graph-level prediction: classification, regression\n",
    "\n",
    "As you can imagine, there is not one single method by which we can do all of these tasks. Instead, we have to consider different kinds of **prediction head**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a7f377",
   "metadata": {},
   "source": [
    "\n",
    "### Prediction heads\n",
    "\n",
    "A prediction head is the part of our GNN that goes from the node embeddings output by the encoder to whatever it is we want to predict, e.g. a node class, if an edge exists or not, or some property of the whole graph. To consider each of these in turn: \n",
    "\n",
    "* Node-level tasks: use the node embeddings as the input to a model to perform the task we desire, e.g. for $N$-class classification can use a linear layer from $d$ embedding features to $N$ classes\n",
    "\n",
    "* Edge-level tasks: edges link pairs of nodes, so we must somehow aggregate the pair of node embeddings e.g. using standard linear algebra methods to combine two vectors such as dot products etc.\n",
    "\n",
    "* Graph-level tasks: we need to aggregate over all of the node embeddings to get a single embedding for the whole graph, e.g. via some global pooling like mean pooling.\n",
    "\n",
    "For both edge and graph tasks, we can apply standard methods to the aggregated edge or graph embeddings. The schematics below illustrate each type of prediction head.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 30px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 330px; margin: 0;\">\n",
    "<img src='plots/node-prediction-head.png' width=330/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 320 px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>(a)</strong>: a node prediction head - we can directly use the node embeddings for predicting individual node properties.\n",
    "</div></div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 330px; margin: 0;\">\n",
    "<img src='plots/edge-prediction-head.png' width=330/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 320 px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>(b)</strong>: an edge prediction head - we aggregate pairs of node embeddings to get an edge embedding, used to make predictions for edge-level tasks.\n",
    "</div></div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 330px; margin: 0;\">\n",
    "<img src='plots/graph-prediction-head.png' width=330/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 320 px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>(c)</strong>: a graph prediction head - we aggregate all nodes across the graph to get a single graph embedding, which is used for prediciton of graph-level values.\n",
    "</div></div>\n",
    "</div>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:500px;display:inline-block;vertical-align:top'>\n",
    "\n",
    "*Illustration of various types of prediction head.*\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb3f0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There are some more subtleties for the aggregation for edges or graphs, which you can read more about in [Appendix E](#appendix-e).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a672bbf",
   "metadata": {},
   "source": [
    "To put this in the context of the encoder-decoder structure, we can think of whatever operation we apply to the deep node embeddings (or edge embeddings, or graph embedding) to get to our target, be that a class, continuous value, or cluster ID, as the decoder. \n",
    "\n",
    "For example, for node classification our decoder could be a fully-connected neural network that outputs the class of the node. The parameters of this neural network are trained at the same time as the encoder parameters, creating a complete GNN. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b63f95",
   "metadata": {},
   "source": [
    "### How to train a GNN\n",
    "\n",
    "When it comes to training a GNN, the general procedure is much the same as all of the different ML techniques we have looked at so far:\n",
    "\n",
    "* Define a (differentiable) loss function that measures how close our prediction is to what we want to get\n",
    "\n",
    "* Separate data into training, validation, and test datasets\n",
    "\n",
    "* Pass training data through the GNN, record the predictions and calculate the loss\n",
    "\n",
    "* Update all the parameters of the GNN according to some optimisation algorithm e.g. gradient descent\n",
    "\n",
    "When we consider our loss function for supervised learning problems we can use the same functions we have seen before:\n",
    "\n",
    "* Regression: mean squared error, mean absolute error, etc\n",
    "\n",
    "* Classification: cross entropy, negative log likelihood, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a6eeae",
   "metadata": {},
   "source": [
    "<!-- <div style=\"background-color: #FFF8C6\"> -->\n",
    "\n",
    "### Transductive vs inductive training\n",
    "\n",
    "When we are considering graph learning tasks, are working with either an **inductive** or a **transductive** setting. These are defined as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "* **inductive setting**: exactly what we have done for ML so far: training, validation, and test datasets are strictly separated\n",
    "\n",
    "    * This prevents any learning on the validation or test datasets, with the aim of generalising beyond the training dataset\n",
    "\n",
    "    * In the context of graphs, we can consider two different scenarios:\n",
    "\n",
    "        * A graph-level task: each graph is independent of the other graphs, so it is easy to have completely separate sets for training, validation, and testing\n",
    "\n",
    "        * A node or edge task: in GNNs, predictions for nodes and edges implicitly depend on any other nodes or edges they are connected to, via the message-passing framework - so we have to make sure any non-training objects are *removed entirely from the graph*\n",
    "\n",
    "            * For a node-level example, for an inductive scenario we must break any edges linking non-training nodes to training nodes, so they cannot influence the learning on the training nodes\n",
    "\n",
    "            * No structure from non-training set nodes is included\n",
    "\n",
    "            * This lets us generalise to new nodes added to the graph, but may have worse general performance\n",
    "            \n",
    "<br></br>\n",
    "\n",
    "* **transductive setting**: training, validation, and test sets are not entirely separate: non-training nodes are kept in the graph but with no labels\n",
    "\n",
    "    * Non-training nodes can still inform prediction on training nodes through message passing\n",
    "\n",
    "    * The model parameters **do not** learn labels for non-training nodes, just for training nodes\n",
    "\n",
    "    * Training node learning is implicitly informed by the node features of its neighbours, even if they aren't training nodes\n",
    "    \n",
    "    * Some properties of this include:\n",
    "\n",
    "        * Generally improved performance within the same graph\n",
    "\n",
    "        * Cannot generalise to new nodes added to the graph\n",
    "        \n",
    "        * Not possible for e.g. graph-level classification as each graph is independent of the other graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078cd840",
   "metadata": {},
   "source": [
    "Often, we might find that transductive learning is good for tasks where we have very few labelled instances, as we can make use of other data whereas an inductive approach would be stuck with just the labelled data. You will see examples of both types in the exercises later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec00cf8",
   "metadata": {},
   "source": [
    "### Graph manipulation\n",
    "\n",
    "Sometimes the graph we want to do some task with simply is not the optimal graph to use, and instead requires some manipulation. This is just another way to say sometimes our graph needs preprocessing, but at the graph level rather than just the feature level. This can include feature augmentation, but also handling of graphs that are either too sparse or too dense. For more details, see [Appendix F](#appendix-f).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748998c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have covered:\n",
    "\n",
    "* What node embeddings are and how they can be found with the encoder-decoder structure\n",
    "\n",
    "* How we build GNN layers to find deep node embeddings\n",
    "\n",
    "* How to go from node embeddings to different kinds of prediction for our graph learning tasks\n",
    "\n",
    "* Some issues that can be encountered with GNNs and graph data, and how they can be mitigated\n",
    "\n",
    "In the following section, we will discuss how we build and train GNNs in Python using PyTorch Geometric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c56ad5",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Building and training GNNs in PyTorch Geometric <a id='pyg-gnn'></a>[^](#index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d460ed",
   "metadata": {},
   "source": [
    "Now we have discussed what GNNs are and how we can train them, we will cover how to actually build and train a GNN using PyTorch Geometric. We will first step through the main things we need to consider, and then put it all together as an example with a simple but classic graph dataset: [Zachary's karate club network](https://www.journals.uchicago.edu/doi/abs/10.1086/jar.33.4.3629752)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0144dff",
   "metadata": {},
   "source": [
    "## GNN layers in PyTorch Geometric\n",
    "\n",
    "PyTorch Geometric has many different GNN layers pre-defined for you, to make building and training GNNs as easy as possible. Because PyG is built on top of regular PyTorch, the syntax and structure for building models is also much the same as we have seen before. \n",
    "\n",
    "The most basic layer implemented in PyG is called `GCNConv`, which is the graph convolutional operator as defined in [this paper](https://arxiv.org/abs/1609.02907). This layer is very similar to the basic GNN layer we introduced above with self-loops added (so a single trainable weight matrix), but with normalisation factors based on the number of node neighbours added. We can instantiate this easily for a layer with 10 input dimensions and 3 output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82dadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as gnn\n",
    "\n",
    "gcn = gnn.GCNConv(10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a4db8",
   "metadata": {},
   "source": [
    "It is exactly as simple as that; however, evaluating the output of the layer requires more than just our node features $\\mathbf{x}$ and instead also requires our list of edge indices, so we can find the neighbourhood of each node. We can illustrate this with a simple example with node feature values randomly generated between 0 and 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68994000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]])\n",
    "x = torch.rand(size = (3, 10), dtype = torch.float)\n",
    "data = Data(x = x, edge_index = edge_index)\n",
    "\n",
    "output = gcn(data.x, data.edge_index)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4db879",
   "metadata": {},
   "source": [
    "We can see that the output of our layer now has 3 node features per node, and a standard gradient function assigned so we can train the layer parameters using standard gradient-based methods. We can examine the linear weight matrix and the bias vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear layer: {gcn.lin}, weights = {gcn.lin.weight}\\n\")\n",
    "print(f\"Bias vector: {gcn.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8c074",
   "metadata": {},
   "source": [
    "We can see the general structure of the parameters is very similar to `Linear` layers in regular PyTorch, but we have separated out the weights and the bias to apply the weights to determine our message and then apply the bias after aggregation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b49944",
   "metadata": {},
   "source": [
    "\n",
    "Of course, some of the more complex GNN layers that have been proposed and are defined in PyG can have more complicated syntax, but in general the syntax is very similar to regular PyTorch.\n",
    "\n",
    " The types of layers in PyG range from convolutional, like `GCNConv`, to aggregation, normalisation, pooling and more. For a full list of available layers, see the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48960d9b",
   "metadata": {},
   "source": [
    "## Models in PyG\n",
    "\n",
    "Because PyTorch Geometric is built on top of PyTorch, the way we put together models is very similar to all the neural networks we have looked at so far but often needs some extra syntax to account for graph data. \n",
    "\n",
    "Putting models together can either be done with `torch_geometric.nn.Sequential`, or by subclassing `torch.nn.Module` and manually specifying the `forward` method. We will put together a simple three layer GNN with both of these methods in the cells below.\n",
    "\n",
    "We are going to prepare models with 34 inputs, hidden layer sizes of 4, 4, and 2, and an output size of 4 to correspond to the number of classes. We have selected 2 as the number of dimensions for the final hidden layer so we can easily visualise the embedding space at this layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63492b75",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "While here we have chosen our final hidden layer to have 2 output features so we can visualise the final layer embeddings in a 2D plot, in general we may want more features for our final embeddings to properly capture the full information in the graph. In this case, we can instead use dimensionality reduction techniques such as [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to project our embedding space down to 2 dimensions for visualisation. \n",
    "\n",
    "For an example of this, see [this PyG Colab tutorial for node classification](https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e48244",
   "metadata": {},
   "source": [
    "\n",
    "Because the inputs for PyG are more complicated than for regular PyTorch, if we use `Sequential` we have to declare at the start what our inputs are, and then for each layer we have to declare both the inputs and what it maps to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48808b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model_sequential = gnn.Sequential('x, edge_index', [\n",
    "                                  (gnn.GCNConv(34, 4), 'x, edge_index -> x'),\n",
    "                                  (nn.LeakyReLU(), 'x -> x'),\n",
    "                                  (gnn.GCNConv(4, 4), 'x, edge_index -> x'),\n",
    "                                  (nn.LeakyReLU(), 'x -> x'),\n",
    "                                  (gnn.GCNConv(4, 2), 'x, edge_index -> x'),\n",
    "                                  (nn.LeakyReLU(), 'x -> x'),\n",
    "                                  nn.Linear(2, 4)\n",
    "                                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c3b5b",
   "metadata": {},
   "source": [
    "This model will take an input graph with 34 node features, finding 4, 4, and 2 dimensional embeddings at layers 1, 2, and 3 respectively, and then applies a linear layer at the end to get to a four outputs corresponding to four possible classes. We have used a leaky ReLU as our activation function throughout.\n",
    "\n",
    "The advantage of the PyG `Sequential` compared to the standard PyTorch `Sequential` is that because of this input declaration setup, we can more easily pass additional information into our model or add in skip connections without needing to exactly specify all the details of a `forward` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cd075",
   "metadata": {},
   "source": [
    "Now we will try reproducing the same model by subclassing `torch.nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bee975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GCNConv(34, 4)\n",
    "        self.conv2 = gnn.GCNConv(4, 4)\n",
    "        self.conv3 = gnn.GCNConv(4, 2)\n",
    "        self.lin = nn.Linear(2, 4)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "model_module = GCN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0f722",
   "metadata": {},
   "source": [
    "As we have seen before the `forward` function is called when we pass data to the model. Because the `Batch` object returned by a `DataLoader` in PyG is a subclass of the `Data` object, we need to make sure we explicitly get the node features and edge index from the `Batch` in our `forward` method, as well as any other attributes of the data we might need for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb663dde",
   "metadata": {},
   "source": [
    "One other key kind of layer we need to be aware of are global pooling layers; this are necessary to aggregate our node embeddings to compute a whole graph embedding, for graph-level learning tasks. We can chain these with other GNN or normal NN layers - the following cell shows an example for both a `Sequential` and a subclassed `nn.Module` for a graph-level classification task, for some hypothetical data with 5 node features and 3 possible graph classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Sequential version\n",
    "model_graph_sequential = gnn.Sequential('x, edge_index, batch', [\n",
    "    (gnn.GCNConv(5, 20), 'x, edge_index -> x'),\n",
    "    (nn.LeakyReLU(), 'x -> x'),\n",
    "    (gnn.GCNConv(20, 20), 'x, edge_index -> x'),\n",
    "    (nn.LeakyReLU(), 'x -> x'),\n",
    "    (gnn.global_mean_pool, 'x, batch -> x'),\n",
    "    nn.Linear(20, 3),\n",
    "])\n",
    "\n",
    "class GCN_graph(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GCNConv(5, 20)\n",
    "        self.conv2 = gnn.GCNConv(20,20)\n",
    "        self.lin = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # get node embeddings\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        # pool node embeddings to get graph embedding\n",
    "        x = gnn.global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model_graph_module = GCN_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c59f9",
   "metadata": {},
   "source": [
    "The internal batching procedure of PyTorch Geometric concatenates all the nodes across the graphs in the batch together, and the `batch` attribute of a `Batch` object identifies which nodes belong to which graph. As a result, to get accurate graph-level pooling of embeddings, we need to pass this attribute to the global pooling we use, in this case `global_mean_pool`. For a `Batch` called `data`, we can get this attribute with `data.batch`. \n",
    "\n",
    "There are many different options for pooling of node embeddings, which you can read more about in [the documentation](https://pytorch-geometric.readthedocs.io/en/2.5.3/modules/nn.html#pooling-layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041a447",
   "metadata": {},
   "source": [
    "## Zachary's karate club\n",
    "\n",
    "As a brief aside, we need to load in our data before we can train a GNN. To do this, we will use the karate club dataset originally described in 1977. The social network of a karate club was studied by Wayne W. Zachary for a period of three years, who documented links between the 34 members of the club based on interactions outside the club. Eventually, the club split into two after a conflict between the instructor and the administrator, and with the data he had collected Zachary assigned all but one member to the correct group after the split.\n",
    "\n",
    "For the purposes of machine learning on graphs, however, two classes is generally considered too simple to be interesting; instead, the [paper introducing Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) demonstrated their performance by first clustering the karate club graph to find 4 classes, and then trained and predicted on the data. It is this version of the dataset that is saved in PyTorch Geometric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27752d0e",
   "metadata": {},
   "source": [
    "Like in regular PyTorch, PyTorch Geometric has a separate module for many benchmark datasets for graph learning methods. We can import the dataset using `KarateClub`, and do the same inspections we have done so far for other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import KarateClub\n",
    "\n",
    "dataset = KarateClub()\n",
    "\n",
    "# Get number of graphs in the dataset\n",
    "print(f\"Dataset contains {len(dataset)} graphs\")\n",
    "\n",
    "# Get number of classes\n",
    "print(f\"Dataset contains {dataset.num_classes} classes\")\n",
    "\n",
    "# Get number of node features\n",
    "print(f\"Dataset has {dataset.num_node_features} node features\")\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "print(data)\n",
    "\n",
    "print(f\"Is the graph undirected: {data.is_undirected()}\")\n",
    "print(f\"Does the graph have any self loops: {data.has_self_loops()}\")\n",
    "print(f\"Any isolated nodes: {data.has_isolated_nodes()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eeaa5",
   "metadata": {},
   "source": [
    "As expected we can see that we have 1 graph with 4 classes. Each node has 34 features, which are not inherent to the original observations made by Zachary but instead are a one-hot encoding of the node IDs to allow application of GNNs (as discussed in the feature augmentation section previously). Most of the other properties of the graph are simple, with no self loops, no directed edges, and no isolated nodes.\n",
    "\n",
    "Crucially, we can also look at how many nodes are in the training set by summing the train_mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training nodes = {data.train_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85b8e4e",
   "metadata": {},
   "source": [
    "We have just 4 nodes in our training set, which corresponds to about 12% of all of our data. It is not uncommon to have significantly smaller proportions of training data in graph datasets, as it can be difficult to accurately label a large amount of data e.g. in social media networks to classify users as bots or not, it can be very time consuming to manually classify accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56573c5c",
   "metadata": {},
   "source": [
    "We will use `networkx` here to visualise the graph, including some of our matplotlib formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d75cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = to_networkx(data, to_undirected = True)\n",
    "fig = plt.figure(figsize = (7, 7), dpi = 300)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "nx.draw_networkx(G, pos = nx.spring_layout(G, seed = 1), with_labels = False,\n",
    "                 node_color = data.y, cmap = \"Set2\")\n",
    "plt.title(\"Zachary's Karate Club network\",fontsize = 20)\n",
    "\n",
    "# Manually make a legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "Set2 = plt.get_cmap('Set2')\n",
    "classes = data.y.unique()\n",
    "legend_elements = [Patch(facecolor=Set2(i/(len(classes) - 1)), label = f'Class {i}') for i in data.y.unique()]\n",
    "plt.legend(handles = legend_elements, title='Node classes', loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ce404",
   "metadata": {},
   "source": [
    "Because of the simplicity of this data, only a training mask is provided and then all other nodes are assumed to be test nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716b61c",
   "metadata": {},
   "source": [
    "We will now detail how we can train our previous GNN model setups on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00a307",
   "metadata": {},
   "source": [
    "## Training GNNs\n",
    "\n",
    "Because PyTorch Geometric is built on top of PyTorch, we train GNNs with the same steps we use for regular or convolutional neural networks: \n",
    "\n",
    "* Feed training data through the network to get the output\n",
    "\n",
    "* Calculate loss between the output and the target values\n",
    "\n",
    "* Calculate the gradient of the loss with respect to each parameter in the network\n",
    "\n",
    "* Use the gradients to perform a step with the optimiser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa59fc9",
   "metadata": {},
   "source": [
    "Before we actually start training the GNN, let us define a model instance and then let us see what the final embeddings *for all nodes* look like without any training; we will redefine our module version of our model to allow it to return the embeddings as well as the final classifier outout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.conv1 = gnn.GCNConv(34, 4)\n",
    "        self.conv2 = gnn.GCNConv(4, 4)\n",
    "        self.conv3 = gnn.GCNConv(4, 2)\n",
    "        self.lin = nn.Linear(2, 4)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        output = self.lin(h)\n",
    "        return output, h\n",
    "    \n",
    "model = GCN(seed=1234)\n",
    "\n",
    "output, embedding = model(data)\n",
    "\n",
    "def plot_embedding(embedding, epoch_loss = None, ax = None, markersize = 100, legend = False):\n",
    "    embedding = embedding.detach().cpu().numpy()\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(7,7),dpi = 300)\n",
    "        ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(embedding[:,0], embedding[:,1], s = markersize, c = data.y, cmap=\"Set2\",edgecolor='black',lw=1)\n",
    "    if epoch_loss is not None:\n",
    "        ax.set_title(f\"Epoch: {epoch_loss[0]}, Loss: {epoch_loss[1]:.4f}\",fontsize = 18)\n",
    "        # ax.set_xticks([])\n",
    "        # ax.set_yticks([])\n",
    "    else:\n",
    "        ax.set_title('Final layer embeddings\\nwithout training',fontsize = 18)\n",
    "    ax.set_xlabel('Embedding dimension 1 [A.U.]',fontsize = 16)\n",
    "    ax.set_ylabel('Embedding dimension 2 [A.U.]',fontsize = 16)\n",
    "    ax.tick_params(axis = 'both',labelsize=14)\n",
    "    if legend:\n",
    "        ax.legend(*scatter.legend_elements(),loc='best',title='Classes',fontsize = 14,title_fontsize=16)\n",
    "    ax.get_figure().tight_layout()\n",
    "\n",
    "\n",
    "plot_embedding(embedding, legend = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445e3b0",
   "metadata": {},
   "source": [
    "We can see that while we don't immediately have something that looks like 4 distinct clusters, we can that nodes with the same label seem to have similar embedding coordinates, suggesting we can do a good job even with an untrained GNN to represent the graph in a lower dimensional space. We'll now try training this GNN and see how the prediction evolves as a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e3815",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF8C6\">\n",
    "\n",
    "\n",
    "**Note**: you can see we have used the `ELU` function to start with here, rather than `ReLU`; because the node features are just one-hot encodings of the node ID, we can very easily end up with the message from each node being 0 immediately, which means we become *very* dependent on our starting weights and in many cases our model can't learn at all as we end up with many dead neurons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bcdb66",
   "metadata": {},
   "source": [
    "Because the network we are working with here only has 4 nodes in the training set, we can completely skip the `DataLoader` and simply pass all of the training data to the network in each epoch, with no need for batching. In general with graphs we will need to use batches for graph-level tasks, or for when we have large networks with many nodes used for training. \n",
    "\n",
    "The code cell below shows how we can train and evaluate this model, including evaluating the loss for the training data and calculating the performance on the test data after training. We will also visualise the final layer embedding every 50 epochs of training, for 400 epochs. We will also record the loss both for the training dataset and the test dataset, so we can plot loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time # Measure training time\n",
    "\n",
    "n_epochs = 400\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out, h  = model(data)\n",
    "    loss = loss_fcn(out[data.train_mask], data.y[data.train_mask]) # get training outputs & labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%50 == 0:\n",
    "        plot_embedding(h, (epoch, loss))\n",
    "    train_losses.append(loss.item())\n",
    "    vloss = loss_fcn(out[~data.train_mask],data.y[~data.train_mask])\n",
    "    test_losses.append(vloss.item())\n",
    "end_time = time.time()\n",
    "print(f'Total training time: {end_time-start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc397c",
   "metadata": {},
   "source": [
    "By the end of the training, we've done a pretty reasonable job of producing linearly-separable clusters per label in the embedding space. Let us check the loss curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test performance\n",
    "\n",
    "# Plot loss\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig = plt.figure(figsize=(8,6),dpi = 150)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(1,n_epochs+1),train_losses,label='Train loss',color='black')\n",
    "ax.plot(np.arange(1,n_epochs+1),test_losses,label='Validation loss',color='#D55E00')\n",
    "ax.set_xlabel('Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax.set_title('Karate Club loss\\nduring training',fontsize = 20)\n",
    "ax.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax.legend(loc='center right',fontsize = 12,framealpha = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5126c9d",
   "metadata": {},
   "source": [
    "We can clearly see we are overfitting here - this is hardly surprising, given we only have 4 training samples, but certainly something we would need to be careful of in general. \n",
    "\n",
    "Now we can check the final classification accuracy; we will assume the largest output neuron corresponds to the class we should select for a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9fdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "out, h = model(data)\n",
    "\n",
    "preds = out.max(dim = 1)[1].numpy() # Get predicted class for all of the nodes\n",
    "true = data.y.detach().cpu().numpy()\n",
    "\n",
    "accuracy_test = accuracy_score(true[~data.train_mask], preds[~data.train_mask]) #calculate accuracy for test set nodes\n",
    "print(f\"Classification accuracy across the test nodes: {accuracy_test:.1%}\")\n",
    "\n",
    "accuracy = accuracy_score(true, preds) #calculate accuracy across the whole graph\n",
    "print(f\"Classification accuracy across the whole graph: {accuracy:.1%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e7d9d",
   "metadata": {},
   "source": [
    "We can see that even using only 4 of the nodes in our training set, we can achieve a 80.0% accuracy across the test set and 82.4% across the whole graph; clearly, we have learned a lot from just the structure of the graph and these four nodes. \n",
    "\n",
    "Please note that in our training code, although we only compute the loss based on the nodes in the training set, we do still pass the whole graph through the network so while we don't learn the labels of the test nodes, their presence in the graph still contributes to the learning of our training nodes. \n",
    "\n",
    "In other words, we have performed **transductive learning** here. If we wanted to train in an **inductive** fashion, we might first randomly select nodes to split the graph into two subgraphs, set one as the training subgraph and one as the test, then solely train on the training subgraph and evaluate on the test subgraph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cf8f8",
   "metadata": {},
   "source": [
    "We have done some simple GNN setups here; we will do some more complicated things in the exercises, but there is also a series of Colab Notebooks with some other tutorials in the documentation [here]((https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)), and the PyTorch Geometric Git repository has a series of other examples available [here](https://github.com/pyg-team/pytorch_geometric/tree/master/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e7f91",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### Example\n",
    "\n",
    "Now we will try to improve our performance for this task, by varying the hyperparameters of our GNN. To do this, we will use two different architectures. The steps are as follows:\n",
    "\n",
    "* Define two different GCN models: make sure both of them have 34 input features, 2 embedding features in the final embedding space, and 4 output values for multiclass classification. Make sure to use the same seed for both models.\n",
    "\n",
    "    1. Increasing the size of the embedding layers: first hidden layer with 16 output features and second hidden layer with 16 output features\n",
    "\n",
    "    1. Add an additional `GCNConv` layer: use the same set of hidden sizes, but add an additional layer with 4 input features and 4 output features. The final GNN layer should still have 2 inputs and 4 outputs\n",
    "\n",
    "* Write a function to train a model for a single epoch, where it takes the model and optimizer as inputs to the function\n",
    "\n",
    "* Train each model for 400 epochs, using `CrossEntropyLoss` and the `Adam` optimizer\n",
    "\n",
    "* For each trained model, plot:\n",
    "    * The training and validation loss as a function of epoch\n",
    "\n",
    "    * The final layer embeddings\n",
    "\n",
    "* Also calculate the validation node classification accuracy for each model\n",
    "\n",
    "Use the same activation function for both models; you can use `ELU`, as we have used above, or you could choose another activation function you have learned about on this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e68b9",
   "metadata": {},
   "source": [
    "We will denote these two possible models as \"wider\" and \"deeper\" respectively; \"wider\" refers the model with more hidden channels, while \"deeper\" refers to the model with an extra `GCNConv` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model definitions\n",
    "\n",
    "class KarateGNN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        layer_input_sizes = [data.num_node_features, *hidden_sizes]\n",
    "        layer_output_sizes = [*hidden_sizes, 2]\n",
    "        # Need to specifically use ModuleList rather than a standard Python list, as otherwise\n",
    "        # we would need to manually register the parameters of the GCNConv layers so they could actually\n",
    "        # be learned\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for in_channels, out_channels in zip(layer_input_sizes, layer_output_sizes):\n",
    "            self.hidden_layers.append(gnn.GCNConv(in_channels = in_channels, out_channels = out_channels))\n",
    "\n",
    "        self.lin = nn.Linear(in_features = 2, out_features = 4)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv in self.hidden_layers:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            \n",
    "\n",
    "        output = self.lin(x)\n",
    "        return output, x\n",
    "    \n",
    "karate_gnn_wider = KarateGNN(hidden_sizes = [16, 16], seed = 1234)\n",
    "\n",
    "karate_gnn_deeper = KarateGNN(hidden_sizes = [4, 4, 4], seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer_wider = optim.Adam(karate_gnn_wider.parameters(), lr = 0.01)\n",
    "optimizer_deeper = optim.Adam(karate_gnn_deeper.parameters(), lr = 0.01)\n",
    "\n",
    "def train_epoch(model,optimizer, loss_fcn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out, h  = model(data)\n",
    "    loss = loss_fcn(out[data.train_mask], data.y[data.train_mask]) # get training outputs & labels\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def val_epoch(model, loss_fcn):\n",
    "    model.eval()\n",
    "    out, h = model(data)\n",
    "    loss = loss_fcn(out[~data.train_mask], data.y[~data.train_mask])\n",
    "    return loss\n",
    "\n",
    "# Optional: define function to format running average of epoch times\n",
    "def duration(epoch_t):\n",
    "    t = np.mean(epoch_t[-20:])\n",
    "    secs = int(t % 60)\n",
    "    millis = (t - int(t)) * 1000\n",
    "    if secs == 0:\n",
    "        return f'{millis:.1f} ms'\n",
    "    else:\n",
    "        return f'{t + millis:.3f} seconds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Train models\n",
    "\n",
    "n_epochs = 400\n",
    "\n",
    "train_losses = {'wider':[],'deeper':[]}\n",
    "val_losses = {'wider':[],'deeper':[]}\n",
    "\n",
    "epoch_times = []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_epoch(karate_gnn_wider, optimizer_wider, loss_fcn).detach().numpy()\n",
    "    val_loss = val_epoch(karate_gnn_wider,loss_fcn).detach().numpy()\n",
    "    train_losses['wider'].append(train_loss)\n",
    "    val_losses['wider'].append(val_loss)\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_times.append(epoch_end_time - epoch_start_time)\n",
    "    if epoch%20==0:\n",
    "        print(f\"Epoch {epoch}: WIDER train loss = {train_loss:.3f}, val_loss = {val_loss:.3f}, running average epoch time = {duration(epoch_times)}\")\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal wider model training time: {end_time-start_time:.2f} seconds\\n\")\n",
    "    \n",
    "epoch_times = []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_epoch(karate_gnn_deeper, optimizer_deeper, loss_fcn).detach().numpy()\n",
    "    val_loss = val_epoch(karate_gnn_deeper,loss_fcn).detach().numpy()\n",
    "    train_losses['deeper'].append(train_loss)\n",
    "    val_losses['deeper'].append(val_loss)\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_times.append(epoch_end_time - epoch_start_time)\n",
    "    if epoch%20==0:\n",
    "        print(f\"Epoch {epoch}: Deeper train loss = {train_loss:.3f}, val_loss = {val_loss:.3f}, running average epoch time = {duration(epoch_times)}\")\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal deeper model training time: {end_time - start_time:.2f} seconds\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine/plot results\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6),dpi = 300)\n",
    "\n",
    "ax1.plot(np.arange(1,n_epochs+1),train_losses['wider'],label='Train loss',color='black')\n",
    "ax1.plot(np.arange(1,n_epochs+1),val_losses['wider'],label='Validation loss',color='#D55E00')\n",
    "ax1.set_xlabel('Epoch',fontsize = 16)\n",
    "ax1.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax1.set_title('Wider GNN model',fontsize = 20)\n",
    "ax1.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax1.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax1.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax1.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax1.legend(loc='upper right',fontsize = 12,framealpha = 1)\n",
    "\n",
    "ax2.plot(np.arange(1,n_epochs+1),train_losses['deeper'],label='Train loss',color='black')\n",
    "ax2.plot(np.arange(1,n_epochs+1),val_losses['deeper'],label='Validation loss',color='#D55E00')\n",
    "ax2.set_xlabel('Epoch',fontsize = 16)\n",
    "ax2.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax2.set_title('Deeper GNN model',fontsize = 20)\n",
    "ax2.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax2.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax2.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax2.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax2.legend(loc='upper right',fontsize = 12,framealpha = 1)\n",
    "\n",
    "shared_ylim = [np.min([ax1.get_ylim()[0], ax2.get_ylim()[0]]),np.max([ax1.get_ylim()[1], ax2.get_ylim()[1]])]\n",
    "ax1.set_ylim(shared_ylim)\n",
    "ax2.set_ylim(shared_ylim)\n",
    "\n",
    "fig.suptitle('Comparison of losses during training',fontsize = 24)\n",
    "fig.set_layout_engine('constrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b57a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final layer embeddings\n",
    "\n",
    "_, h_wider = karate_gnn_wider.eval()(data)\n",
    "\n",
    "\n",
    "_, h_deeper = karate_gnn_deeper.eval()(data)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,7),dpi = 300)\n",
    "plot_embedding(h_wider, ax = ax1, markersize=200, legend = True)\n",
    "ax1.set_title('Wider GNN final\\nlayer embeddings',fontsize = 18)\n",
    "plot_embedding(h_deeper, ax = ax2, markersize = 200, legend = True)\n",
    "ax2.set_title('Deeper GNN final\\nlayer embeddings',fontsize = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02895e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification accuracy\n",
    "\n",
    "def get_model_acc(model, mask):\n",
    "    model.eval()\n",
    "    pred, _ = model(data)\n",
    "    correct = pred[mask].argmax(dim = 1) == data.y[mask]\n",
    "    acc = int(correct.sum())/int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "wider_val_acc = get_model_acc(karate_gnn_wider, ~data.train_mask)\n",
    "deeper_val_acc = get_model_acc(karate_gnn_deeper, ~data.train_mask)\n",
    "\n",
    "print(f'Wider GNN validation accuracy = {wider_val_acc:.1%}')\n",
    "print(f'Deeper GNN validation accuracy = {deeper_val_acc:.1%}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fda90",
   "metadata": {},
   "source": [
    "It's also interesting to look at our accuracy per class; from our final layer embedding plots, we might expect to particularly struggle with class 2, as we only have four samples and in our embedding space they don't tend to form a distinct cluster (or, in the case of the deeper model, overlap completely with another class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wider_class_accs = [get_model_acc(karate_gnn_wider, (~data.train_mask) & (data.y == i)) for i in range(dataset.num_classes)]\n",
    "deeper_class_accs = [get_model_acc(karate_gnn_deeper, (~data.train_mask) & (data.y == i)) for i in range(dataset.num_classes)]\n",
    "\n",
    "print(\"Wider GNN individual class validation accuracies:\")\n",
    "for i, acc in enumerate(wider_class_accs):\n",
    "    print(f\"    Class {i} accuracy: {acc:.1%}\")\n",
    "print(\"\")\n",
    "print(\"Deeper GNN individual class validation accuracies:\")\n",
    "for i, acc in enumerate(deeper_class_accs):\n",
    "    print(f\"    Class {i} accuracy: {acc:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10912d72",
   "metadata": {},
   "source": [
    "As expected, we do poorly on class 2 but also generally on class 1; we can see from the deeper GNN that it might be learning mostly how to discriminate between class 0 and class 3. This could be explained by the oversmoothness problem discussedin [Appendix E](#appendix-e). The wider GNN performs effectively as well as our original GNN, suggesting the issue was not the overall capacity of the model. However, we can clearly see overfitting on the training data for both of these GNNs, so additional training is not likely to improve performance. We may need some regularisation to improve our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1adee",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "\n",
    "End of example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1441228",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we have discussed how we can build and train GNNs using PyTorch Geometric, including:\n",
    "\n",
    "* How to instantiate a simple GNN layer using PyG, specifically `GCNConv`\n",
    "\n",
    "* Different methods of constructing GNN models using `torch_geometric.nn.Sequential` and by subclassing `torch.nn.Module`, including the specific syntax necessary for both\n",
    "\n",
    "* Details of Zachary's karate club dataset, a simple but classic dataset for graph-based learning\n",
    "\n",
    "* How we put everything together and perform a node classification task\n",
    "\n",
    "In the next section, you will work through some exercises for node classification and graph classification, using the Cora and ENZYMES datasets respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38168e",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Exercises <a id='exercises'></a>[^](#index)\n",
    "\n",
    "In this section, we will look at a couple different graph datasets and see the differences needed for graph classification, as well as try a couple other types of GNN layer from PyTorch Geometric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59fa20",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "So far we have talked about how we cannot use fully-connected neural networks for graph datasets, but how well can we do if we did try using a simple multilayer perceptron? To investigate this, we will use the Cora dataset mentioned in [Section 2](#python-graphs) of this notebook. \n",
    "\n",
    "This will be a **transductive** training scenario, as although we will only use the training nodes for training, we effectively only mask the non-training nodes when it comes to calculating the training loss to update our model parameters. They still contribute to finding the output for our training nodes because they are still in the graph and can still pass messages based on their features.\n",
    "\n",
    "To start with, we will load in the dataset. Note that we apply a transform as we load the data in, to normalise the node features. Without this, the feature vector for each node will just sum to how many of the dictionary words are contained in the corresponding paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data', name = 'Cora', transform = NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe4675",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "First, a few questions about the dataset:\n",
    "\n",
    "* How many nodes are in the graph?\n",
    "\n",
    "* How many features does each node have?\n",
    "\n",
    "* How many different classes are there?\n",
    "\n",
    "* How many training, validation, and test samples are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "print(f'Number of nodes: {dataset[0].num_nodes}')\n",
    "print(f'Number of node features: {dataset.num_node_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'Number of training samples: {dataset[0].train_mask.sum()}')\n",
    "print(f'Number of validation samples: {dataset[0].val_mask.sum()}')\n",
    "print(f'Number of test samples: {dataset[0].test_mask.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38284bc6",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now, train a simple 2-layer MLP on the node features from the training nodes of the Cora dataset, using an activation function of your choice. Remember the following:\n",
    "\n",
    "* Use 16 hidden channels in the MLP, as well as a dropout layer with your choice of value for `p`\n",
    "\n",
    "* Apply NN layers in the order linear, activation, dropout\n",
    "\n",
    "* Use an `Adam` optimizer with your choice of `lr` and `weight_decay`, and the `CrossEntropyLoss` loss function\n",
    "\n",
    "* You can get the number of node features and classes from the dataset using `dataset.num_node_features` and `dataset.num_classes` respectively\n",
    "\n",
    "* You must extract the node features array from the `Dataset` object, which contains a single `Data` object representing the graph\n",
    "\n",
    "* Use the graph `train_mask` attribute to select just the training nodes for training and `val_mask` to select the validation nodes. \n",
    "\n",
    "* When evaluating performance on the validation nodes, make sure to put the model into evaluation mode by running `model.eval()`\n",
    "\n",
    "* Keep the test set to one side for now; we will use it at the end of the Cora exercises to evaluate performance from all of our models.\n",
    "\n",
    "* Be sure to track the train and validation loss as you train\n",
    "\n",
    "Train the MLP for 200 epochs, and then evaluate the training and validation classification accuracy. Select the prediction class for each model output by finding the maximum value output, as we did in the `KarateClub` example. Also, plot the training and validation loss as a function of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the graph from the dataset\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_sizes, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        input_channels = [dataset.num_node_features,*hidden_sizes[:-1]]\n",
    "        output_channels = hidden_sizes\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for in_features, out_features in zip(input_channels, output_channels):\n",
    "            self.hidden_layers.append(nn.Linear(in_features = in_features, out_features = out_features))\n",
    "        \n",
    "        self.out_layer = nn.Linear(hidden_sizes[-1], dataset.num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.hidden_layers:\n",
    "            X = layer(X)\n",
    "            X = F.relu(X)\n",
    "            X = F.dropout(X, p = 0.5, training = self.training)\n",
    "        \n",
    "        out = self.out_layer(X)\n",
    "        return out\n",
    "\n",
    "mlp = MLP(hidden_sizes = [16], seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training routine, optimiser, loss function\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr = 0.01, weight_decay = 5e-4)\n",
    "\n",
    "def train_epoch(model, optimizer, loss_fcn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x)\n",
    "    loss = loss_fcn(out[data.train_mask],data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def val_epoch(model, loss_fcn):\n",
    "    model.eval()\n",
    "    out = model(data.x)\n",
    "    loss = loss_fcn(out[data.val_mask], data.y[data.val_mask])\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_acc(model, mask):\n",
    "    model.eval()\n",
    "    pred = model(data.x).argmax(dim = 1)\n",
    "    test_correct = pred[mask] == data.y[mask]\n",
    "    test_acc = int(test_correct.sum())/int(mask.sum())\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "n_epochs = 200\n",
    "\n",
    "train_losses, val_losses, val_acc, epoch_t = [], [], [], []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_epoch(mlp, optimizer, loss_fcn).detach().numpy()\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss = val_epoch(mlp, loss_fcn).detach().numpy()\n",
    "    val_losses.append(val_loss)\n",
    "    epoch_t.append(time.time() - epoch_start_time)\n",
    "    if epoch%20 == 0:\n",
    "        print(f'Epoch {epoch}: train loss = {train_loss:.3f}, val loss = {val_loss:.3f}, running average epoch time = {duration(epoch_t)}')\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b23e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test performance\n",
    "\n",
    "# Plot loss\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "fig = plt.figure(figsize=(8,6),dpi = 150)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(1,n_epochs+1),train_losses,label='Train loss',color='black')\n",
    "ax.plot(np.arange(1,n_epochs+1),val_losses,label='Validation loss',color='#D55E00')\n",
    "ax.set_xlabel('Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax.set_title('MLP loss during training',fontsize = 20)\n",
    "ax.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax.legend(loc='center right',fontsize = 12,framealpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f365c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification accuracy\n",
    "\n",
    "mlp_train_acc = get_acc(mlp, data.train_mask)\n",
    "mlp_val_acc = get_acc(mlp, data.val_mask)\n",
    "print(f'Train accuracy: {mlp_train_acc:.1%}')\n",
    "print(f'Validation accuracy: {mlp_val_acc:.1%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac332b45",
   "metadata": {},
   "source": [
    "From the loss curve we can clearly see that we very quickly overfit to the training data, and while we perform well on the training data we achieve only just more than 50% accuracy on the validation data. Clearly, this type of architecture is not sufficient to handle this data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f493c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "You should find that while you can get some performance, you probably don't do very well on this data as we have in no way leveraged connections between different nodes. Instead, to do this we will need to use our GNN structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c41a8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Now we will attempt the same node classification again, but this time using a 2-layer GNN. Remember the following points:\n",
    "\n",
    "* Define a GNN with 2 `GCNConv` layers, with 16 hidden channels like our MLP; don't use any linear layers, and instead have the final GNN have `num_classes` output channels\n",
    "\n",
    "* Include an activation function and dropout layer after your first GNN layer, like in the MLP; use the same activation function and value of `p` for the dropout layer\n",
    "\n",
    "* Use either `Sequential` or a `nn.Module` subclass to define your model; make sure you correctly pass the node features and edge indices to the `GCNConv` layers\n",
    "\n",
    "* Use a `CrossEntropyLoss` loss function and the `Adam` optimizer, with the same parameters as for the MLP training\n",
    "\n",
    "* Make sure to train only on the training set nodes, and evaluate the performance only on the validation set nodes\n",
    "\n",
    "Train the GNN for 200 epochs. How does the validation accuracy compare with your MLP results? Comment in the Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GNN\n",
    "\n",
    "class CoraGCN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        layer_input_sizes = [data.num_node_features, *hidden_sizes[:-1]]\n",
    "        layer_output_sizes = [*hidden_sizes]\n",
    "        # Need to specifically use ModuleList rather than a standard Python list, as otherwise\n",
    "        # we would need to manually register the parameters of the GCNConv layers so they could actually\n",
    "        # be learned\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for in_channels, out_channels in zip(layer_input_sizes, layer_output_sizes):\n",
    "            self.hidden_layers.append(gnn.GCNConv(in_channels = in_channels, out_channels = out_channels))\n",
    "\n",
    "        self.out_layer = gnn.GCNConv(in_channels = hidden_sizes[-1], out_channels = dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p = 0.5, training = self.training)\n",
    "\n",
    "        out = self.out_layer(x, edge_index)\n",
    "        return out\n",
    "    \n",
    "cora_gcn = CoraGCN(hidden_sizes = [16], seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop, loss function, optimizer\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cora_gcn.parameters(), lr = 0.01, weight_decay = 5e-4)\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fcn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def val_epoch(model):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    loss = loss_fcn(out[data.val_mask], data.y[data.val_mask])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "train_loss, val_loss, epoch_t = [], [], []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    tloss = train_epoch(cora_gcn, optimizer).detach().numpy()\n",
    "    train_loss.append(tloss)\n",
    "    vloss = val_epoch(cora_gcn).detach().numpy()\n",
    "    val_loss.append(vloss)\n",
    "    epoch_t.append(time.time() - epoch_start_time)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}: train loss = {tloss:.3f}, validation loss = {vloss:.3f}, running average epoch time = {duration(epoch_t)}')\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test performance\n",
    "\n",
    "# Plot loss\n",
    "fig = plt.figure(figsize=(8,6),dpi = 150)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(1,n_epochs+1),train_loss,label='Train loss',color='black')\n",
    "ax.plot(np.arange(1,n_epochs+1),val_loss,label='Validation loss',color='#D55E00')\n",
    "ax.set_xlabel('Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax.set_title('GNN loss during training',fontsize = 20)\n",
    "ax.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax.legend(loc='center right',fontsize = 12,framealpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification accuracy\n",
    "\n",
    "def get_acc_gnn(model, mask):\n",
    "    model.eval()\n",
    "    pred = model(data).argmax(dim = 1)\n",
    "    test_correct = pred[mask] == data.y[mask]\n",
    "    test_acc = int(test_correct.sum())/int(mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "gcn_train_acc = get_acc_gnn(cora_gcn, data.train_mask)\n",
    "gcn_val_acc = get_acc_gnn(cora_gcn, data.val_mask)\n",
    "print(f'GNN train accuracy: {gcn_train_acc:.1%}')\n",
    "print(f'GNN validation accuracy: {gcn_val_acc:.1%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e08e65",
   "metadata": {},
   "source": [
    "Suddenly, although our training accuracy has dropped we see a much greater validation accuracy of 79%. Clearly, the graph structure has improved our ability to classify these nodes. Also, we can clearly see that both the training and validation loss are continuing to decrease, so we could potentially train this model for more epochs to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c439c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Now we have tried a GNN, we will try varying some parameters of this GNN to see if we can improve our performance; specifically, try:\n",
    "\n",
    "1. Increasing the number of hidden channels, e.g. to 32\n",
    "\n",
    "1. Adding an additional `GCNConv` layer with associated activation and dropout layers, with 10 hidden channels\n",
    "\n",
    "The procedure for building and training these is otherwise the same as in the previous exercise. How do these results compare with your previous results? Are either of these approaches better than the other? Answer in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426640b",
   "metadata": {},
   "source": [
    "Note that we use the labelling of \"wider\" and \"deeper\" in the same way as we do in the Karate Club exericse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "\n",
    "# For completeness will include CoraGCN definition again\n",
    "\n",
    "class CoraGCN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        layer_input_sizes = [data.num_node_features, *hidden_sizes[:-1]]\n",
    "        layer_output_sizes = [*hidden_sizes]\n",
    "        # Need to specifically use ModuleList rather than a standard Python list, as otherwise\n",
    "        # we would need to manually register the parameters of the GCNConv layers so they could actually\n",
    "        # be learned\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for in_channels, out_channels in zip(layer_input_sizes, layer_output_sizes):\n",
    "            self.hidden_layers.append(gnn.GCNConv(in_channels = in_channels, out_channels = out_channels))\n",
    "\n",
    "        self.out_layer = gnn.GCNConv(in_channels = hidden_sizes[-1], out_channels = dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p = 0.5, training = self.training)\n",
    "\n",
    "        out = self.out_layer(x, edge_index)\n",
    "        return out\n",
    "\n",
    "cora_gcn_wider = CoraGCN(hidden_sizes=[32], seed = 1234)\n",
    "\n",
    "cora_gcn_deeper = CoraGCN(hidden_sizes=[16, 10], seed = 1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "optimizer_wider = optim.Adam(cora_gcn_wider.parameters(), lr = 0.01, weight_decay = 5e-4)\n",
    "optimizer_deeper = optim.Adam(cora_gcn_deeper.parameters(), lr = 0.01, weight_decay = 5e-4)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Repeating training & validation functions for completeness\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fcn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def val_epoch(model):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    loss = loss_fcn(out[data.val_mask], data.y[data.val_mask])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91685d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "train_losses = {'wider':[],'deeper':[]}\n",
    "val_losses = {'wider':[],'deeper':[]}\n",
    "\n",
    "epoch_t = []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_epoch(cora_gcn_wider, optimizer_wider).detach().numpy()\n",
    "    val_loss = val_epoch(cora_gcn_wider).detach().numpy()\n",
    "    train_losses['wider'].append(train_loss)\n",
    "    val_losses['wider'].append(val_loss)\n",
    "    epoch_t.append(time.time() - epoch_start_time)\n",
    "    if epoch%20==0:\n",
    "        print(f\"Epoch {epoch}: Wider train loss = {train_loss:.3f}, val_loss = {val_loss:.3f}, running average epoch time = {duration(epoch_t)}\")\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "print('\\n')\n",
    "\n",
    "epoch_t = []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train_epoch(cora_gcn_deeper, optimizer_deeper).detach().numpy()\n",
    "    val_loss = val_epoch(cora_gcn_deeper).detach().numpy()\n",
    "    train_losses['deeper'].append(train_loss)\n",
    "    val_losses['deeper'].append(val_loss)\n",
    "    epoch_t.append(time.time() - epoch_start_time)\n",
    "    if epoch%20==0:\n",
    "        print(f\"Epoch {epoch}: Deeper train loss = {train_loss:.3f}, val_loss = {val_loss:.3f}, running average epoch time = {duration(epoch_t)}\")\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,7),dpi = 300)\n",
    "\n",
    "ax1.plot(np.arange(1,n_epochs+1),train_losses['wider'],label='Train loss',color='black')\n",
    "ax1.plot(np.arange(1,n_epochs+1),val_losses['wider'],label='Validation loss',color='#D55E00')\n",
    "ax1.set_xlabel('Epoch',fontsize = 20)\n",
    "ax1.set_ylabel('Cross entropy loss',fontsize = 20)\n",
    "ax1.set_title('Wider Cora GCN model',fontsize = 24)\n",
    "ax1.tick_params(labelsize=16, which='both', top = True, right = True, direction='in')\n",
    "ax1.xaxis.set_minor_locator(MultipleLocator(4))\n",
    "ax1.yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "ax1.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax1.legend(loc='upper right',fontsize = 18,framealpha = 1)\n",
    "\n",
    "ax2.plot(np.arange(1,n_epochs+1),train_losses['deeper'],label='Train loss',color='black')\n",
    "ax2.plot(np.arange(1,n_epochs+1),val_losses['deeper'],label='Validation loss',color='#D55E00')\n",
    "ax2.set_xlabel('Epoch',fontsize = 20)\n",
    "ax2.set_ylabel('Cross entropy loss',fontsize = 20)\n",
    "ax2.set_title('Deeper Cora GCN model',fontsize = 24)\n",
    "ax2.tick_params(labelsize=16, which='both', top = True, right = True, direction='in')\n",
    "ax2.xaxis.set_minor_locator(MultipleLocator(4))\n",
    "ax2.yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "ax2.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax2.legend(loc='upper right',fontsize = 18,framealpha = 1)\n",
    "\n",
    "shared_ylim = [np.min([ax1.get_ylim()[0], ax2.get_ylim()[0]]),np.max([ax1.get_ylim()[1], ax2.get_ylim()[1]])]\n",
    "ax1.set_ylim(shared_ylim)\n",
    "ax2.set_ylim(shared_ylim)\n",
    "\n",
    "fig.suptitle('Comparison of losses during training',fontsize = 30)\n",
    "fig.set_layout_engine('constrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b929f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wider_train_acc = get_acc_gnn(cora_gcn_wider, data.train_mask)\n",
    "wider_val_acc = get_acc_gnn(cora_gcn_wider, data.val_mask)\n",
    "print(f'GNN train accuracy: {wider_train_acc:.1%}')\n",
    "print(f'GNN validation accuracy: {wider_val_acc:.1%}')\n",
    "print('')\n",
    "deeper_train_acc = get_acc_gnn(cora_gcn_deeper, data.train_mask)\n",
    "deeper_val_acc = get_acc_gnn(cora_gcn_deeper, data.val_mask)\n",
    "print(f'GNN train accuracy: {deeper_train_acc:.1%}')\n",
    "print(f'GNN validation accuracy: {deeper_val_acc:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b052f",
   "metadata": {},
   "source": [
    "From the loss curves, while we are learning the training data more effectively than the validation data, the validation loss is still improving so we might be able to train some more, although it could be plateauing soon. \n",
    "\n",
    "From the classification accuracies, both networks achieve 100% training set accuracy. We see that the wider network performs better on the validation data than our original network, but the deeper network has worse performance. We might simply require more epochs to train the deeper network, or there could be more fundamental issues with that architecture for this problem - without spending time to train it more and test it, we can't say for certain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70365cb3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "Finally, we are going to try using a different type of GNN layer to see if we can improve our performance further - specifically, we will use the `GATConv` layer, which implements the graph attentional operator from [this paper](https://arxiv.org/abs/1710.10903). This makes use of so-called **attention mechanisms**, which allow us to pay more or less attention to different nodes for a given node. We will see attention mechanisms in more detail when we discuss transformers.\n",
    "\n",
    "This layer has a couple additional parameters that we will specify, namely `heads` and `dropout`. `heads` refers to the number of attention heads used, i.e. how many different ways we can attend to nodes. `dropout` refers to the `p` parameter of a dropout layer, which is applied to the attention calculation. Full details of this layer can be seen in the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv).\n",
    "\n",
    "Build a GNN with 2 `GATConv` layers using the `CoraGAT` class defined in the cell below. Use 8 hidden channels in this model.\n",
    "\n",
    "The training and testing procedure is the same as the other models we have trained. Remember the following:\n",
    "\n",
    "* Use the same loss function, optimizer, and optimizer parameters as before\n",
    "\n",
    "* Train on the training set nodes and evaluate on the validation set\n",
    "\n",
    "Train for 200 epochs and calculate the validation set performance. Does this perform better than your other models? Answer in the Markdown cell below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "\n",
    "class CoraGAT(nn.Module):\n",
    "    def __init__(self, hidden_size, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        # number of output channels of GATConv is equal to out_channels * heads i.e. hidden_size channels per head\n",
    "        self.conv1 = gnn.GATConv(in_channels = dataset.num_node_features, out_channels = hidden_size, heads = 8)\n",
    "        self.conv2 = gnn.GATConv(in_channels = hidden_size * 8, out_channels = dataset.num_classes, heads = 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p = 0.6, training = self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p = 0.6, training = self.training)\n",
    "        out = self.conv2(x, edge_index)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "cora_gat = CoraGAT(hidden_size = 8, seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49417819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "optimizer_gat = optim.Adam(cora_gat.parameters(), lr = 0.01, weight_decay = 5e-4)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Repeating training & validation functions for completeness\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fcn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def val_epoch(model):\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    loss = loss_fcn(out[data.val_mask], data.y[data.val_mask])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "gat_train_loss, gat_val_loss, epoch_t = [], [], []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    tloss = train_epoch(cora_gat, optimizer_gat).detach().numpy()\n",
    "    gat_train_loss.append(tloss)\n",
    "    vloss = val_epoch(cora_gat).detach().numpy()\n",
    "    gat_val_loss.append(vloss)\n",
    "    epoch_t.append(time.time() - epoch_start_time)\n",
    "    if epoch%20 == 0:\n",
    "        print(f'Epoch {epoch}: train loss = {tloss:.3f}, validation loss = {vloss:.3f}, running average epoch time = {duration(epoch_t)}')\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test performance\n",
    "\n",
    "# Plot loss\n",
    "fig = plt.figure(figsize=(8,6),dpi = 150)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(1,n_epochs+1),gat_train_loss,label='Train loss',color='black')\n",
    "ax.plot(np.arange(1,n_epochs+1),gat_val_loss,label='Validation loss',color='#D55E00')\n",
    "ax.set_xlabel('Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax.set_title('GNN with GATConv loss during training',fontsize = 20)\n",
    "ax.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax.legend(loc='upper right',fontsize = 12,framealpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68887240",
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_train_acc = get_acc_gnn(cora_gat, data.train_mask)\n",
    "gat_val_acc = get_acc_gnn(cora_gat, data.val_mask)\n",
    "print(f'GNN with GATConv train accuracy: {gat_train_acc:.1%}')\n",
    "print(f'GNN with GATConv validation accuracy: {gat_val_acc:.1%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8c8f5",
   "metadata": {},
   "source": [
    "Now even though we reduced the hidden layer size, we have improved our validation performance by another 1.6%. The addition of attention heads significantly increases the number of parameters in the model as well as changes how different nodes are used in prediction. We could in general try varying our hyperparameters for any of these models to improve the performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a73f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Finally, now evaluate the performance of all of your MLP, all three GCN models, and the GAT model, on the **test** dataset, which you can get using `data.test_mask`. Which performs best? Answer in the Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e83eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "mlp_test_acc = get_acc(mlp, data.test_mask)\n",
    "gcn_test_acc = get_acc_gnn(cora_gcn, data.test_mask)\n",
    "gcn_wider_test_acc = get_acc_gnn(cora_gcn_wider, data.test_mask)\n",
    "gcn_deeper_test_acc = get_acc_gnn(cora_gcn_deeper, data.test_mask)\n",
    "gat_test_acc = get_acc_gnn(cora_gat, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc034b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table to visualise all accuracy results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "accs = {'MLP':[mlp_train_acc, mlp_val_acc, mlp_test_acc],\n",
    "        'GCN':[gcn_train_acc, gcn_val_acc, gcn_test_acc],\n",
    "        'Wider GCN':[wider_train_acc, wider_val_acc, gcn_wider_test_acc],\n",
    "        'Deeper GCN':[deeper_train_acc, deeper_val_acc, gcn_deeper_test_acc],\n",
    "        'GAT':[gat_train_acc, gat_val_acc, gat_test_acc]}\n",
    "\n",
    "df = pd.DataFrame.from_dict(accs, orient='index', columns=['Training','Validation','Test'])\n",
    "# using pandas styling to highlight best value for each category\n",
    "\n",
    "def make_pretty(styler):\n",
    "    styler.set_caption(\"Cora classification accuracy\")\n",
    "    styler.format(lambda v : f'{v:.1%}')\n",
    "    styler.background_gradient(axis = None,high = 0.5, cmap='twilight', gmap = df == df.max(axis = 0).values.astype(float))\n",
    "    return styler\n",
    "\n",
    "df.style.pipe(make_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca67768",
   "metadata": {},
   "source": [
    "Clearly the GAT model has the best performance in all cases, although many of the models achieve 100% training accuracy. The test performance is only slightly better for GAT vs the first GCN or the wider GCN, but significantly outperforms the MLP or deeper GCN. If you have the time and the inclination, feel free to play about with hyperparameters or different architectures to see if you can improve the performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442356ca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "<!--Now start looking at ENZYMES; do graph classification, brief comment on how to go from node embeddings to graph embeddings, then eventually use `GraphConv` to mention skip connections-->\n",
    "\n",
    "Now we are going to do a graph classification exercise, instead of the node classification exercises we have done so far. To do this, we will use a different dataset from the `TUDataset` class, called MUTAG. This is another molecular classification task, where each graph represents a molecule and they are classified based on their effect on a given type of bacteria. This specifically is an **inductive** training scenario, as we will have completely separate training, validation and test datasets consisting of different graphs.\n",
    "\n",
    "First, we will load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(root='data', name = 'MUTAG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b56ae5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now, inspect the data: \n",
    "\n",
    "* How many graphs are in the dataset?\n",
    "\n",
    "* How many node features are there?\n",
    "\n",
    "* How many classes are there?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of graphs in dataset: {len(dataset)}\")\n",
    "print(f\"Numer of node features: {dataset.num_node_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb96081",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Split the dataset into training, validation, and test datasets, with a 50:25:25 split. Define `DataLoader` objects for these datasets using `torch_geometric.loader.DataLoader`, with a batch size of 32. We will keep the test data to one side and use it to evaluate performance across all of our models at the end of these graph classification exercises.\n",
    "\n",
    "Note: because PyTorch Geometric is built on top of PyTorch, you can use the same utility functions we have seen previously for other types of neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "#Seed the random split for consistency\n",
    "g = torch.Generator()\n",
    "g.manual_seed(2)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.5, 0.25, 0.25], generator = g)\n",
    "print(f\"Train dataset has {len(train_dataset)} entries\")\n",
    "print('')\n",
    "print(f\"Validation dataset has {len(val_dataset)} entries\")\n",
    "print('')\n",
    "print(f\"Test dataset has {len(test_dataset)} entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbad66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will seed the training DataLoader, so we produce a consistent output from our training, see \n",
    "# https://docs.pytorch.org/docs/stable/notes/randomness.html#dataloader for details\n",
    "# import numpy\n",
    "# import random\n",
    "\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     numpy.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True, worker_init_fn = seed_worker, generator = g)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True, generator = g)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 64, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6565ca",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Now we will train a simple GNN to classify these graphs. \n",
    "\n",
    "Build a GNN that does the following steps:\n",
    "\n",
    "* Uses three `GCNConv` layers, with 64 output features for each\n",
    "\n",
    "* ReLU activation functions after the first 2 `GCNConv` layers\n",
    "\n",
    "* Apply a global pooling to the node embeddings using one of the global pooling functions in `torch_geometric.nn`, e.g. `global_mean_pool`, `global_add_pool`, etc\n",
    "\n",
    "* Include a dropout layer after global pooling, with your choice of value for `p`\n",
    "\n",
    "* Finally, apply a linear layer to convert the graph embedding to a class vector, as we have done before for multiclass classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667503e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "class MutagGCN(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.conv1 = gnn.GCNConv(dataset.num_node_features, 64)\n",
    "        self.conv2 = gnn.GCNConv(64, 64)\n",
    "        self.conv3 = gnn.GCNConv(64, 64)\n",
    "        self.lin = nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # Find node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "\n",
    "        # Pool node embeddings to get graph embedding\n",
    "        x = gnn.global_mean_pool(x, batch)\n",
    "\n",
    "        # Apply classifier to graph embedding\n",
    "        x = F.dropout(x, p = 0.5, training = self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "mutag_gcn = MutagGCN(seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d065c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Finally, train your GNN for 200 epochs using the training data, and evaluate your final performance on the test data. As before, use a `CrossEntropyLoss` loss function and the `Adam` optimizer, with a learning rate of 0.01.\n",
    "\n",
    "How well do you do? Calculate any metrics you feel are appropriate based on what you have learned in the course so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "\n",
    "optimizer = optim.Adam(mutag_gcn.parameters(), lr = 0.01)\n",
    "loss_fcn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fcn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/len(train_dataset)\n",
    "\n",
    "def val_epoch(model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            out = model(data)\n",
    "            loss = loss_fcn(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss/len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaeab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "train_loss, val_loss, epoch_t = [],[], []\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "    tloss = train_epoch(mutag_gcn, optimizer)\n",
    "    train_loss.append(tloss)\n",
    "    vloss = val_epoch(mutag_gcn)\n",
    "    val_loss.append(vloss)\n",
    "    epoch_t.append(time.time()-epoch_start_time)\n",
    "    if epoch%20 == 0:\n",
    "        print(f\"Epoch {epoch}: training loss = {tloss:.3f}, validation loss = {vloss:.3f}, running average epoch time = {duration(epoch_t)}\")\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eebad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test performance\n",
    "\n",
    "# Plot loss\n",
    "fig = plt.figure(figsize=(8,6),dpi = 150)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(1,n_epochs+1),train_loss,label='Train loss',color='black')\n",
    "ax.plot(np.arange(1,n_epochs+1),val_loss,label='Validation loss',color='#D55E00')\n",
    "ax.set_xlabel('Epoch',fontsize = 16)\n",
    "ax.set_ylabel('Cross entropy loss',fontsize = 16)\n",
    "ax.set_title('MUTAG GNN loss during training',fontsize = 20)\n",
    "ax.tick_params(labelsize=12, which='both', top = True, right = True, direction='in')\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(5))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax.legend(loc='upper right',fontsize = 12,framealpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a966f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate training & validation accuracy\n",
    "\n",
    "def evaluate_acc(model):\n",
    "    train_corr, val_corr = 0, 0\n",
    "    model.eval()\n",
    "    for data in train_loader:\n",
    "        pred = model(data).argmax(dim = 1)\n",
    "        train_corr += (pred == data.y).sum()\n",
    "    for data in val_loader:\n",
    "        pred = model(data).argmax(dim = 1)\n",
    "        val_corr += (pred == data.y).sum()\n",
    "    return train_corr/len(train_dataset), val_corr/len(val_dataset)\n",
    "\n",
    "gcn_train_acc, gcn_val_acc = evaluate_acc(mutag_gcn)\n",
    "print(f\"Training accuracy = {gcn_train_acc:.1%}\")\n",
    "print(f\"Validation accuracy = {gcn_val_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056cd96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC-AUC curve\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def get_roc_vals(model, loader):\n",
    "    y, pred = [], []\n",
    "    for data in loader:\n",
    "        y.append(data.y)\n",
    "        pred.append(model(data).detach())\n",
    "    y = torch.concat(y, dim = 0).numpy()\n",
    "    pred = torch.concat(pred, dim = 0).numpy()\n",
    "    fpr, tpr, _ = roc_curve(y,pred[:,1])\n",
    "    roc_auc = roc_auc_score(y, pred[:,1])\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "def plot_roc_curve(model, name, ax):\n",
    "    train_fpr, train_tpr, train_auc = get_roc_vals(model, train_loader)\n",
    "    val_fpr, val_tpr, val_auc = get_roc_vals(model, val_loader)\n",
    "\n",
    "    ax.plot(train_fpr, train_tpr, color='#D55E00', label=f'Training ROC-AUC = {train_auc:.3f}')\n",
    "    ax.plot(val_fpr, val_tpr, color='#56B4E9', label = f'Validation ROC-AUC = {val_auc:.3f}')\n",
    "    ax.set_title(f'{name} ROC curves',fontsize = 24)\n",
    "    ax.set_xlabel('False positive rate',fontsize = 20)\n",
    "    ax.set_ylabel('True positive rate',fontsize = 20)\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "    ax.tick_params(which='both',direction='in',top = True, right = True, labelsize = 16)\n",
    "    ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "    ax.legend(loc='best',fontsize = 18, framealpha = 1)\n",
    "    return {'train':{'fpr':train_fpr,'tpr':train_tpr,'AUC':train_auc},\n",
    "            'val':{'fpr':val_fpr,'tpr':val_tpr,'AUC':val_auc}}\n",
    "\n",
    "fig = plt.figure(figsize=(8,6),dpi = 300)\n",
    "ax = fig.add_subplot(111)\n",
    "mutag_gcn_roc = plot_roc_curve(mutag_gcn, 'MUTAG GCN', ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817dc554",
   "metadata": {},
   "source": [
    "Here it looks like were are not overfitting too badly, as the training and validation loss are similar, so we could try training this model for longer if we wanted. Our classification accuracies and our ROC-AUC scores are similar for both the training and validation sets, so that is another good sign we are not yet overfitting. \n",
    "\n",
    "The validation loss starting lower than the training loss is attributed to the specific random split of the data and initial weights for the model, while the large variation in loss as a function of epoch i.e. the spikiness of the curves is likely due to the relative batch size and size of the training data; we only have two batches of training data, and 1 batch of validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93117e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "Like before we will now try varying the architecture of our GNN to see if we can improve our performance. Try the following two changes in **two separate models**:\n",
    "\n",
    "1. Add an additional `GCNConv` layer with `ReLU` activation after it, before the final `GCNConv` layer, with 64 output channels\n",
    "\n",
    "1. Increase the number of output channels of your existing `GCNConv` layers from 64 to 128.\n",
    "\n",
    "Otherwise, use the same parameters in your previous model. Make sure you implement these changes in two separate models, not in the same model, and use a different optimizer instance for each model. Train each model for 200 epochs.\n",
    "\n",
    "How does the performance compare to your previous model? Does one method perform better, and does this agree with the results you observed for the Cora dataset in Exercise 3, i.e. does a wider or deeper GNN improve results in both cases or do you see somethign different? Write your answers in the Markdown cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "\n",
    "class MutagGCNCustom(nn.Module):\n",
    "    def __init__(self, hidden_sizes, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        input_sizes = [dataset.num_node_features, *hidden_sizes[:-1]]\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for in_channels, hidden_channels in zip(input_sizes, hidden_sizes):\n",
    "            self.conv_layers.append(gnn.GCNConv(in_channels = in_channels,\n",
    "                                                out_channels = hidden_channels))\n",
    "        self.lin = nn.Linear(hidden_sizes[-1], dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # Get node embeddings\n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i != len(self.conv_layers)-1:\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        # Pool node embeddings to get graph embedding\n",
    "        x = gnn.global_mean_pool(x, batch)\n",
    "\n",
    "        # Classifier on final embedding\n",
    "        x = F.dropout(x, p = 0.5, training = self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "mutag_gcn_deeper = MutagGCNCustom(hidden_sizes = [64, 64, 64, 64], seed = 1234)\n",
    "mutag_gcn_wider = MutagGCNCustom(hidden_sizes = [128, 128, 128], seed = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop, loss function, optimizers\n",
    "\n",
    "optimizer_deeper = optim.Adam(mutag_gcn_deeper.parameters(), lr = 0.01)\n",
    "optimizer_wider = optim.Adam(mutag_gcn_wider.parameters(), lr = 0.01)\n",
    "# specify reduction = 'sum' so we can add up the loss over batches and average after\n",
    "loss_fcn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_fcn(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/len(train_dataset)\n",
    "\n",
    "def val_epoch(model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            out = model(data)\n",
    "            loss = loss_fcn(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss/len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "train_losses = {'wider':[],'deeper':[]}\n",
    "val_losses = {'wider':[],'deeper':[]}\n",
    "\n",
    "epoch_t = []\n",
    "print('Deeper mode:')\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    tloss = train_epoch(mutag_gcn_deeper, optimizer_deeper)\n",
    "    train_losses['deeper'].append(tloss)\n",
    "    vloss = val_epoch(mutag_gcn_deeper)\n",
    "    val_losses['deeper'].append(vloss)\n",
    "    epoch_t.append(time.time()-epoch_start_time)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'    Epoch {epoch}: training loss = {tloss:.3f}, validation loss = {vloss:.3f}, running average epoch time = {duration(epoch_t)}')\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "print('\\n')\n",
    "\n",
    "print('')\n",
    "epoch_t = []\n",
    "print('Wider model:')\n",
    "start_time = time.time()\n",
    "for epoch in range(1,n_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    tloss = train_epoch(mutag_gcn_wider, optimizer_wider)\n",
    "    train_losses['wider'].append(tloss)\n",
    "    vloss = val_epoch(mutag_gcn_wider)\n",
    "    val_losses['wider'].append(vloss)\n",
    "    epoch_t.append(time.time()-epoch_start_time)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'    Epoch {epoch}: training loss = {tloss:.3f}, validation loss = {vloss:.3f}, running average epoch time = {duration(epoch_t)}')\n",
    "train_duration = time.time() - start_time\n",
    "print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,7),dpi = 300)\n",
    "\n",
    "ax1.plot(np.arange(1,n_epochs+1),train_losses['deeper'],label='Train loss',color='black')\n",
    "ax1.plot(np.arange(1,n_epochs+1),val_losses['deeper'],label='Validation loss',color='#D55E00')\n",
    "ax1.set_xlabel('Epoch',fontsize = 20)\n",
    "ax1.set_ylabel('Cross entropy loss',fontsize = 20)\n",
    "ax1.set_title('Deeper MUTAG GCN model',fontsize = 24)\n",
    "ax1.tick_params(labelsize=16, which='both', top = True, right = True, direction='in')\n",
    "ax1.xaxis.set_minor_locator(MultipleLocator(4))\n",
    "ax1.yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "ax1.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax1.legend(loc='upper right',fontsize = 18,framealpha = 1)\n",
    "\n",
    "ax2.plot(np.arange(1,n_epochs+1),train_losses['wider'],label='Train loss',color='black')\n",
    "ax2.plot(np.arange(1,n_epochs+1),val_losses['wider'],label='Validation loss',color='#D55E00')\n",
    "ax2.set_xlabel('Epoch',fontsize = 20)\n",
    "ax2.set_ylabel('Cross entropy loss',fontsize = 20)\n",
    "ax2.set_title('Wider MUTAG GCN model',fontsize = 24)\n",
    "ax2.tick_params(labelsize=16, which='both', top = True, right = True, direction='in')\n",
    "ax2.xaxis.set_minor_locator(MultipleLocator(4))\n",
    "ax2.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "ax2.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "ax2.legend(loc='upper right',fontsize = 18,framealpha = 1)\n",
    "\n",
    "shared_ylim = [np.min([ax1.get_ylim()[0], ax2.get_ylim()[0]]),np.max([ax1.get_ylim()[1], ax2.get_ylim()[1]])]\n",
    "ax1.set_ylim(shared_ylim)\n",
    "ax2.set_ylim(shared_ylim)\n",
    "\n",
    "fig.suptitle('Comparison of losses during training',fontsize = 30)\n",
    "fig.set_layout_engine('constrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af21c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classification accuracy\n",
    "\n",
    "gcn_deeper_train_acc, gcn_deeper_val_acc = evaluate_acc(mutag_gcn_deeper)\n",
    "gcn_wider_train_acc, gcn_wider_val_acc = evaluate_acc(mutag_gcn_wider)\n",
    "\n",
    "print(\"Deeper model:\")\n",
    "print(f\"    Training accuracy = {gcn_deeper_train_acc:.1%}\")\n",
    "print(f\"    Validation accuracy = {gcn_deeper_val_acc:.1%}\")\n",
    "\n",
    "print(\"Wider model:\")\n",
    "print(f\"    Training accuracy = {gcn_wider_train_acc:.1%}\")\n",
    "print(f\"    Validation accuracy = {gcn_wider_val_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fef70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize = (18,6),dpi = 300)\n",
    "mutag_gcn_deeper_roc = plot_roc_curve(mutag_gcn_deeper,'Deeper MUTAG GCN',ax1)\n",
    "mutag_gcn_wider_roc = plot_roc_curve(mutag_gcn_wider,'Wider MUTAG GCN',ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbaece",
   "metadata": {},
   "source": [
    "Compared to our initial model, the deeper model has better training performance and worse validation performance. From the loss we can clearly see we are overfitting on the training data, as the validation loss is increasing. This model may need more regularisation to perform better.\n",
    "\n",
    "However, the wider model has superior classification accuracy for both the training and validation datasets. However, it does look like we are starting to overfit towards the end of training, so some additional regularisation might be of benefit here as well. Both ROC-AUC scores are also superior to the baseline model.\n",
    "\n",
    "This is a similar result to what we saw for the Cora dataset, although in this case we see more obvious overfitting to the training data for the deeper and wider models. By increasing the number of hidden neurons, we have increased the capacity of the model without overly increasing the complexity or causing nodes to converge to similar values (the oversmoothness problem, see earlier). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346e7a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "## Exercise 7\n",
    "\n",
    "Once again, we will now try using a different kind of GNN layer to see if we can improve our performance further. As mentioned in [Section 3](#embeddings), if we have added self-loops to our adjacency matrix then we can share weights for message calculation for both a node's neighbourhood and its own message, which we can incorporate into the next layer embedding value. \n",
    "\n",
    "However, in this setup we cannot separate the contributions of the node itself from its neighbourhood, and we can lose information as a result of normalising over the neighbourhood. One way to mitigate this is to instead define a second matrix of weights that is used exclusively for calculating the message from a node's own previous layer embedding, and omitting the neighbourhood normalisation included in `GCNConv`. This is implemented in PyTorch Geometric as `GraphConv`. \n",
    "\n",
    "Reimplement any (or all, if you have time) of the GNN models you have trained on the MUTAG dataset, only this time replacing all `GCNConv` layers with `GraphConv`. Remember the following:\n",
    "\n",
    "* Keep all other parameters the same as your previous training\n",
    "\n",
    "* Train for 200 epochs and evaluate final performance on the validation dataset\n",
    "\n",
    "How does the performance compare to your previous results? Which GNN layer performs better? Answer in the Markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaff8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model(s)\n",
    "\n",
    "class MutagGraphCN(nn.Module):\n",
    "    def __init__(self, hidden_sizes, seed):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        input_sizes = [dataset.num_node_features, *hidden_sizes]\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for in_channels, hidden_channels in zip(input_sizes, hidden_sizes):\n",
    "            self.conv_layers.append(gnn.GraphConv(in_channels = in_channels,\n",
    "                                                  out_channels = hidden_channels))\n",
    "        self.lin = nn.Linear(hidden_sizes[0], dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # Get node embeddings\n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if i != len(self.conv_layers)-1:\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        # Pool node embeddings to get graph embedding\n",
    "        x = gnn.global_mean_pool(x, batch)\n",
    "\n",
    "        # Classifier on final embedding\n",
    "        x = F.dropout(x, p = 0.5, training = self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "    \n",
    "mutag_graphcn = MutagGraphCN([64, 64, 64], seed = 1234)\n",
    "mutag_graphcn_deeper = MutagGraphCN([64, 64, 64, 64], seed = 1234)\n",
    "mutag_graphcn_wider = MutagGraphCN([128, 128, 128], seed = 1234)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec540ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop/loss function/optimizer\n",
    "\n",
    "optimizer_graphcn = optim.Adam(mutag_graphcn.parameters(),lr = 0.01)\n",
    "optimizer_graphcn_deeper = optim.Adam(mutag_graphcn_deeper.parameters(),lr = 0.01)\n",
    "optimizer_graphcn_wider = optim.Adam(mutag_graphcn_wider.parameters(),lr = 0.01)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Use same training functions as before, define a function to do the whole training\n",
    "def train_n_epochs(model, optimizer, n_epochs):\n",
    "    train_loss, val_loss, epoch_t = [], [], []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        tloss = train_epoch(model, optimizer)\n",
    "        train_loss.append(tloss)\n",
    "        vloss = val_epoch(model)\n",
    "        val_loss.append(vloss)\n",
    "        epoch_t.append(time.time() - epoch_start_time)\n",
    "        if epoch%20 == 0:\n",
    "            print(f'    Epoch {epoch}: training loss = {tloss:.3f}, validation loss = {vloss:.3f}, running average epoch time = {duration(epoch_t)}')\n",
    "    train_duration = time.time() - start_time\n",
    "    print(f'\\nTotal training time = {train_duration:.2f} seconds')\n",
    "    print('\\n')\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model(s)\n",
    "print('Baseline GraphConv model:')\n",
    "graphcn_train_loss, graphcn_val_loss = train_n_epochs(mutag_graphcn, optimizer_graphcn, n_epochs = 200)\n",
    "print('')\n",
    "print('Deeper GraphConv model:')\n",
    "graphcn_deeper_train_loss, graphcn_wider_val_loss = train_n_epochs(mutag_graphcn_deeper, optimizer_graphcn_deeper, n_epochs = 200)\n",
    "print('')\n",
    "print('Wider GraphConv model:')\n",
    "graphcn_wider_train_loss, graphcn_deeper_val_loss = train_n_epochs(mutag_graphcn_wider, optimizer_graphcn_wider, n_epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642925ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(tloss, vloss, name, n_epochs, ax):\n",
    "    ax.plot(np.arange(1,n_epochs+1),tloss,label='Train loss',color='black')\n",
    "    ax.plot(np.arange(1,n_epochs+1),vloss,label='Validation loss',color='#D55E00')\n",
    "    ax.set_xlabel('Epoch',fontsize = 20)\n",
    "    ax.set_ylabel('Cross entropy loss',fontsize = 20)\n",
    "    ax.set_title(f'{name} model',fontsize = 24)\n",
    "    ax.tick_params(labelsize=16, which='both', top = True, right = True, direction='in')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(4))\n",
    "    ax.yaxis.set_minor_locator(MultipleLocator(0.004))\n",
    "    ax.grid(color='xkcd:dark blue',alpha = 0.2)\n",
    "    ax.legend(loc='upper right',fontsize = 18,framealpha = 1)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(24,7),dpi = 300)\n",
    "\n",
    "plot_loss_curves(graphcn_train_loss, graphcn_val_loss, 'MUTAG Baseline GraphConv',200, ax1)\n",
    "plot_loss_curves(graphcn_deeper_train_loss, graphcn_deeper_val_loss, 'MUTAG Deeper GraphConv',200, ax2)\n",
    "plot_loss_curves(graphcn_wider_train_loss, graphcn_wider_val_loss, 'MUTAG Wider GraphConv',200, ax3)\n",
    "\n",
    "shared_ylim = [np.min([ax.get_ylim()[0] for ax in (ax1,ax2,ax3)]),\n",
    "               np.max([ax.get_ylim()[1] for ax in (ax1, ax2,ax3)])]\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.set_ylim(shared_ylim)\n",
    "\n",
    "fig.suptitle('Comparison of losses during training',fontsize = 30)\n",
    "fig.set_layout_engine('constrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece45f8f",
   "metadata": {},
   "source": [
    "All three of these appear to begin overfitting after a certain point, which happens sooner for the deeper and wider models. We can look at other metrics to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classification accuracy\n",
    "\n",
    "graphcn_train_acc, graphcn_val_acc = evaluate_acc(mutag_graphcn)\n",
    "graphcn_deeper_train_acc, graphcn_deeper_val_acc = evaluate_acc(mutag_graphcn_deeper)\n",
    "graphcn_wider_train_acc, graphcn_wider_val_acc = evaluate_acc(mutag_graphcn_wider)\n",
    "\n",
    "print(\"Baseline GraphConv model:\")\n",
    "print(f\"    Training accuracy = {graphcn_train_acc:.1%}\")\n",
    "print(f\"    Validation accuracy = {graphcn_val_acc:.1%}\")\n",
    "\n",
    "print(\"\\nDeeper GraphConv model:\")\n",
    "print(f\"    Training accuracy = {graphcn_deeper_train_acc:.1%}\")\n",
    "print(f\"    Validation accuracy = {graphcn_deeper_val_acc:.1%}\")\n",
    "\n",
    "print(\"\\nWider GraphConv model:\")\n",
    "print(f\"    Training accuracy = {graphcn_wider_train_acc:.1%}\")\n",
    "print(f\"    Validation accuracy = {graphcn_wider_val_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize = (24,6),dpi = 300)\n",
    "mutag_graphcn_roc = plot_roc_curve(mutag_graphcn, 'MUTAG GraphCN',ax1)\n",
    "mutag_graphcn_deeper_roc = plot_roc_curve(mutag_graphcn_deeper,'Deeper MUTAG GCN',ax2)\n",
    "mutag_graphcn_wider_roc = plot_roc_curve(mutag_graphcn_wider,'Wider MUTAG GCN',ax3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5738eb",
   "metadata": {},
   "source": [
    "Of these three models, while the loss curves would indicate the baseline `GraphConv` model is overfitting the least we see the best classification accuracy for the wider `GraphConv` model. The ROC-AUC scores are equivalent (to 3 significant figures) for the baseline and wider models, whereas the deeper model has a slightly better training result and a significantly worse validation result. All of these models may benefit from regularisation, but especially the deeper model.\n",
    "\n",
    "All three of these models significantly outperform the `GCNConv` models, suggesting there is indeed a benefit to separately considering neighbour and self contributions to a node's embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e8a60",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "Finally, evaluate the classification accuracy on the test set for all of the models you have trained on the MUTAG dataset and compare the results. Which model performed the best? Is this consistent with your validation set performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def evaluate_test_acc(model):\n",
    "    test_corr = 0\n",
    "    model.eval()\n",
    "    for data in test_loader:\n",
    "        pred = model(data).argmax(dim = 1)\n",
    "        test_corr += (pred == data.y).sum()\n",
    "    return test_corr/len(test_dataset)\n",
    "\n",
    "gcn_test_acc = evaluate_test_acc(mutag_gcn)\n",
    "gcn_deeper_test_acc = evaluate_test_acc(mutag_gcn_deeper)\n",
    "gcn_wider_test_acc = evaluate_test_acc(mutag_gcn_wider)\n",
    "graphcn_test_acc = evaluate_test_acc(mutag_graphcn)\n",
    "graphcn_deeper_test_acc = evaluate_test_acc(mutag_graphcn_deeper)\n",
    "graphcn_wider_test_acc = evaluate_test_acc(mutag_graphcn_wider)\n",
    "    \n",
    "# Make a table to visualise all accuracy results\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "accs = {'GCN':[gcn_train_acc, gcn_val_acc, gcn_test_acc],\n",
    "        'Deeper GCN':[gcn_deeper_train_acc, gcn_deeper_val_acc, gcn_deeper_test_acc],\n",
    "        'Wider GCN':[gcn_wider_train_acc, gcn_wider_val_acc, gcn_wider_test_acc],\n",
    "        'GraphCN':[graphcn_train_acc, graphcn_val_acc, graphcn_test_acc],\n",
    "        'Deeper GraphCN':[graphcn_deeper_train_acc, graphcn_deeper_val_acc, graphcn_deeper_test_acc],\n",
    "        'Wider GraphCN':[graphcn_wider_train_acc, graphcn_wider_val_acc, graphcn_wider_test_acc]}\n",
    "\n",
    "df = pd.DataFrame.from_dict(accs, orient='index', columns=['Training','Validation','Test'])\n",
    "# using pandas styling to highlight best value for each category\n",
    "\n",
    "def make_pretty(styler):\n",
    "    styler.set_caption(\"MUTAG classification accuracy\")\n",
    "    styler.format(lambda v : f'{v:.1%}')\n",
    "    styler.background_gradient(axis = None,high = 0.5, cmap='twilight', gmap = df == df.max(axis = 0).values.astype(float))\n",
    "    return styler\n",
    "\n",
    "df.style.pipe(make_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea644dde",
   "metadata": {},
   "source": [
    "We can see that while many of our models perform equivalently well on the test dataset, our best performing model across all three datasets is the wider `GraphConv` model. In general it seems the deeper models (both `GCNConv` and `GraphConv`) tend to overfit more than the shallower models. Of course, we know the wider `GraphConv` model was also beginning to overfit, so if we were to try training any of these models for longer we might find one of the less overfit models would perform better in the long term. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba01e2e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C2F5DD\">\n",
    "\n",
    "### End of exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f4daf",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "# Appendices <a id='appendices'></a> [^](#index)\n",
    "\n",
    "## Appendix A: More complex types of graph <a id='appendix-a'></a> [^](#index)\n",
    "\n",
    "There are other, more complex types of graph that can exist that still have very interesting problems to solve. Some examples include:\n",
    "\n",
    "* **Multirelational graphs**: these are graphs with multiple types of edges that connect nodes, and a given node can be connected by multiple types of edge.\n",
    "\n",
    "    * In the London Underground example where we say edges represent train routes, the edge type could be which train line the route corresponds to, e.g. both the Circle and District Line run between Gloucester Road and South Kensington\n",
    "<br></br>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 80px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 400px; margin: 0;\">\n",
    "<div style=\"height: 30px;\"></div>\n",
    "<img src='plots/tube-selection.png' width=400 style=\"align-self: center;\"/>\n",
    "<div style=\"height: 30px;\"></div>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 400px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Left</strong>: a section of the London underground map ([source](https://www.tubemaplondon.org/#sirv-viewer-284230498900)).\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 400px; margin: 0;\">\n",
    "<img src='plots/multirelational-graph-fitted.png' width=400 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 400px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Right</strong>: the same section of the London underground tube map, represented as a multirelational graph.\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "* **Heterogeneous graphs**: graphs where we have different types of nodes, so we can separate nodes into distinct sets. Edges in heterogeneous graphs typically satisfy some conditions based on the types of nodes they connect, e.g. they may only connect one type of node to another, or may only connect one type to the same type. We then often have multiple types of edges in these graphs. \n",
    "\n",
    "    * For example, we could have a biomedical graph where the three types of nodes are diseases, medications, and proteins. Edges between disease and medication nodes could represent treatment, whereas edges between two medication nodes might represent interactions between the two drugs. \n",
    "<br></br>\n",
    "\n",
    "<center>\n",
    "<img src='plots/heterogeneous-graph.png' width=600 align='center'></img>\n",
    "<div style='width:600px'>\n",
    "\n",
    "*An example heterogeneous graph, for an e-commerce application. Node types include users, products and reviews, and edge types are user-buy-product, user-write-review, and review-on-product ([source](https://research.google/blog/teaching-old-labels-new-tricks-in-heterogeneous-graphs/)).*\n",
    "\n",
    "</center>\n",
    "\n",
    "* **Bipartite graphs**: a specific type of heterogeneous graphs where the nodes can be separated into 2 disjoint and independent sets, such that nodes in set A only connect to nodes in set B and vice versa. This means there are at least two edges between any two nodes in one set. This can generalise to N sets where edges are only between sets, which are referred to as **multiplex graphs**.\n",
    "\n",
    "    * An example of this type of graph could be found in looking at paper authorship, where we group nodes as authors and papers. An author is linked to the papers they have authored, and so author-author links can be found by edges going to the same node in the paper set. \n",
    "\n",
    "<center>\n",
    "<img src='plots/bipartite-graph.png' width=400 align='center'></img>\n",
    "<div style='width:400px'>\n",
    "\n",
    "*An example bipartite graph mapping three authors A0, A1, and A2 to a set of 5 papers authored between them.*\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "As before, for the purposes of this notebook we will focus solely on simple graphs, but it is useful to be aware of the wide range of possible types of graph data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9c144",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "\n",
    "## Appendix B: Data assumptions with graphs <a id='appendix-b'></a> [^](#index)\n",
    "\n",
    "While on the face of it node classification and relation prediction seem like supervised learning and clustering seems like unsupervised learning, with graph data we in general break a fundamental assumption we have made about our data so far: that data points are **independent** and **identically distributed**. We can will these assumptions in turn: \n",
    "\n",
    "* data points are **independent**: the value at one data point has no dependence on the values of any other data point\n",
    "\n",
    "* data points are **identically distributed**: the underlying distribution determining the value at a given data point is the same for all data points\n",
    "\n",
    "However, for graph data:\n",
    "\n",
    "* each node is not independent; instead, it is connected to other nodes, and the feature values of the nodes in the neighbourhood (and indeed more widely across the graph) are assumed inform the values at the node of interest\n",
    "\n",
    "* nodes are not identically distributed: nodes are assumed to be affected by or similar to nodes in their neighbourhood, but each node has a different neighbourhood and so has a different underlying distribution determining the feature values at the node\n",
    "\n",
    "In fact, one of the things that makes graph node classification approaches successfully is *exploiting* relationships between nodes in order to improve our predictions. For example, there are several different ideas we can exploit: \n",
    "\n",
    "* **homiphily**: assuming that nodes have a tendency to share attributes with neighbours\n",
    "\n",
    "* **structural equivalence**: assuming that nodes with a similar local structure (with regards to number of edges, cycles in the graph, etc.) will have similar labels\n",
    "\n",
    "* **heterophily**: assuming that nodes are preferentially connected to other nodes with *different* labels\n",
    "\n",
    "Regardless of what kind of structure we assume or even observe in our data, the key to good performance in graph tasks is in exploiting these relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec79be5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "\n",
    "## Appendix C: Shallow embeddings <a id='appendix-c'></a> [^](#index)\n",
    "\n",
    "### Shallow embeddings\n",
    "\n",
    "An encoder is be some function that maps a node $v$ into some $d$ dimensional embedding space. The simplest possible such encoder is a basic lookup table. We represent this as follows:\n",
    "\n",
    "* For a node $v$, define an indicator vector $\\mathbf{v}$ that is 1 at the index of node $v$ and 0 elsewhere. By definition, $|\\mathbf{v}|$ is equal to the number of nodes in the graph.\n",
    "\n",
    "* Define some embedding matrix $\\mathbf{Z}$ with $d$ rows and a column for every node, where $d$ is the dimensionality of the embedding space, i.e. $\\mathbf{Z}\\,\\in\\mathbb{R}^{d\\times|\\mathbf{v}|}$\n",
    "\n",
    "* The function of the encoder is then given as \n",
    "\n",
    "$$\\text{ENCODER}(v) = \\mathbf{Z}\\mathbf{v}$$\n",
    "\n",
    "Effectively, for every node in the graph we individually learn an embedding for that node. This means:\n",
    "\n",
    "* Each node embedding is optimised directly\n",
    "\n",
    "* Node embeddings are optimised to ensure the node pair decoder output is close to the similarity of the two nodes\n",
    "\n",
    "* We must define the similarity function to learn our embeddings\n",
    "\n",
    "There are many ways we can define similarity between nodes, e.g. a simple example could be we say two nodes are similar if they have an edge joining them. One common way of defining node similarity is as the probability that the two nodes co-occur on some random walk across the graph.\n",
    "\n",
    "A key point about shallow embeddings is that they don't care about node features or node labels - they only use the structure of the graph to embed the nodes. This is essentially an unsupervised learning of the node embeddings, although this may also be referred to as **self-supervised** as the \"labels\" are from the graph structure.\n",
    "<br></br>\n",
    "\n",
    "However, there are some key issues with shallow embeddings:\n",
    "\n",
    "* By definition we don't use node features, so our embeddings are independent of node features, whereas we generally want to include this information\n",
    "\n",
    "* Two nodes can have very similar local structures e.g. number & type adjacent nodes, graph cycles, etc., but may not be likely to be both reached in a random walk - these will still have very different embeddings in a shallow framework\n",
    "\n",
    "* We *cannot generalise to unseen nodes* - each embedding is learned for the node, so we would have to manually learn a new embedding for a new node. \n",
    "\n",
    "**For a more comprehensive discussion of shallow embeddings, please see Section I of *Graph Representation Learning* (link to prepublication pdf available at the start of this notebook).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43cf17",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "## Appendix D: Permutation invariance and equivariance <a id='appendix-d'></a> [^](#index)\n",
    "\n",
    "* For some function $f$ that maps a graph $G$ to a *vector*, if for any permutation of $G$ the output of $f$ is unchanged, we say $f$ is a **permutation-invariant** function\n",
    "\n",
    "* For some function $f$ that maps a graph $G$ to a *matrix*, if for any permutation of $G$ the output of $f$ is permuted in the *same way*, we say that $f$ is a **permutation-equivariant** function\n",
    "\n",
    "If we want our deep encoder to operate on our graph correctly, it must be composed of permutation-invariant and permutation-equivariant functions.\n",
    "\n",
    "\n",
    "Let us denote our graph $G = (\\mathbf{A}, \\mathbf{X})$ where $\\mathbf{A} \\in \\mathbb{R}^{|V|\\times|V|}$ is the adjacency matrix and $\\mathbf{X} \\in \\mathbb{R}^{|V|\\times m}$ is the matrix of node features and $m$ denotes the number of node features per node.\n",
    "\n",
    "\n",
    "\n",
    "We now define permutation invariance and equivariance as follows:\n",
    "\n",
    "* Permutation invariance: for any graph function $f:  \\mathbb{R}^{|V|\\times|V|} \\times \\mathbb{R}^{|V|\\times m} \\to \\mathbb{R}^d$, $f$ is **permutation-invariant** if \n",
    "\n",
    "$$f(\\mathbf{A}, \\mathbf{X}) = f(\\mathbf{P}\\mathbf{A}\\mathbf{P}^T, \\mathbf{P}\\mathbf{X})\\text{ for any permutation of the graph }\\mathbf{P}$$\n",
    "\n",
    "* Permutation equivariance: for any graph function $f: \\mathbb{R}^{|V|\\times|V|} \\times \\mathbb{R}^{|V|\\times m} \\to \\mathbb{R}^{m\\times d}$, $f$ is **permutation-equivariant** if \n",
    "\n",
    "$$\\mathbf{P} f(\\mathbf{A}, \\mathbf{X}) = f(\\mathbf{P}\\mathbf{A}\\mathbf{P}^T, \\mathbf{P}\\mathbf{X})\\text{ for any permutation of the graph }\\mathbf{P}$$\n",
    "\n",
    "If we were to consider a simple fully-connected neural network and flatten our adjacency matrix into a vector to act as an input to this network, we can immediately see that we are *not* permutation-invariant: the order of the vector we feed into our neural network matters but is dependent on the arbitrary node labelling we used for the adjacency matrix. \n",
    "\n",
    "Similarly, CNNs rely on a consistent definition of locality across the input, but the number of edges connected to a node can be different for every node so we cannot just learn weights of the convolutional filter like we do for a CNN; again, CNN operations are not permutation invariant.\n",
    "\n",
    "Instead, we have to design new layers to work on graph data, which rely on so-called **message passing** and **aggregation** of information from neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c1483",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "\n",
    "## Appendix E: aggregating embeddings for edge and graph tasks <a id='appendix-e'></a> [^](#index)\n",
    "\n",
    "\n",
    "* Edge-level tasks - we need to make predictions based on pairs of node embeddings, so need to somehow aggregate them:\n",
    "    \n",
    "    * We could simply concatenate the embeddings from the pair of nodes into a single vector, then apply a linear layer to get the desired size output vector e.g. with an embedding space with $d$ dimensions and $n$ target classes, the linear layer would have $2d$ input neurons and $n$ output neurons\n",
    "\n",
    "    * Alternatively, we could take some product of the two node embeddings; for predicting a single output e.g. if a link exists, this could just be the dot product between the embeddings, but if we want to predict $n$ output values for a given edge then things are more complicated (see non-examinable box below)\n",
    "<br></br>\n",
    "\n",
    "* Graph-level tasks - we need to make predictions based on *all* of the node embeddings, so need to aggregate them all:\n",
    "\n",
    "    * We could apply the same type of aggregation functions used in GNN layers, e.g. mean pooling, max pooling, sum pooling, to get one aggregated embedding for the graph (best for *small* graphs)\n",
    "\n",
    "    * For larger graphs, global pooling often loses information e.g. if different regions of the graph have a different scale to their embeddings\n",
    "        * Instead, can hierarchically pool node embeddings into sub-graph embeddings, and then pool the subgraph embeddings to get a whole graph embedding\n",
    "\n",
    "        * A famous example of this idea is [DiffPool](https://arxiv.org/abs/1806.08804), which effectively runs two GNNs simultaneously; at each layer, one clusters the nodes and the other computes embeddings, and embeddings are pooled according to their learned clusters to produce a new graph with fewer nodes to input into the next layer. This is illustrated in the schematic below.\n",
    "\n",
    "    * Regardless of how we do it, after we have an embedding for the graph we can then apply standard techniques e.g. a fully-connected neural network to classify the graph embedding\n",
    "\n",
    "<center>\n",
    "<img src='plots/diffpool.png' width=800></img>\n",
    "<div style=\"text-align:center;\">\n",
    "<div style='width:760px;display:inline-block;vertical-align:top'>\n",
    "<div style=\"text-align: justify;\">\n",
    "\n",
    "\n",
    "*A schematic illustrating DiffPool [[source](https://arxiv.org/abs/1806.08804)]. At each layer of the whole model, one GNN layer learns embeddings for the graph and another GNN layer applies a softmax to a different set of learned embeddings, to determine clusters to assign each node to. The computed embeddings are then aggregated over the computed clusters to define a new graph to be passed into the next layer of the model. Effectively, two GNNs run in parallel to compute embeddings and to cluster nodes.*\n",
    "</div></div></div>\n",
    "</center>\n",
    "\n",
    "For prediction of $n$ edge features based on the dot product of the node embeddings, rather than just taking a dot product we must instead define a learnable matrix for each of the $n$ features:\n",
    "\n",
    "* Let $\\mathbf{z}_u$, $\\mathbf{z}_v$ $\\in \\mathbb{R}^{d}$ denote the final layer $d$-dimensional node embeddings with for nodes $u$ and $v$ respectively\n",
    "\n",
    "* Let $\\mathbf{W}^{(i)} \\in \\mathbb{R}^{(d\\times d)}$ denote the learnable matrix for edge feature $i$\n",
    "\n",
    "* Let $\\hat{y}_{uv}^{(i)}$ denote edge feature $i$ for the edge joining nodes $u$ and $v$\n",
    "\n",
    "Then, we require that\n",
    "\n",
    "$$\\hat{y}_{uv}^{(i)} = \\mathbf{z}_u^T \\mathbf{W}^{(i)} \\mathbf{z}_v,$$\n",
    "\n",
    "and the final edge feature vector is given as \n",
    "\n",
    "$$\\hat{y}_{uv} = \\left(\\hat{y}_{uv}^{(1)}, \\cdots, \\hat{y}_{uv}^{(n)}\\right) \\in \\mathbb{R}^n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58932d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#efdff2\">\n",
    "\n",
    "\n",
    "## Appendix F: Graph manipulations <a id='appendix-f'></a> [^](#index)\n",
    "\n",
    "* Feature augmentation: could be necessary if we have *no* node features in our graph as GNN architectures needs node features to work\n",
    "    * We could add a new feature to all nodes, all with the same value, so we can still learn from the structure in the graph\n",
    "<br></br>\n",
    "\n",
    "* Handling sparse graphs: this happens when there are not many connections between nodes, so when we apply a message passing layer we are only getting messages from a very small number of nodes\n",
    "\n",
    "    * One option is to add virtual edges e.g. between two-hop neighbours - this is equivalent to replacing the adjacency matrix $\\mathbf{A}$ with a new adjacency matrix equal to $\\mathbf{A} + \\mathbf{A}^2$\n",
    "\n",
    "        * This is useful in bipartite graphs, as nodes in the same group are separated by at least two hops; for e.g. author-publication graphs we can then effectively add edges between co-authors\n",
    "\n",
    "    * Alternatively, we could add a virtual node connected to all the nodes in the graph - this means every node is within a 2 hop neighbourhood\n",
    "\n",
    "    * Examples of both of these approaches are shown for a bipartite graph in the figure below.\n",
    "<br></br>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; align-items: flex-start;\">\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 500px; margin: 0;\">\n",
    "<img src='plots/bipartite-graph-2hop.png' width=500 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 500px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Left</strong>: a bipartite author-publication graph with virtual edges between 2-hop neighbours.\n",
    "</div></div>\n",
    "<div style=\"display: flex; flex-direction: column; align-items: flex-start; width: 500px; margin: 0;\">\n",
    "<img src='plots/bipartite-graph-virtual-node.png' width=500 style=\"align-self: center;\"/>\n",
    "<div style=\"margin-top: 10px; text-align: justify; max-width: 500px; font-style: italic; line-height: 1.2;\">\n",
    "\n",
    "<strong>Right</strong>: the same bipartite graph, with an additional virtual node connected to all other nodes. Each node is thus in at least a 2-hop neighbourhood of any other node.\n",
    "</div></div>\n",
    "</div>\n",
    "\n",
    "* Handling dense graphs: this occurs when we have many nodes with many edges, which can cause issues with computatational cost\n",
    "    * Instead of using every node in the neighbourhood to update our embedding, we can instead randomly sample the neighbourhood and use a subset of the nodes for updates\n",
    "\n",
    "        * So long as we select a new random sample each step in the training, this in general produces embeddings similar to the case where we use all neighbours\n",
    "\n",
    "You might in general implement these manipulations as preprocessing to graphs before passing to your GNN. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
